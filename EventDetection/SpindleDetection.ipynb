{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.stats\n",
    "import mne\n",
    "from mne.time_frequency import tfr_morlet\n",
    "from mne.stats import permutation_cluster_1samp_test\n",
    "import gc\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "from os.path import exists\n",
    "import mne\n",
    "import numpy as np\n",
    "from mne import create_info\n",
    "from IPython.utils import io\n",
    "import yasa\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matlab\n",
    "import matlab.engine\n",
    "import os\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = \"C:\\\\Users\\\\User\\\\Cloud-Drive\\\\BigFiles\\\\OmissionExpOutput\\\\\\imported_eventDetectionChan\\\\filter0.1\"\n",
    "import_type = \"eventDetectionChan\"\n",
    "output_dir_name = 'eventDetection\\\\done_on_imported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_path = f'{pkl_dir}\\\\{import_type}.pkl'\n",
    "\n",
    "with open(import_path, \"rb\") as file:\n",
    "    [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "\n",
    "fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "if not os.path.exists(fig_output_dir):\n",
    "    os.mkdir(fig_output_dir)\n",
    "\n",
    "configu['electrodes_names']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One file yasa detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = '32'\n",
    "# datafile = 2\n",
    "# id = f\"{sub}_{datafile}\"\n",
    "\n",
    "# datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "# datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "\n",
    "# exmp_scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=True)\n",
    "# sp = yasa.spindles_detect(datafile_data,sf=configu['sample_freq'], hypno = exmp_scoring_upsampled,include=[2, 3],ch_names=configu['electrodes_names'],multi_only=True)\n",
    "# print(sp)\n",
    "# print(sp.summary())\n",
    "# # sp.plot_average(center='Peak',time_before = 1,time_after = 1)\n",
    "# # plt.show()\n",
    "# #%matplotlib widget\n",
    "# #sp.plot_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# sp.plot_average(center='Peak', time_before=0.1, time_after=0.1);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch yasa detection and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yasa_output_dir = f\"{configu['outputs_dir_path']}/{output_dir_name}/yasa\"\n",
    "# if not os.path.exists(yasa_output_dir):\n",
    "#     os.mkdir(yasa_output_dir)\n",
    "\n",
    "# for id in allsubsdata_perFile:\n",
    "#     datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "#     datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "#     if 2 not in datafile_scoring or 3 not in datafile_scoring:\n",
    "#         continue\n",
    "\n",
    "#     exmp_scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=False)\n",
    "#     sp = yasa.spindles_detect(datafile_data,sf=configu['sample_freq'], hypno = exmp_scoring_upsampled,include=[2, 3],ch_names=configu['electrodes_names'],multi_only=True)\n",
    "#     #summary_df  = sp.summary()\n",
    "#     summary_df.to_csv(f\"{yasa_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_yasaSpindles.csv\")\n",
    "#     sp.plot_average(center='Peak', time_before=1, time_after=1)\n",
    "#     plt.savefig(f\"{yasa_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_yasaSpindles_average.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One file Nir&Andrillon Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matlab\n",
    "# import matlab.engine\n",
    "\n",
    "# sub = '32'\n",
    "# datafile = 2 \n",
    "# id = f\"{sub}_{datafile}\"\n",
    "\n",
    "# datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "# datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "# datafile_data_cz = datafile_data[1,:]\n",
    "# exmp_scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data_cz, sf_data=configu['sample_freq'], verbose=True)\n",
    "\n",
    "# eng = matlab.engine.start_matlab()\n",
    "# eng.SpindlesDetectionAlgorithm_fieldtrip(np.array(datafile_data_cz), configu['sample_freq'], np.array(exmp_scoring_upsampled))\n",
    "\n",
    "# #spindles, spindlesOutEnergy, spindlesOutDuration, rej_cand = eng.SpindlesDetectionAlgorithm_fieldtrip(datafile_data_cz, configu['sample_freq'], exmp_scoring_upsampled)\n",
    "\n",
    "# eng.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Nir&Andrillon Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31-ינו-23 19:52:21 | WARNING | Hypnogram is SHORTER than data by 4.94 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 19:53:02 | WARNING | Hypnogram is SHORTER than data by 26.41 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 19:53:40 | WARNING | Hypnogram is SHORTER than data by 21.62 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 19:53:40 | WARNING | Hypnogram is SHORTER than data by 25.70 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 19:54:16 | WARNING | Hypnogram is SHORTER than data by 2.56 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 19:54:54 | WARNING | Hypnogram is SHORTER than data by 3.10 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 19:54:54 | WARNING | Hypnogram is SHORTER than data by 5.26 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 19:54:54 | WARNING | Hypnogram is SHORTER than data by 23.19 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 19:55:31 | WARNING | Hypnogram is SHORTER than data by 22.21 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 19:56:09 | WARNING | Hypnogram is SHORTER than data by 16.46 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 19:56:09 | WARNING | Hypnogram is SHORTER than data by 0.64 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 19:56:46 | WARNING | Hypnogram is SHORTER than data by 15.76 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 19:57:23 | WARNING | Hypnogram is SHORTER than data by 6.78 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 19:58:01 | WARNING | Hypnogram is SHORTER than data by 23.73 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 19:58:38 | WARNING | Hypnogram is SHORTER than data by 23.01 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 19:59:15 | WARNING | Hypnogram is SHORTER than data by 24.42 seconds. Padding hypnogram with last value to match data.size.\n"
     ]
    }
   ],
   "source": [
    "def deleteAuxANFiles(andriNir_aux_output_dir):\n",
    "    all_files_in_outputDir = os.listdir(andriNir_aux_output_dir)\n",
    "    for file_in_outputDir in all_files_in_outputDir:\n",
    "            os.remove(os.path.join( andriNir_aux_output_dir, file_in_outputDir ))\n",
    "\n",
    "\n",
    "andriNir_code_dir = \"C:\\\\Users\\\\User\\\\Cloud-Drive\\\\AnatArzi\\\\scripts\\\\Matlab_scripts - Sharons\\\\Andrillon_Nir_Algo\"\n",
    "andriNir_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\\\\Andrillon_Nir\"\n",
    "andriNir_aux_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\\\\Andrillon_Nir\\\\aux_mats\"\n",
    "spindles_output_columns =  ['spindleStartTime', 'spindleEndTime', 'peakTime', 'peakEnergy', 'peakEnergyNorm', 'freqSpindle', 'spindleDuration/SR', 'PowerSP', 'PowerAlpha', 'currentStage']\n",
    "electrode_column_name = 'electrode'\n",
    "\n",
    "if not os.path.exists(andriNir_output_dir):\n",
    "    os.mkdir(andriNir_output_dir)\n",
    "\n",
    "electrodes_names_eventDetect = ['F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "\n",
    "all_electodes_ss_key = 'SS_algo-AN_all'\n",
    "\n",
    "for id in allsubsdata_perFile:\n",
    "    datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "    datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "    scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=True)\n",
    "    ss_allElectrodes = pd.DataFrame()\n",
    "\n",
    "    for electd_i, electrode_name_eventDetect in enumerate(electrodes_names_eventDetect):\n",
    "        curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name_eventDetect)[0][0]\n",
    "\n",
    "        ## create aux files to use in MATLAB\n",
    "        datafile_1elect_eeg = datafile_data[curr_electrode_num,:]\n",
    "        if 2 not in datafile_scoring or 3 not in datafile_scoring:\n",
    "            continue\n",
    "        \n",
    "        mat_to_save =  {'datafile_data': datafile_1elect_eeg, 'scoring_upsampled': scoring_upsampled, 'sample_freq': configu['sample_freq'], 'electrode_name':electrode_name_eventDetect}\n",
    "        scipy.io.savemat(f\"{andriNir_aux_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\",mat_to_save)\n",
    "\n",
    "        ## run Andrillon & Nir SS detection over all subjects files\n",
    "        eng = matlab.engine.start_matlab()\n",
    "        eng.cd(andriNir_code_dir, nargout=0)\n",
    "        out = eng.batch_useAndrillonNirEventDetection(andriNir_aux_output_dir, andriNir_output_dir,nargout=0)\n",
    "        eng.quit()\n",
    "\n",
    "        ## add the spindles data to the main subject dictionary\n",
    "        try: \n",
    "            spindles_file_name = f\"{andriNir_output_dir}\\\\Spindles_andrillonNir_{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\"\n",
    "            matlabImport = scipy.io.loadmat(spindles_file_name, simplify_cells=True)\n",
    "        except Exception: \n",
    "            print(f\"Error importing spindles sub file at: {allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}\")\n",
    "            continue\n",
    "\n",
    "        ss = matlabImport['spindles']\n",
    "        if np.size(ss) >0 :\n",
    "            df = pd.DataFrame(np.double(ss))\n",
    "            df.columns = spindles_output_columns\n",
    "            tile_electrode = np.tile(f\"{electrode_name_eventDetect}\", ss.shape[0])[None].T\n",
    "            df[electrode_column_name] = tile_electrode\n",
    "            \n",
    "            allsubsdata_perFile[id][f'SS_algo-AN@@{electrode_name_eventDetect}'] = df\n",
    "            if electd_i ==0: ss_allElectrodes =  df\n",
    "            else:  ss_allElectrodes = pd.concat([ss_allElectrodes,df])\n",
    "\n",
    "        # sw = matlabImport['allWaves']\n",
    "        # if np.size(sw) >0 :\n",
    "        #     tile_electrode = np.tile(f\"{electrode_name_eventDetect}\", sw.shape[0])[None].T\n",
    "        #     sw = np.hstack([sw, tile_electrode])\n",
    "        #     allsubsdata_perFile[id][f'SW_algo-AN@@{electrode_name_eventDetect}'] = np.asarray(sw,dtype=object)  \n",
    "        #     sw_allElectrodes = np.concatenate((sw_allElectrodes,sw))\n",
    " \n",
    "    if not ss_allElectrodes.empty:\n",
    "        allsubsdata_perFile[id][all_electodes_ss_key]  = ss_allElectrodes\n",
    "        #allsubsdata_perFile[id][f'SW_algo-AN_all']  = sw_allElectrodes\n",
    "    \n",
    "deleteAuxANFiles(andriNir_aux_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub:32_1, before excludeNonN2Events:2419, after:789\n",
      "sub:32_2, before excludeNonN2Events:2231, after:765\n",
      "sub:33_1, before excludeNonN2Events:2491, after:1530\n",
      "sub:33_2, before excludeNonN2Events:3556, after:2758\n",
      "sub:34_1, before excludeNonN2Events:1363, after:416\n",
      "sub:34_2, before excludeNonN2Events:1667, after:443\n",
      "sub:35_1, before excludeNonN2Events:1076, after:190\n",
      "sub:35_3, before excludeNonN2Events:1742, after:951\n",
      "sub:35_4, before excludeNonN2Events:668, after:250\n",
      "sub:38_1, before excludeNonN2Events:2531, after:1379\n",
      "sub:38_2, before excludeNonN2Events:3091, after:1793\n",
      "sub:38_3, before excludeNonN2Events:2127, after:1433\n"
     ]
    }
   ],
   "source": [
    "after_n2_exclution_key = f\"{all_electodes_ss_key}_n2\"\n",
    "for id in allsubsdata_perFile:\n",
    "    if all_electodes_ss_key not in allsubsdata_perFile[id]: continue\n",
    "    ss_allElectrodes = allsubsdata_perFile[id][all_electodes_ss_key]\n",
    "    ss_allElectrodes_n2 = ss_allElectrodes.loc[ss_allElectrodes['currentStage'] == 2.0]\n",
    "    if len(ss_allElectrodes_n2) == 0: \n",
    "        continue\n",
    "    else:\n",
    "        print(f\"sub:{id}, before excludeNonN2Events:{len(ss_allElectrodes)}, after:{len(ss_allElectrodes_n2)}\")\n",
    "    allsubsdata_perFile[id][after_n2_exclution_key] = ss_allElectrodes_n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub:32_1, before filt:2419, after:532\n",
      "sub:32_2, before filt:2231, after:417\n",
      "sub:33_1, before filt:2491, after:677\n",
      "sub:33_2, before filt:3556, after:711\n",
      "sub:34_1, before filt:1363, after:350\n",
      "sub:34_2, before filt:1667, after:238\n",
      "sub:35_1, before filt:1076, after:395\n",
      "sub:35_3, before filt:1742, after:427\n",
      "sub:35_4, before filt:668, after:151\n",
      "sub:38_1, before filt:2531, after:491\n",
      "sub:38_2, before filt:3091, after:536\n",
      "sub:38_3, before filt:2127, after:395\n"
     ]
    }
   ],
   "source": [
    "## create a spindle array, where each spindle occour once and attribued to the electrode where the spindle was the most powerful\n",
    "\n",
    "ss_key_to_use = all_electodes_ss_key # after_n2_exclution_key\n",
    "uniqeElctd_ss_key = f\"{ss_key_to_use}_uniqeElctd\"\n",
    "\n",
    "def overlap(a, b):\n",
    "    return a[1] >= b[0] and a[0] <= b[1]\n",
    "\n",
    "for id in allsubsdata_perFile:\n",
    "    if ss_key_to_use not in allsubsdata_perFile[id]:  continue\n",
    "    filterd_events_allElectrodes = allsubsdata_perFile[id][ss_key_to_use].copy(deep=True)\n",
    "    \n",
    "    filterd_events_allElectrodes.sort_values(by=['spindleStartTime'],inplace=True) \n",
    "    deleted = filterd_events_allElectrodes.copy(deep=True)\n",
    "    filtered = pd.DataFrame([], columns = deleted.columns)\n",
    "    simultan = pd.DataFrame([], columns = deleted.columns)\n",
    "    while len(deleted)>0:\n",
    "        if len(simultan)==0:\n",
    "            simultan = pd.concat([simultan, deleted.iloc[[0]]])\n",
    "            deleted.drop(deleted.index[0], axis=0, inplace=True)\n",
    "            still_overlap = True\n",
    "        else:\n",
    "            while still_overlap == True:\n",
    "                still_overlap = False\n",
    "                simultan.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "\n",
    "                ## check now_overlap_all_in_simultan:\n",
    "                for index, simultan_row in simultan.iterrows():\n",
    "                    simultan_0 = simultan_row['spindleStartTime']\n",
    "                    simultan_1 = simultan_row['spindleEndTime']\n",
    "                    deleted_0 = deleted.iloc[0]['spindleStartTime']\n",
    "                    deleted_1 = deleted.iloc[0]['spindleEndTime']\n",
    "                    if overlap([simultan_0,simultan_1],[deleted_0,deleted_1]):\n",
    "                        simultan = pd.concat([simultan, deleted.iloc[[0]]])\n",
    "                        deleted.drop(deleted.index[0], axis=0, inplace=True)\n",
    "                        still_overlap = True\n",
    "                        break\n",
    "                if still_overlap: continue\n",
    "                else:\n",
    "                    ## check max ps and add to filt\n",
    "                    simultan.sort_values(by=['PowerSP'],inplace=True)\n",
    "                    row_df = simultan.iloc[[0]]\n",
    "                    row_df.iat[0, row_df.columns.get_loc(electrode_column_name)] = np.array(simultan[electrode_column_name])\n",
    "                    filtered = pd.concat([filtered,row_df])\n",
    "                    simultan = pd.DataFrame([], columns = deleted.columns)                       \n",
    "        \n",
    "    filtered.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "    allsubsdata_perFile[id][uniqeElctd_ss_key] = filtered   \n",
    "    print(f\"sub:{id}, before filt:{np.shape(allsubsdata_perFile[id][ss_key_to_use])[0]}, after:{np.shape(allsubsdata_perFile[id][uniqeElctd_ss_key])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "#display(HTML(allsubsdata_perFile[id][uniqeElctd_ss_key].to_html()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change scoring to fit EDF_viewer events format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_scoring_to_edfViewer(score):\n",
    "    if score == 0:\n",
    "        return 'W'\n",
    "    elif score ==1:\n",
    "        return 'N1'\n",
    "    elif score ==2:\n",
    "        return 'N2'\n",
    "    elif score ==3:\n",
    "        return 'N3'\n",
    "    elif score ==4:\n",
    "        return 'TREM'\n",
    "    elif score ==5:\n",
    "        return 'PREM'\n",
    "    elif score ==6:\n",
    "        return 'MOVE'\n",
    "    elif score ==7:\n",
    "        return 'ARTIFACT'\n",
    "    else:\n",
    "        Exception('no such score')\n",
    "        \n",
    "\n",
    "for id in allsubsdata_perFile:\n",
    "    curr_file_scoring = allsubsdata_perFile[id]['scoring']\n",
    "    new_format_score = np.zeros((len(curr_file_scoring),3), dtype=object)\n",
    "    for ind, score in enumerate(curr_file_scoring):\n",
    "            new_format_score[ind,:] = [30*ind,30,format_scoring_to_edfViewer(score)] ## onset (sec), duration, desc\n",
    "\n",
    "    allsubsdata_perFile[id]['scoring_efdViewFormat'] = new_format_score\n",
    "    # np.savetxt('test2.txt', new_format_score, delimiter='\\t',fmt='%s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change spindles events to fit EDF_viewer events format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no SS_algo-AN_all_uniqeElctd for sub 32_3\n",
      "no SS_algo-AN_all_uniqeElctd for sub 33_3\n",
      "no SS_algo-AN_all_uniqeElctd for sub 33_4\n",
      "no SS_algo-AN_all_uniqeElctd for sub 34_3\n"
     ]
    }
   ],
   "source": [
    "event_key_for_save = uniqeElctd_ss_key\n",
    "SS_efdViewFormat_key = 'SS_efdViewFormat'\n",
    "header = np.array(['Onset',\"Duration\",\"Annotation\"])\n",
    "for id in allsubsdata_perFile:\n",
    "        if event_key_for_save not in allsubsdata_perFile[id]:\n",
    "                print(f\"no {event_key_for_save} for sub {id}\")\n",
    "                continue\n",
    "        ss_df = allsubsdata_perFile[id][uniqeElctd_ss_key]\n",
    "        ss_mat_edfFormat = np.zeros((len(ss_df),3), dtype=object)\n",
    "        startTime_arr = np.array(ss_df['spindleStartTime']) / np.double(configu['sample_freq'])\n",
    "        endTime_arr = np.array(ss_df['spindleEndTime']) / np.double(configu['sample_freq'])\n",
    "        duration_arr = endTime_arr - startTime_arr\n",
    "        electd_arr_per_ss = np.array(ss_df[electrode_column_name])\n",
    "        desc = [f\"SS@@{electd_arr[0]}\" for electd_arr in electd_arr_per_ss]\n",
    "        new_format_ss = np.array([startTime_arr,duration_arr,desc]).T ## onset (sec), duration,desc\n",
    "        #new_format_ss = np.concatenate((new_format_ss,header))\n",
    "        #new_format_ss = np.vstack(new_format_ss, header)\n",
    "        new_format_ss = np.r_[new_format_ss,[header]]\n",
    "        allsubsdata_perFile[id][f'SS_efdViewFormat'] = new_format_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [spindleStartTime spindleEndTime peakTime peakEnergy peakEnergyNorm freqSpindle spindleDuration/SR PowerSP PowerAlpha currentStage];\n",
    "# def EDFViewerformat(key_name):\n",
    "#         curr_file_ss = allsubsdata_perFile[id][key_name]\n",
    "#         new_format_ss = np.zeros((np.shape(curr_file_ss)[0],3), dtype=object)\n",
    "#         for ind, ss in enumerate(new_format_ss):\n",
    "#                 SpindleStartTime = np.double(curr_file_ss[ind,0]) / np.double(configu['sample_freq'])  \n",
    "#                 SpindleDuration = (np.double(curr_file_ss[ind,1])  -np.double(curr_file_ss[ind,0])) / np.double(configu['sample_freq'])\n",
    "#                 if np.size(curr_file_ss[ind,10]) > 1:  \n",
    "#                         event_desc = f\"{key_name.split('_')[0]}@@{curr_file_ss[ind,10][0]}\"\n",
    "#                 else:\n",
    "#                         event_desc = f\"{key_name.split('_')[0]}@@{curr_file_ss[ind,10]}\"\n",
    "#                 new_format_ss[ind,:] = [SpindleStartTime,SpindleDuration,event_desc] ## onset (sec), duration,desc\n",
    "#         allsubsdata_perFile[id][f'{key_name}_efdViewFormat'] = new_format_ss\n",
    "\n",
    "\n",
    "\n",
    "# events_types_for_save = [uniqeElctd_ss_key]\n",
    "# for event_type in events_types_for_save: \n",
    "#         for id in allsubsdata_perFile:\n",
    "#                 if event_type not in allsubsdata_perFile[id]:\n",
    "#                         print(f\"no {event_type} for sub {id}\")\n",
    "#                         continue\n",
    "#                 EDFViewerformat(event_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "edfViewFormat_events_output_dir = f'{fig_output_dir}\\\\EDFViewFormat_events'\n",
    "if not os.path.exists(edfViewFormat_events_output_dir):\n",
    "    os.mkdir(edfViewFormat_events_output_dir)\n",
    "\n",
    "events_types_for_save = ['scoring_efdViewFormat','SS']\n",
    "# Save events (scoring and ss) per file, in a format suitable for EDF_viewer\n",
    "for id in allsubsdata_perFile:\n",
    "        filename = f\"{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_events\"\n",
    "        all_events_with_header = np.asarray([['Onset',\"Duration\",\"Annotation\"]],dtype=object)\n",
    "        for event_type in events_types_for_save:\n",
    "                events_type_found = [ v for k, v in allsubsdata_perFile[id].items() if (event_type in k) and ('_efdViewFormat' in k)]\n",
    "                if np.size(events_type_found) == 0: continue\n",
    "                for events_found in events_type_found:\n",
    "                        for event_found in events_found:\n",
    "                                all_events_with_header = np.concatenate((all_events_with_header,[event_found]),dtype=object)\n",
    "        np.savetxt(f\"{edfViewFormat_events_output_dir}\\\\{filename}.txt\", all_events_with_header, delimiter='\\t',fmt='%s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test TP / FP for the algorithm of Andrillon-Nir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 3)\n",
      "(204, 3)\n",
      "(737, 3)\n",
      "(235, 3)\n",
      "156\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "## get the array of before manual scanning\n",
    "txt_file_name = \"C:\\\\Users\\\\User\\\\Cloud-Drive\\\\BigFiles\\\\OmissionExpOutput\\\\eventDetection\\\\done_on_preProcessed\\\\EDFViewFormat_events\\\\32_2_events.txt\"\n",
    "before = np.loadtxt(txt_file_name, delimiter=\"\\t\",dtype='object')\n",
    "print(np.shape(before))\n",
    "before[1:,0] = [np.double(x) for x in before[1:,0]]\n",
    "before = np.delete(before,0,0)\n",
    "ss_ind = np.array([],dtype=int)\n",
    "for ind_i, desc in enumerate(before[:,2]):\n",
    "    if \"SS\" in desc:\n",
    "        ss_ind = np.append(ss_ind, ind_i)\n",
    "before_ss = before[ss_ind,:]\n",
    "print(np.shape(before_ss))\n",
    "\n",
    "## get the array of after manual scanning\n",
    "txt_file_name = \"C:\\\\Users\\\\User\\\\Cloud-Drive\\\\BigFiles\\\\OmissionExpOutput\\\\eventDetection\\\\done_on_preProcessed\\\\EDFViewFormat_events\\\\s_32_sleep2_preProcessed_annotations.txt\"\n",
    "after = np.loadtxt(txt_file_name, delimiter=\"\\t\",dtype='object')\n",
    "print(np.shape(after))\n",
    "after[1:,0] = [np.double(x) for x in after[1:,0]]\n",
    "\n",
    "ss_ind = np.array([],dtype=int)\n",
    "for ind_i, desc in enumerate(after[:,2]):\n",
    "    if \"SS\" in desc:\n",
    "        ss_ind = np.append(ss_ind, ind_i)\n",
    "after_ss = after[ss_ind,:]\n",
    "print(np.shape(after_ss))\n",
    "\n",
    "## compare to find rate od TP and FP\n",
    "TP = 0\n",
    "FP = 0\n",
    "for ss_auto in before_ss:\n",
    "    found = False\n",
    "    for ss_manu in after_ss:\n",
    "        if ss_auto[0] == ss_manu[0]:\n",
    "            TP +=1\n",
    "            found = True\n",
    "            break\n",
    "    if found == False:\n",
    "        FP +=1\n",
    "\n",
    "print(TP)\n",
    "print(FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how to df \n",
    "\n",
    "## add row to df and empty df\n",
    "x = pd.DataFrame([], columns = deleted.columns)\n",
    "y= deleted.iloc[[0]]\n",
    "y2= deleted.iloc[[1]]\n",
    "z = pd.concat([x,y])\n",
    "zz = pd.concat([z,y2])\n",
    "display(zz)\n",
    "\n",
    "## delete first row\n",
    "# zz.drop(zz.index[0], axis=0, inplace=True) \n",
    "# display(zz)\n",
    "\n",
    "## get value at i,j\n",
    "print(deleted.iloc[0]['spindleStartTime'])\n",
    "\n",
    "## get column as array\n",
    "arr = np.array(zz['electrode'])\n",
    "print(arr)\n",
    "\n",
    "## replace inplace value in i,j to array:\n",
    "zz.iat[1, zz.columns.get_loc(electrode_column_name)] = arr\n",
    "zz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f544ce1a915a9875fad91c894e2c0bcad4b7a79945aa6027ef3ad27810072aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
