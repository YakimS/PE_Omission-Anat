{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.stats\n",
    "import mne\n",
    "from mne.time_frequency import tfr_morlet\n",
    "from mne.stats import permutation_cluster_1samp_test\n",
    "import gc\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "from os.path import exists\n",
    "import mne\n",
    "import numpy as np\n",
    "from mne import create_info\n",
    "from IPython.utils import io\n",
    "import yasa\n",
    "import seaborn as sns\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = \"C:\\\\Users\\\\User\\\\Cloud-Drive\\\\BigFiles\\\\OmissionExpOutput\\\\\\imported_eventDetectionChan\"\n",
    "import_type = \"eventDetectionChan\"\n",
    "output_dir_name = 'eventDetection\\\\done_on_imported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_path = f'{pkl_dir}\\\\{import_type}.pkl'\n",
    "\n",
    "with open(import_path, \"rb\") as file:\n",
    "    [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "\n",
    "fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "if not os.path.exists(fig_output_dir):\n",
    "    os.mkdir(fig_output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One file yasa detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = '32'\n",
    "# datafile = 2\n",
    "# id = f\"{sub}_{datafile}\"\n",
    "\n",
    "# datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "# datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "\n",
    "# exmp_scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=True)\n",
    "# sp = yasa.spindles_detect(datafile_data,sf=configu['sample_freq'], hypno = exmp_scoring_upsampled,include=[2, 3],ch_names=configu['electrodes_names'],multi_only=True)\n",
    "# print(sp)\n",
    "# print(sp.summary())\n",
    "# # sp.plot_average(center='Peak',time_before = 1,time_after = 1)\n",
    "# # plt.show()\n",
    "# #%matplotlib widget\n",
    "# #sp.plot_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# sp.plot_average(center='Peak', time_before=0.1, time_after=0.1);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch yasa detection and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yasa_output_dir = f\"{configu['outputs_dir_path']}/{output_dir_name}/yasa\"\n",
    "# if not os.path.exists(yasa_output_dir):\n",
    "#     os.mkdir(yasa_output_dir)\n",
    "\n",
    "# for id in allsubsdata_perFile:\n",
    "#     datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "#     datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "#     if 2 not in datafile_scoring or 3 not in datafile_scoring:\n",
    "#         continue\n",
    "\n",
    "#     exmp_scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=False)\n",
    "#     sp = yasa.spindles_detect(datafile_data,sf=configu['sample_freq'], hypno = exmp_scoring_upsampled,include=[2, 3],ch_names=configu['electrodes_names'],multi_only=True)\n",
    "#     #summary_df  = sp.summary()\n",
    "#     summary_df.to_csv(f\"{yasa_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_yasaSpindles.csv\")\n",
    "#     sp.plot_average(center='Peak', time_before=1, time_after=1)\n",
    "#     plt.savefig(f\"{yasa_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_yasaSpindles_average.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One file Nir&Andrillon Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matlab\n",
    "# import matlab.engine\n",
    "\n",
    "# sub = '32'\n",
    "# datafile = 2 \n",
    "# id = f\"{sub}_{datafile}\"\n",
    "\n",
    "# datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "# datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "# datafile_data_cz = datafile_data[1,:]\n",
    "# exmp_scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data_cz, sf_data=configu['sample_freq'], verbose=True)\n",
    "\n",
    "# eng = matlab.engine.start_matlab()\n",
    "# eng.SpindlesDetectionAlgorithm_fieldtrip(np.array(datafile_data_cz), configu['sample_freq'], np.array(exmp_scoring_upsampled))\n",
    "\n",
    "# #spindles, spindlesOutEnergy, spindlesOutDuration, rej_cand = eng.SpindlesDetectionAlgorithm_fieldtrip(datafile_data_cz, configu['sample_freq'], exmp_scoring_upsampled)\n",
    "\n",
    "# eng.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Nir&Andrillon Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['E127', 'E94', 'E126', 'E1', 'E31', 'E9', 'E22', 'E36', 'E104',\n",
       "       'E70', 'E83', 'E24', 'E124', 'E52', 'E92', 'Cz'], dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configu['electrodes_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27-ינו-23 17:25:46 | WARNING | Hypnogram is SHORTER than data by 4.94 seconds. Padding hypnogram with last value to match data.size.\n",
      "27-ינו-23 17:25:50 | WARNING | Hypnogram is SHORTER than data by 26.41 seconds. Padding hypnogram with last value to match data.size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported sub file at: 32_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27-ינו-23 17:25:53 | WARNING | Hypnogram is SHORTER than data by 21.62 seconds. Padding hypnogram with last value to match data.size.\n",
      "27-ינו-23 17:25:53 | WARNING | Hypnogram is SHORTER than data by 25.70 seconds. Padding hypnogram with last value to match data.size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported sub file at: 32_2\n"
     ]
    },
    {
     "ename": "MatlabExecutionError",
     "evalue": "\n  File C:\\Users\\User\\Cloud-Drive\\AnatArzi\\scripts\\Matlab_scripts - Sharons\\Andrillon_Nir_Algo\\batch_useAndrillonNirEventDetection.m, line 16, in batch_useAndrillonNirEventDetection\nUnable to find file or directory 'C:\\Users\\User\\Cloud-Drive\\BigFiles\\OmissionExpOutput\\eventDetection\\done_on_imported\\Andrillon_Nir\\33_1_CzAndScoring.mat'.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMatlabExecutionError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [81], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m eng \u001b[39m=\u001b[39m matlab\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39mstart_matlab()\n\u001b[0;32m     38\u001b[0m eng\u001b[39m.\u001b[39mcd(andriNir_code_dir, nargout\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m out \u001b[39m=\u001b[39m eng\u001b[39m.\u001b[39;49mbatch_useAndrillonNirEventDetection(andriNir_aux_output_dir, andriNir_output_dir,nargout\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     40\u001b[0m eng\u001b[39m.\u001b[39mquit()\n\u001b[0;32m     42\u001b[0m \u001b[39m## add the spindles data to the main subject dictionary\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\matlab\\engine\\matlabengine.py:70\u001b[0m, in \u001b[0;36mMatlabFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m FutureResult(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine(), future, nargs, _stdout, _stderr, feval\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     69\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m FutureResult(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine(), future, nargs, _stdout,\n\u001b[0;32m     71\u001b[0m                         _stderr, feval\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mresult()\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\matlab\\engine\\futureresult.py:67\u001b[0m, in \u001b[0;36mFutureResult.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     65\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(pythonengine\u001b[39m.\u001b[39mgetMessage(\u001b[39m'\u001b[39m\u001b[39mTimeoutCannotBeNegative\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__future\u001b[39m.\u001b[39;49mresult(timeout)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\matlab\\engine\\fevalfuture.py:82\u001b[0m, in \u001b[0;36mFevalFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result_ready:\n\u001b[0;32m     80\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m(pythonengine\u001b[39m.\u001b[39mgetMessage(\u001b[39m'\u001b[39m\u001b[39mMatlabFunctionTimeout\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m---> 82\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m pythonengine\u001b[39m.\u001b[39;49mgetFEvalResult(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_future,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_nargout, \u001b[39mNone\u001b[39;49;00m, out\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_out, err\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_err)\n\u001b[0;32m     83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieved \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "\u001b[1;31mMatlabExecutionError\u001b[0m: \n  File C:\\Users\\User\\Cloud-Drive\\AnatArzi\\scripts\\Matlab_scripts - Sharons\\Andrillon_Nir_Algo\\batch_useAndrillonNirEventDetection.m, line 16, in batch_useAndrillonNirEventDetection\nUnable to find file or directory 'C:\\Users\\User\\Cloud-Drive\\BigFiles\\OmissionExpOutput\\eventDetection\\done_on_imported\\Andrillon_Nir\\33_1_CzAndScoring.mat'.\n"
     ]
    }
   ],
   "source": [
    "import matlab\n",
    "import matlab.engine\n",
    "import os\n",
    "\n",
    "def deleteAuxANFiles(andriNir_aux_output_dir):\n",
    "    all_files_in_outputDir = os.listdir(andriNir_output_dir)\n",
    "    for file_in_outputDir in all_files_in_outputDir:\n",
    "        if file_in_outputDir.endswith(\".mat\") and not file_in_outputDir.startswith(\"Spindles_andrillonNir\"):\n",
    "            os.remove(os.path.join( andriNir_output_dir, file_in_outputDir ))\n",
    "\n",
    "andriNir_code_dir = \"C:\\\\Users\\\\User\\\\Cloud-Drive\\\\AnatArzi\\\\scripts\\\\Matlab_scripts - Sharons\\\\Andrillon_Nir_Algo\"\n",
    "andriNir_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\\\\Andrillon_Nir\"\n",
    "andriNir_aux_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\\\\Andrillon_Nir\\\\aux_mats\"\n",
    "if not os.path.exists(andriNir_output_dir):\n",
    "    os.mkdir(andriNir_output_dir)\n",
    "\n",
    "\n",
    "electrodes_names_eventDetect = ['Cz']\n",
    "\n",
    "for id in allsubsdata_perFile:\n",
    "    datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "    datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "    scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=True)\n",
    "\n",
    "    for electrode_name_eventDetect in electrodes_names_eventDetect:\n",
    "        curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name_eventDetect)[0][0]\n",
    "\n",
    "        ## create aux files to use in MATLAB\n",
    "        datafile_1elect_eeg = datafile_data[curr_electrode_num,:]\n",
    "        if 2 not in datafile_scoring or 3 not in datafile_scoring:\n",
    "            continue\n",
    "        \n",
    "        mat_to_save =  {'datafile_data': datafile_1elect_eeg, 'scoring_upsampled': scoring_upsampled, 'sample_freq': configu['sample_freq'], 'electrode_name':electrode_name_eventDetect}\n",
    "        scipy.io.savemat(f\"{andriNir_aux_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\",mat_to_save)\n",
    "\n",
    "        ## run Andrillon & Nir SS detection over all subjects files\n",
    "        eng = matlab.engine.start_matlab()\n",
    "        eng.cd(andriNir_code_dir, nargout=0)\n",
    "        out = eng.batch_useAndrillonNirEventDetection(andriNir_aux_output_dir, andriNir_output_dir,nargout=0)\n",
    "        eng.quit()\n",
    "\n",
    "        ## add the spindles data to the main subject dictionary\n",
    "        try: \n",
    "            spindles_file_name = f\"{andriNir_output_dir}\\\\Spindles_andrillonNir_{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\"\n",
    "            matlabImport = scipy.io.loadmat(spindles_file_name, simplify_cells=True)\n",
    "            print(f\"imported sub file at: {allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}\")\n",
    "        except Exception: \n",
    "            print(f\"Error importing spindles sub file at: {allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}\")\n",
    "            continue\n",
    "        allsubsdata_perFile[id][f'SS_algo-AN@@{electrode_name_eventDetect}'] = matlabImport['spindles'] \n",
    "        allsubsdata_perFile[id][f'SW_algo-AN@@{electrode_name_eventDetect}']  = matlabImport['allWaves']\n",
    "    \n",
    "deleteAuxANFiles(andriNir_aux_output_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excludeNonN23Events():\n",
    "    for id in allsubsdata_perFile:\n",
    "        if 'SS_algo-AN' not in allsubsdata_perFile[id]: continue\n",
    "\n",
    "        ss_ind_for_deletion = np.asarray([],dtype='int')\n",
    "        for ind_ss, ss in enumerate(allsubsdata_perFile[id]['SS_algo-AN']):\n",
    "            if ss[-1] != 2 and ss[-1] != 3:\n",
    "                ss_ind_for_deletion = np.append(ss_ind_for_deletion, ind_ss)\n",
    "\n",
    "        if np.size(ss_ind_for_deletion) ==0: continue\n",
    "\n",
    "        allsubsdata_perFile[id]['SS_algo-AN_n2n3'] = np.delete(allsubsdata_perFile[id]['SS_algo-AN'], ss_ind_for_deletion, axis=0)\n",
    "\n",
    "    for id in allsubsdata_perFile:\n",
    "        if 'SW_algo-AN' not in allsubsdata_perFile[id]: continue\n",
    "\n",
    "        sw_ind_for_deletion = np.asarray([],dtype='int')\n",
    "        for ind_ss, ss in enumerate(allsubsdata_perFile[id]['SW_algo-AN']):\n",
    "            if ss[-1] != 2 and ss[-1] != 3:\n",
    "                sw_ind_for_deletion = np.append(sw_ind_for_deletion, ind_ss)\n",
    "\n",
    "        if np.size(sw_ind_for_deletion) ==0: continue\n",
    "\n",
    "        allsubsdata_perFile[id]['SW_algo-AN_n2n3'] = np.delete(allsubsdata_perFile[id]['SW_algo-AN'], sw_ind_for_deletion, axis=0)\n",
    "\n",
    "#excludeNonN23Events() ## deprecated: names of dic items arent updated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kcomp_algo-manu@@E22\n",
    "# SS_algo-AN_n2n3\n",
    "# SS_algo-manu@@Cz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change scoring to fit EDF_viewer events format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_scoring_to_edfViewer(score):\n",
    "    if score == 0:\n",
    "        return 'W'\n",
    "    elif score ==1:\n",
    "        return 'N1'\n",
    "    elif score ==2:\n",
    "        return 'N2'\n",
    "    elif score ==3:\n",
    "        return 'N3'\n",
    "    elif score ==4:\n",
    "        return 'TREM'\n",
    "    elif score ==5:\n",
    "        return 'PREM'\n",
    "    elif score ==6:\n",
    "        return 'MOVE'\n",
    "    elif score ==7:\n",
    "        return 'ARTIFACT'\n",
    "    else:\n",
    "        Exception('no such score')\n",
    "        \n",
    "\n",
    "for id in allsubsdata_perFile:\n",
    "    curr_file_scoring = allsubsdata_perFile[id]['scoring']\n",
    "    new_format_score = np.zeros((len(curr_file_scoring),3), dtype=object)\n",
    "    for ind, score in enumerate(curr_file_scoring):\n",
    "            new_format_score[ind,:] = [30*ind,30,format_scoring_to_edfViewer(score)] ## onset (sec), duration, desc\n",
    "\n",
    "    allsubsdata_perFile[id]['scoring_efdViewFormat'] = new_format_score\n",
    "    # np.savetxt('test2.txt', new_format_score, delimiter='\\t',fmt='%s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change spindles events to fit EDF_viewer events format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [spindleStartTime spindleEndTime peakTime peakEnergy peakEnergyNorm freqSpindle spindleDuration/SR PowerSP PowerAlpha currentStage];\n",
    "def EDFViewerformat(key_name):\n",
    "        curr_file_ss = allsubsdata_perFile[id][key_name]\n",
    "        new_format_ss = np.zeros((np.shape(curr_file_ss)[0],3), dtype=object)\n",
    "        for ind, ss in enumerate(new_format_ss):\n",
    "                SpindleStartTime = curr_file_ss[ind,0] / np.double(configu['sample_freq'])  \n",
    "                SpindleDuration = (curr_file_ss[ind,1]  -curr_file_ss[ind,0]) / np.double(configu['sample_freq'])  \n",
    "                new_format_ss[ind,:] = [SpindleStartTime,SpindleDuration,key_name] ## onset (sec), duration,desc\n",
    "        allsubsdata_perFile[id][f'{key_name}_efdViewFormat'] = new_format_ss\n",
    "\n",
    "edfViewFormat_events_output_dir = f'{fig_output_dir}\\\\EDFViewFormat_events'\n",
    "if not os.path.exists(edfViewFormat_events_output_dir):\n",
    "    os.mkdir(edfViewFormat_events_output_dir)\n",
    "\n",
    "events_types_for_save = ['SS_algo-AN','SW_algo-AN']\n",
    "for event_type in events_types_for_save: \n",
    "        for id in allsubsdata_perFile:\n",
    "                events_type_found = [ v for k, v in allsubsdata_perFile[id].items() if event_type in k]\n",
    "                if np.size(events_type_found) == 0: \n",
    "                        print(f\"no {event_type} for sub {id}\")\n",
    "                        continue\n",
    "                for key in events_type_found: EDFViewerformat(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save events (scoring and ss) per file, in a format suitable for EDF_viewer\n",
    "for id in allsubsdata_perFile:\n",
    "        filename = f\"{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_events\"\n",
    "        all_events_with_header = np.asarray([['Onset',\"Duration\",\"Annotation\"]],dtype=object)\n",
    "        for event_type in events_types_for_save:\n",
    "                events_type_found = [ v for k, v in allsubsdata_perFile[id].items() if (event_type in k) and ('efdViewFormat' in k)]\n",
    "                if np.size(events_type_found) == 0: continue\n",
    "                for event_type_found in events_type_found:\n",
    "                        all_events_with_header = np.concatenate((all_events_with_header,allsubsdata_perFile[id][event_type_found]),dtype=object)\n",
    "        np.savetxt(f\"{edfViewFormat_events_output_dir}\\\\{filename}.txt\", all_events_with_header, delimiter='\\t',fmt='%s')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f544ce1a915a9875fad91c894e2c0bcad4b7a79945aa6027ef3ad27810072aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
