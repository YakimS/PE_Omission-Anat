{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30233"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.stats\n",
    "import mne\n",
    "from mne.time_frequency import tfr_morlet\n",
    "from mne.stats import permutation_cluster_1samp_test\n",
    "import gc\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "from os.path import exists\n",
    "import mne\n",
    "import numpy as np\n",
    "from mne import create_info\n",
    "from IPython.utils import io\n",
    "import yasa\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matlab\n",
    "import matlab.engine\n",
    "import os\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = \"C:\\\\Users\\\\User\\\\Cloud-Drive\\\\BigFiles\\\\OmissionExpOutput\\\\\\imported_eventDetectionChan\\\\filter0.1\"\n",
    "import_type = \"eventDetectionChan\"\n",
    "output_dir_name = 'eventDetection\\\\done_on_imported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_path = f'{pkl_dir}\\\\{import_type}.pkl'\n",
    "\n",
    "with open(import_path, \"rb\") as file:\n",
    "    [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "\n",
    "fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "if not os.path.exists(fig_output_dir):\n",
    "    os.mkdir(fig_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EMG1', 'EMG2', 'EMG3', 'LOC', 'ROC', 'Fp1', 'Fp2', 'C3', 'C4',\n",
       "       'O1', 'O2', 'F3', 'F4', 'P3', 'P4', 'Cz'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configu['electrodes_names']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One file yasa detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = '32'\n",
    "# datafile = 2\n",
    "# id = f\"{sub}_{datafile}\"\n",
    "\n",
    "# datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "# datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "\n",
    "# exmp_scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=True)\n",
    "# sp = yasa.spindles_detect(datafile_data,sf=configu['sample_freq'], hypno = exmp_scoring_upsampled,include=[2, 3],ch_names=configu['electrodes_names'],multi_only=True)\n",
    "# print(sp)\n",
    "# print(sp.summary())\n",
    "# # sp.plot_average(center='Peak',time_before = 1,time_after = 1)\n",
    "# # plt.show()\n",
    "# #%matplotlib widget\n",
    "# #sp.plot_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# sp.plot_average(center='Peak', time_before=0.1, time_after=0.1);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch yasa detection and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yasa_output_dir = f\"{configu['outputs_dir_path']}/{output_dir_name}/yasa\"\n",
    "# if not os.path.exists(yasa_output_dir):\n",
    "#     os.mkdir(yasa_output_dir)\n",
    "\n",
    "# for id in allsubsdata_perFile:\n",
    "#     datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "#     datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "#     if 2 not in datafile_scoring or 3 not in datafile_scoring:\n",
    "#         continue\n",
    "\n",
    "#     exmp_scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=False)\n",
    "#     sp = yasa.spindles_detect(datafile_data,sf=configu['sample_freq'], hypno = exmp_scoring_upsampled,include=[2, 3],ch_names=configu['electrodes_names'],multi_only=True)\n",
    "#     #summary_df  = sp.summary()\n",
    "#     summary_df.to_csv(f\"{yasa_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_yasaSpindles.csv\")\n",
    "#     sp.plot_average(center='Peak', time_before=1, time_after=1)\n",
    "#     plt.savefig(f\"{yasa_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_yasaSpindles_average.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One file Nir&Andrillon Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matlab\n",
    "# import matlab.engine\n",
    "\n",
    "# sub = '32'\n",
    "# datafile = 2 \n",
    "# id = f\"{sub}_{datafile}\"\n",
    "\n",
    "# datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "# datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "# datafile_data_cz = datafile_data[1,:]\n",
    "# exmp_scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data_cz, sf_data=configu['sample_freq'], verbose=True)\n",
    "\n",
    "# eng = matlab.engine.start_matlab()\n",
    "# eng.SpindlesDetectionAlgorithm_fieldtrip(np.array(datafile_data_cz), configu['sample_freq'], np.array(exmp_scoring_upsampled))\n",
    "\n",
    "# #spindles, spindlesOutEnergy, spindlesOutDuration, rej_cand = eng.SpindlesDetectionAlgorithm_fieldtrip(datafile_data_cz, configu['sample_freq'], exmp_scoring_upsampled)\n",
    "\n",
    "# eng.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Nir&Andrillon Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31-ינו-23 06:20:26 | WARNING | Hypnogram is SHORTER than data by 4.94 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 06:20:33 | WARNING | Hypnogram is SHORTER than data by 26.41 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 06:20:39 | WARNING | Hypnogram is SHORTER than data by 21.62 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 06:20:39 | WARNING | Hypnogram is SHORTER than data by 25.70 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 06:20:46 | WARNING | Hypnogram is SHORTER than data by 2.56 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 06:20:53 | WARNING | Hypnogram is SHORTER than data by 3.10 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 06:20:53 | WARNING | Hypnogram is SHORTER than data by 5.26 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 06:20:53 | WARNING | Hypnogram is SHORTER than data by 23.19 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 06:20:59 | WARNING | Hypnogram is SHORTER than data by 22.21 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 06:21:06 | WARNING | Hypnogram is SHORTER than data by 16.46 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 06:21:06 | WARNING | Hypnogram is SHORTER than data by 0.64 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 06:21:13 | WARNING | Hypnogram is SHORTER than data by 15.76 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 06:21:19 | WARNING | Hypnogram is SHORTER than data by 6.78 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 06:21:26 | WARNING | Hypnogram is SHORTER than data by 23.73 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 06:21:32 | WARNING | Hypnogram is SHORTER than data by 23.01 seconds. Padding hypnogram with last value to match data.size.\n",
      "31-ינו-23 06:21:39 | WARNING | Hypnogram is SHORTER than data by 24.42 seconds. Padding hypnogram with last value to match data.size.\n"
     ]
    }
   ],
   "source": [
    "def deleteAuxANFiles(andriNir_aux_output_dir):\n",
    "    all_files_in_outputDir = os.listdir(andriNir_aux_output_dir)\n",
    "    for file_in_outputDir in all_files_in_outputDir:\n",
    "            os.remove(os.path.join( andriNir_aux_output_dir, file_in_outputDir ))\n",
    "\n",
    "\n",
    "andriNir_code_dir = \"C:\\\\Users\\\\User\\\\Cloud-Drive\\\\AnatArzi\\\\scripts\\\\Matlab_scripts - Sharons\\\\Andrillon_Nir_Algo\"\n",
    "andriNir_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\\\\Andrillon_Nir\"\n",
    "andriNir_aux_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\\\\Andrillon_Nir\\\\aux_mats\"\n",
    "spindles_output_columns =  ['spindleStartTime', 'spindleEndTime', 'peakTime', 'peakEnergy', 'peakEnergyNorm', 'freqSpindle', 'spindleDuration/SR', 'PowerSP', 'PowerAlpha', 'currentStage']\n",
    "electrode_column_name = 'electrode'\n",
    "\n",
    "if not os.path.exists(andriNir_output_dir):\n",
    "    os.mkdir(andriNir_output_dir)\n",
    "\n",
    "electrodes_names_eventDetect = ['F3','F4']#,'C3','C4','P3','P4','O1','O2']\n",
    "\n",
    "all_electodes_ss_key = 'SS_algo-AN_all'\n",
    "\n",
    "for id in allsubsdata_perFile:\n",
    "    datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "    datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "    scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=True)\n",
    "    ss_allElectrodes = pd.DataFrame()\n",
    "\n",
    "    for electd_i, electrode_name_eventDetect in enumerate(electrodes_names_eventDetect):\n",
    "        curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name_eventDetect)[0][0]\n",
    "\n",
    "        ## create aux files to use in MATLAB\n",
    "        datafile_1elect_eeg = datafile_data[curr_electrode_num,:]\n",
    "        if 2 not in datafile_scoring or 3 not in datafile_scoring:\n",
    "            continue\n",
    "        \n",
    "        mat_to_save =  {'datafile_data': datafile_1elect_eeg, 'scoring_upsampled': scoring_upsampled, 'sample_freq': configu['sample_freq'], 'electrode_name':electrode_name_eventDetect}\n",
    "        scipy.io.savemat(f\"{andriNir_aux_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\",mat_to_save)\n",
    "\n",
    "        ## run Andrillon & Nir SS detection over all subjects files\n",
    "        eng = matlab.engine.start_matlab()\n",
    "        eng.cd(andriNir_code_dir, nargout=0)\n",
    "        out = eng.batch_useAndrillonNirEventDetection(andriNir_aux_output_dir, andriNir_output_dir,nargout=0)\n",
    "        eng.quit()\n",
    "\n",
    "        ## add the spindles data to the main subject dictionary\n",
    "        try: \n",
    "            spindles_file_name = f\"{andriNir_output_dir}\\\\Spindles_andrillonNir_{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\"\n",
    "            matlabImport = scipy.io.loadmat(spindles_file_name, simplify_cells=True)\n",
    "        except Exception: \n",
    "            print(f\"Error importing spindles sub file at: {allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}\")\n",
    "            continue\n",
    "\n",
    "        ss = matlabImport['spindles']\n",
    "        if np.size(ss) >0 :\n",
    "            df = pd.DataFrame(np.double(ss))\n",
    "            df.columns = spindles_output_columns\n",
    "            tile_electrode = np.tile(f\"{electrode_name_eventDetect}\", ss.shape[0])[None].T\n",
    "            df[electrode_column_name] = tile_electrode\n",
    "            \n",
    "            allsubsdata_perFile[id][f'SS_algo-AN@@{electrode_name_eventDetect}'] = df\n",
    "            if electd_i ==0: ss_allElectrodes =  df\n",
    "            else:  ss_allElectrodes = pd.concat([ss_allElectrodes,df])\n",
    "\n",
    "        # sw = matlabImport['allWaves']\n",
    "        # if np.size(sw) >0 :\n",
    "        #     tile_electrode = np.tile(f\"{electrode_name_eventDetect}\", sw.shape[0])[None].T\n",
    "        #     sw = np.hstack([sw, tile_electrode])\n",
    "        #     allsubsdata_perFile[id][f'SW_algo-AN@@{electrode_name_eventDetect}'] = np.asarray(sw,dtype=object)  \n",
    "        #     sw_allElectrodes = np.concatenate((sw_allElectrodes,sw))\n",
    " \n",
    "    if not ss_allElectrodes.empty:\n",
    "        allsubsdata_perFile[id][all_electodes_ss_key]  = ss_allElectrodes\n",
    "        #allsubsdata_perFile[id][f'SW_algo-AN_all']  = sw_allElectrodes\n",
    "    \n",
    "deleteAuxANFiles(andriNir_aux_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub:32_1, before excludeNonN2Events:621, after:273\n",
      "sub:32_2, before excludeNonN2Events:824, after:312\n",
      "sub:32_3, before excludeNonN2Events:824, after:312\n",
      "sub:33_1, before excludeNonN2Events:1221, after:702\n",
      "sub:33_2, before excludeNonN2Events:1417, after:1112\n",
      "sub:33_3, before excludeNonN2Events:1417, after:1112\n",
      "sub:33_4, before excludeNonN2Events:1417, after:1112\n",
      "sub:34_1, before excludeNonN2Events:444, after:169\n",
      "sub:34_2, before excludeNonN2Events:460, after:169\n",
      "sub:34_3, before excludeNonN2Events:460, after:169\n",
      "sub:35_1, before excludeNonN2Events:410, after:118\n",
      "sub:35_3, before excludeNonN2Events:847, after:570\n",
      "sub:35_4, before excludeNonN2Events:304, after:124\n",
      "sub:38_1, before excludeNonN2Events:911, after:548\n",
      "sub:38_2, before excludeNonN2Events:1023, after:598\n",
      "sub:38_3, before excludeNonN2Events:757, after:545\n"
     ]
    }
   ],
   "source": [
    "after_n2_exclution_key = f\"{all_electodes_ss_key}_n2\"\n",
    "for id in allsubsdata_perFile:\n",
    "    ss_allElectrodes = allsubsdata_perFile[id][all_electodes_ss_key]\n",
    "    ss_allElectrodes_n2 = ss_allElectrodes.loc[ss_allElectrodes['currentStage'] == 2.0]\n",
    "    if len(ss_allElectrodes_n2) == 0: \n",
    "        continue\n",
    "    else:\n",
    "        print(f\"sub:{id}, before excludeNonN2Events:{len(ss_allElectrodes)}, after:{len(ss_allElectrodes_n2)}\")\n",
    "    allsubsdata_perFile[id][after_n2_exclution_key] = ss_allElectrodes_n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub:32_1, before filt:2419, after:1695\n",
      "sub:32_2, before filt:2231, after:1478\n",
      "sub:32_3, before filt:0, after:0\n",
      "sub:33_1, before filt:2491, after:1699\n",
      "sub:33_2, before filt:3556, after:2265\n",
      "sub:33_3, before filt:0, after:0\n",
      "sub:33_4, before filt:0, after:0\n",
      "sub:34_1, before filt:1363, after:1048\n",
      "sub:34_2, before filt:1667, after:1227\n",
      "sub:34_3, before filt:0, after:0\n",
      "sub:35_1, before filt:1076, after:870\n",
      "sub:35_3, before filt:1742, after:1176\n",
      "sub:35_4, before filt:668, after:465\n",
      "sub:38_1, before filt:2531, after:1647\n",
      "sub:38_2, before filt:3091, after:1929\n",
      "sub:38_3, before filt:2127, after:1372\n"
     ]
    }
   ],
   "source": [
    "## create a spindle array, where each spindle occour once and attribued to the electrode where the spindle was the most powerful\n",
    "\n",
    "ss_key_to_use = all_electodes_ss_key # after_n2_exclution_key\n",
    "uniqeElctd_ss_key = f\"{ss_key_to_use}_uniqeElctd\"\n",
    "\n",
    "def overlap(a, b):\n",
    "    return a[1] >= b[0] and a[0] <= b[1]\n",
    "\n",
    "for id in allsubsdata_perFile:\n",
    "    if ss_key_to_use not in allsubsdata_perFile[id]:  continue\n",
    "    filterd_events_allElectrodes = copy.copy(allsubsdata_perFile[id][ss_key_to_use])\n",
    "    row_deleted = True\n",
    "    k = 0\n",
    "    while row_deleted:\n",
    "        row_deleted = False\n",
    "        for i in np.arange(k,np.shape(filterd_events_allElectrodes)[0]):\n",
    "            i_ss_event = filterd_events_allElectrodes[i]\n",
    "            i_ss_startTime = int(float(i_ss_event[0]))\n",
    "            i_ss_endTime = int(float(i_ss_event[1]))\n",
    "            i_ss_PowerSP = float(i_ss_event[7])\n",
    "            for j in np.arange(i,np.shape(filterd_events_allElectrodes)[0]):\n",
    "                if i == j: continue\n",
    "                j_ss_event = filterd_events_allElectrodes[j]\n",
    "                if overlap([i_ss_startTime,i_ss_endTime],[int(float(j_ss_event[0])),int(float(j_ss_event[1]))]):\n",
    "                    row_deleted = True\n",
    "                    if  i_ss_PowerSP >= float(j_ss_event[7]): # PowerSP\n",
    "                        i_ss_event[-1] = np.append(i_ss_event[-1],j_ss_event[-1]) ## the electrode with the highr event power will be first\n",
    "                        filterd_events_allElectrodes = np.delete(filterd_events_allElectrodes,j,axis=0)\n",
    "                    else:\n",
    "                        j_ss_event[-1] = np.append(j_ss_event[-1],i_ss_event[-1]) ## the electrode with the highr event power will be first\n",
    "                        filterd_events_allElectrodes = np.delete(filterd_events_allElectrodes,i,axis=0)\n",
    "                    break\n",
    "            k+=1\n",
    "            if row_deleted:\n",
    "                break\n",
    "\n",
    "    allsubsdata_perFile[id][uniqeElctd_ss_key] = filterd_events_allElectrodes            \n",
    "    print(f\"sub:{id}, before filt:{np.shape(allsubsdata_perFile[id][ss_key_to_use])[0]}, after:{np.shape(allsubsdata_perFile[id][uniqeElctd_ss_key])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [34], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m j,j_ss_event \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(filterd_events_allElectrodes):\n\u001b[0;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m j: \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     \u001b[39mif\u001b[39;00m overlap([i_ss_startTime,i_ss_endTime],[\u001b[39mint\u001b[39;49m(\u001b[39mfloat\u001b[39;49m(j_ss_event[\u001b[39m0\u001b[39;49m])),\u001b[39mint\u001b[39;49m(\u001b[39mfloat\u001b[39;49m(j_ss_event[\u001b[39m1\u001b[39;49m]))]):\n\u001b[0;32m     22\u001b[0m         row_deleted \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     23\u001b[0m         \u001b[39mif\u001b[39;00m  i_ss_PowerSP \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(j_ss_event[\u001b[39m7\u001b[39m]): \u001b[39m# PowerSP\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [34], line 7\u001b[0m, in \u001b[0;36moverlap\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moverlap\u001b[39m(a, b):\n\u001b[1;32m----> 7\u001b[0m     \u001b[39mreturn\u001b[39;00m a[\u001b[39m1\u001b[39;49m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m b[\u001b[39m0\u001b[39m] \u001b[39mand\u001b[39;00m a[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m b[\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## create a spindle array, where each spindle occour once and attribued to the electrode where the spindle was the most powerful\n",
    "\n",
    "ss_key_to_use = all_electodes_ss_key # after_n2_exclution_key\n",
    "uniqeElctd_ss_key = f\"{ss_key_to_use}_uniqeElctd\"\n",
    "\n",
    "def overlap(a, b):\n",
    "    return a[1] >= b[0] and a[0] <= b[1]\n",
    "\n",
    "for id in allsubsdata_perFile:\n",
    "    if ss_key_to_use not in allsubsdata_perFile[id]:  continue\n",
    "    filterd_events_allElectrodes = copy.copy(allsubsdata_perFile[id][ss_key_to_use])\n",
    "    row_deleted = True\n",
    "    while row_deleted:\n",
    "        row_deleted = False\n",
    "        for i,i_ss_event in enumerate(filterd_events_allElectrodes):\n",
    "            i_ss_startTime = int(float(i_ss_event[0]))\n",
    "            i_ss_endTime = int(float(i_ss_event[1]))\n",
    "            i_ss_PowerSP = float(i_ss_event[7])\n",
    "            for j,j_ss_event in enumerate(filterd_events_allElectrodes):\n",
    "                if i == j: continue\n",
    "                if overlap([i_ss_startTime,i_ss_endTime],[int(float(j_ss_event[0])),int(float(j_ss_event[1]))]):\n",
    "                    row_deleted = True\n",
    "                    if  i_ss_PowerSP >= float(j_ss_event[7]): # PowerSP\n",
    "                        i_ss_event[-1] = np.append(i_ss_event[-1],j_ss_event[-1]) ## the electrode with the highr event power will be first\n",
    "                        filterd_events_allElectrodes = np.delete(filterd_events_allElectrodes,j,axis=0)\n",
    "                    else:\n",
    "                        j_ss_event[-1] = np.append(j_ss_event[-1],i_ss_event[-1]) ## the electrode with the highr event power will be first\n",
    "                        filterd_events_allElectrodes = np.delete(filterd_events_allElectrodes,i,axis=0)\n",
    "                    break\n",
    "            if row_deleted:\n",
    "                break\n",
    "\n",
    "    allsubsdata_perFile[id][uniqeElctd_ss_key] = filterd_events_allElectrodes            \n",
    "    print(f\"sub:{id}, before filt:{np.shape(allsubsdata_perFile[id][ss_key_to_use])[0]}, after:{np.shape(allsubsdata_perFile[id][uniqeElctd_ss_key])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 's'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [31], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mif\u001b[39;00m ss_key_to_use \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allsubsdata_perFile[\u001b[39mid\u001b[39m]:  \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     11\u001b[0m filterd_events_allElectrodes \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mcopy(allsubsdata_perFile[\u001b[39mid\u001b[39m][ss_key_to_use])\n\u001b[1;32m---> 12\u001b[0m filterd_events_allElectrodes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39;49m(filterd_events_allElectrodes, key\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m a_entry: \u001b[39mfloat\u001b[39;49m(a_entry[\u001b[39m0\u001b[39;49m]))  \u001b[39m# sort by startTime\u001b[39;00m\n\u001b[0;32m     14\u001b[0m row_deleted \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mwhile\u001b[39;00m row_deleted:\n",
      "Cell \u001b[1;32mIn [31], line 12\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(a_entry)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mif\u001b[39;00m ss_key_to_use \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allsubsdata_perFile[\u001b[39mid\u001b[39m]:  \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     11\u001b[0m filterd_events_allElectrodes \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mcopy(allsubsdata_perFile[\u001b[39mid\u001b[39m][ss_key_to_use])\n\u001b[1;32m---> 12\u001b[0m filterd_events_allElectrodes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(filterd_events_allElectrodes, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m a_entry: \u001b[39mfloat\u001b[39;49m(a_entry[\u001b[39m0\u001b[39;49m]))  \u001b[39m# sort by startTime\u001b[39;00m\n\u001b[0;32m     14\u001b[0m row_deleted \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mwhile\u001b[39;00m row_deleted:\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 's'"
     ]
    }
   ],
   "source": [
    "## create a spindle array, where each spindle occour once and attribued to the electrode where the spindle was the most powerful\n",
    "\n",
    "ss_key_to_use = after_n2_exclution_key # after_n2_exclution_key\n",
    "uniqeElctd_ss_key = f\"{ss_key_to_use}_uniqeElctd\"\n",
    "\n",
    "def overlap(a, b):\n",
    "    return a[1] >= b[0] and a[0] <= b[1]\n",
    "\n",
    "for id in allsubsdata_perFile:\n",
    "    if ss_key_to_use not in allsubsdata_perFile[id]:  continue\n",
    "    filterd_events_allElectrodes = copy.copy(allsubsdata_perFile[id][ss_key_to_use])\n",
    "    filterd_events_allElectrodes = sorted(filterd_events_allElectrodes, key=lambda a_entry: float(a_entry[0]))  # sort by startTime\n",
    "\n",
    "    row_deleted = True\n",
    "    while row_deleted:\n",
    "        row_deleted = False\n",
    "        for i,i_ss_event in enumerate(filterd_events_allElectrodes):\n",
    "            for j,j_ss_event in enumerate(filterd_events_allElectrodes):\n",
    "                if i == j: continue\n",
    "\n",
    "                i_ss_endTime = float(i_ss_event[1])\n",
    "                j_ss_startTime = float(j_ss_event[0])\n",
    "                if i_ss_endTime < j_ss_startTime: break\n",
    "\n",
    "                i_ss_startTime = float(i_ss_event[0])\n",
    "                j_ss_endTime = float(j_ss_event[1])\n",
    "                i_ss_PowerSP = float(i_ss_event[7])\n",
    "                j_ss_PowerSP = float(j_ss_event[7])\n",
    "                if overlap([i_ss_startTime,i_ss_endTime],[j_ss_startTime,j_ss_endTime]):\n",
    "                    row_deleted = True\n",
    "                    if  i_ss_PowerSP >= j_ss_PowerSP: \n",
    "                        i_ss_event[-1] = np.append(i_ss_event[-1],j_ss_event[-1]) ## the electrode with the highr event power will be first\n",
    "                        filterd_events_allElectrodes = np.delete(filterd_events_allElectrodes,j,axis=0)\n",
    "                    else:\n",
    "                        j_ss_event[-1] = np.append(j_ss_event[-1],i_ss_event[-1]) ## the electrode with the highr event power will be first\n",
    "                        filterd_events_allElectrodes = np.delete(filterd_events_allElectrodes,i,axis=0)\n",
    "                    break\n",
    "            if row_deleted:\n",
    "                break\n",
    "\n",
    "    allsubsdata_perFile[id][uniqeElctd_ss_key] = filterd_events_allElectrodes            \n",
    "    print(f\"sub:{id}, before filt:{np.shape(allsubsdata_perFile[id][ss_key_to_use])[0]}, after:{np.shape(allsubsdata_perFile[id][uniqeElctd_ss_key])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [46], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mwhile\u001b[39;00m np\u001b[39m.\u001b[39mshape(deleted)[\u001b[39m0\u001b[39m]\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[0;32m     19\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39msize(happened_together) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 20\u001b[0m         happened_together \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(happened_together, deleted[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m     21\u001b[0m         deleted \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdelete(deleted,\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m) \u001b[39m# delete first row\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\frame.py:3804\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3804\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3806\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "## create a spindle array, where each spindle occour once and attribued to the electrode where the spindle was the most powerful\n",
    "\n",
    "ss_key_to_use = after_n2_exclution_key # after_n2_exclution_key\n",
    "uniqeElctd_ss_key = f\"{ss_key_to_use}_uniqeElctd\"\n",
    "\n",
    "def overlap(a, b):\n",
    "    return a[1] >= b[0] and a[0] <= b[1]\n",
    "\n",
    "for id in allsubsdata_perFile:\n",
    "    if ss_key_to_use not in allsubsdata_perFile[id]:  continue\n",
    "    filterd_events_allElectrodes = copy.copy(allsubsdata_perFile[id][ss_key_to_use])\n",
    "    filterd_events_allElectrodes = sorted(filterd_events_allElectrodes, key=lambda a_entry: a_entry[0])  # sort by startTime\n",
    "\n",
    "    filtered = np.asarray([])\n",
    "    deleted = filterd_events_allElectrodes = copy.copy(allsubsdata_perFile[id][ss_key_to_use])\n",
    "\n",
    "    happened_together = np.asarray([])\n",
    "    while np.shape(deleted)[0]>0:\n",
    "        if np.size(happened_together) == 0:\n",
    "            happened_together = np.append(happened_together, deleted[0])\n",
    "            deleted = np.delete(deleted,0,0) # delete first row\n",
    "        else:\n",
    "            for i,i_ss_event in enumerate(happened_together):\n",
    "                if overlap([i_ss_startTime,i_ss_endTime],[deleted[0],deleted[1]]):\n",
    "                    happened_together = np.append(happened_together, deleted[0])\n",
    "                    deleted = np.delete(deleted,0,0) # delete first row\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "        ## check max ps and add to filt\n",
    "        happened_together = sorted(happened_together, key=lambda a_entry: float(a_entry[7]))  # sort by PowerSP\n",
    "        max_electd_ss = happened_together[-1]\n",
    "        max_electd_ss[-1] = happened_together[:,-1]\n",
    "        filtered = np.append(max_electd_ss)\n",
    "        happened_together = np.asarray([])\n",
    "\n",
    "    allsubsdata_perFile[id][uniqeElctd_ss_key] = filtered   \n",
    "    print(f\"sub:{id}, before filt:{np.shape(allsubsdata_perFile[id][ss_key_to_use])[0]}, after:{np.shape(allsubsdata_perFile[id][uniqeElctd_ss_key])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a spindle array, where each spindle occour once and attribued to the electrode where the spindle was the most powerful\n",
    "\n",
    "ss_key_to_use = after_n2_exclution_key # after_n2_exclution_key\n",
    "uniqeElctd_ss_key = f\"{ss_key_to_use}_uniqeElctd\"\n",
    "\n",
    "def overlap(a, b):\n",
    "    return a[1] >= b[0] and a[0] <= b[1]\n",
    "\n",
    "for id in allsubsdata_perFile:\n",
    "    if ss_key_to_use not in allsubsdata_perFile[id]:  continue\n",
    "    filterd_events_allElectrodes = allsubsdata_perFile[id][ss_key_to_use].copy(deep=True)\n",
    "    \n",
    "    filterd_events_allElectrodes.sort_values(by=['spindleStartTime'],inplace=True) \n",
    "    deleted = filterd_events_allElectrodes.copy(deep=True)\n",
    "    simultan = pd.DataFrame([])\n",
    "    while len(deleted)>0:\n",
    "        if len(simultan)==0:\n",
    "            simultan = pd.concat([simultan, deleted.iloc[0]])\n",
    "            deleted.drop([0], axis=0, inplace=True)\n",
    "        else:\n",
    "            simultan = simultan.reset_index()  # make sure indexes pair with number of rows\n",
    "            for index, simultan_row in simultan.iterrows():\n",
    "                print(row['c1'], row['c2'])\n",
    "                simultan_0 = simultan_row['spindleStartTime']\n",
    "                simultan_1 = simultan_row['spindleEndTime']\n",
    "                deleted_0 = deleted.iloc[0]['spindleStartTime']\n",
    "                deleted_1 = deleted.iloc[0]['spindleEndTime']\n",
    "                if overlap([simultan_0,simultan_1],[deleted_0,deleted_1]):\n",
    "                    simultan = pd.concat([simultan, deleted.iloc[0]])\n",
    "                    deleted.drop([0], axis=0, inplace=True)\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "        ############3 Im super tired continue from here\n",
    "        ## check max ps and add to filt\n",
    "        simultan.sort_values(by=['PowerSP'],inplace=True) \n",
    "        max_electd_ss = simultan.iloc[0]\n",
    "        max_electd_ss[-1] = simultan[:,-1]\n",
    "        filtered = np.append(max_electd_ss)\n",
    "        simultan = np.asarray([])\n",
    "\n",
    "    allsubsdata_perFile[id][uniqeElctd_ss_key] = filtered   \n",
    "    print(f\"sub:{id}, before filt:{np.shape(allsubsdata_perFile[id][ss_key_to_use])[0]}, after:{np.shape(allsubsdata_perFile[id][uniqeElctd_ss_key])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spindleStartTime      1725777.0\n",
       "spindleEndTime        1726208.0\n",
       "peakTime              1725958.0\n",
       "peakEnergy            26.955765\n",
       "peakEnergyNorm         9.235046\n",
       "freqSpindle           13.972056\n",
       "spindleDuration/SR        0.864\n",
       "PowerSP               10.013066\n",
       "PowerAlpha             7.012449\n",
       "currentStage                2.0\n",
       "electrode                    F3\n",
       "Name: 61, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterd_events_allElectrodes.iloc[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change scoring to fit EDF_viewer events format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_scoring_to_edfViewer(score):\n",
    "    if score == 0:\n",
    "        return 'W'\n",
    "    elif score ==1:\n",
    "        return 'N1'\n",
    "    elif score ==2:\n",
    "        return 'N2'\n",
    "    elif score ==3:\n",
    "        return 'N3'\n",
    "    elif score ==4:\n",
    "        return 'TREM'\n",
    "    elif score ==5:\n",
    "        return 'PREM'\n",
    "    elif score ==6:\n",
    "        return 'MOVE'\n",
    "    elif score ==7:\n",
    "        return 'ARTIFACT'\n",
    "    else:\n",
    "        Exception('no such score')\n",
    "        \n",
    "\n",
    "for id in allsubsdata_perFile:\n",
    "    curr_file_scoring = allsubsdata_perFile[id]['scoring']\n",
    "    new_format_score = np.zeros((len(curr_file_scoring),3), dtype=object)\n",
    "    for ind, score in enumerate(curr_file_scoring):\n",
    "            new_format_score[ind,:] = [30*ind,30,format_scoring_to_edfViewer(score)] ## onset (sec), duration, desc\n",
    "\n",
    "    allsubsdata_perFile[id]['scoring_efdViewFormat'] = new_format_score\n",
    "    # np.savetxt('test2.txt', new_format_score, delimiter='\\t',fmt='%s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change spindles events to fit EDF_viewer events format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [spindleStartTime spindleEndTime peakTime peakEnergy peakEnergyNorm freqSpindle spindleDuration/SR PowerSP PowerAlpha currentStage];\n",
    "def EDFViewerformat(key_name):\n",
    "        curr_file_ss = allsubsdata_perFile[id][key_name]\n",
    "        new_format_ss = np.zeros((np.shape(curr_file_ss)[0],3), dtype=object)\n",
    "        for ind, ss in enumerate(new_format_ss):\n",
    "                SpindleStartTime = np.double(curr_file_ss[ind,0]) / np.double(configu['sample_freq'])  \n",
    "                SpindleDuration = (np.double(curr_file_ss[ind,1])  -np.double(curr_file_ss[ind,0])) / np.double(configu['sample_freq'])\n",
    "                if np.size(curr_file_ss[ind,10]) > 1:  \n",
    "                        event_desc = f\"{key_name.split('_')[0]}@@{curr_file_ss[ind,10][0]}\"\n",
    "                else:\n",
    "                        event_desc = f\"{key_name.split('_')[0]}@@{curr_file_ss[ind,10]}\"\n",
    "                new_format_ss[ind,:] = [SpindleStartTime,SpindleDuration,event_desc] ## onset (sec), duration,desc\n",
    "        allsubsdata_perFile[id][f'{key_name}_efdViewFormat'] = new_format_ss\n",
    "\n",
    "\n",
    "\n",
    "events_types_for_save = [uniqeElctd_ss_key]\n",
    "for event_type in events_types_for_save: \n",
    "        for id in allsubsdata_perFile:\n",
    "                if event_type not in allsubsdata_perFile[id]:\n",
    "                        print(f\"no {event_type} for sub {id}\")\n",
    "                        continue\n",
    "                EDFViewerformat(event_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edfViewFormat_events_output_dir = f'{fig_output_dir}\\\\EDFViewFormat_events'\n",
    "if not os.path.exists(edfViewFormat_events_output_dir):\n",
    "    os.mkdir(edfViewFormat_events_output_dir)\n",
    "\n",
    "events_types_for_save = ['scoring',uniqeElctd_ss_key]\n",
    "# Save events (scoring and ss) per file, in a format suitable for EDF_viewer\n",
    "for id in allsubsdata_perFile:\n",
    "        filename = f\"{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_events\"\n",
    "        all_events_with_header = np.asarray([['Onset',\"Duration\",\"Annotation\"]],dtype=object)\n",
    "        for event_type in events_types_for_save:\n",
    "                events_type_found = [ v for k, v in allsubsdata_perFile[id].items() if (event_type in k) and ('_efdViewFormat' in k)]\n",
    "                if np.size(events_type_found) == 0: continue\n",
    "                for events_found in events_type_found:\n",
    "                        for event_found in events_found:\n",
    "                                all_events_with_header = np.concatenate((all_events_with_header,[event_found]),dtype=object)\n",
    "        np.savetxt(f\"{edfViewFormat_events_output_dir}\\\\{filename}.txt\", all_events_with_header, delimiter='\\t',fmt='%s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test TP / FP for the algorithm of Andrillon-Nir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 3)\n",
      "(204, 3)\n",
      "(737, 3)\n",
      "(235, 3)\n",
      "156\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "## get the array of before manual scanning\n",
    "txt_file_name = \"C:\\\\Users\\\\User\\\\Cloud-Drive\\\\BigFiles\\\\OmissionExpOutput\\\\eventDetection\\\\done_on_preProcessed\\\\EDFViewFormat_events\\\\32_2_events.txt\"\n",
    "before = np.loadtxt(txt_file_name, delimiter=\"\\t\",dtype='object')\n",
    "print(np.shape(before))\n",
    "before[1:,0] = [np.double(x) for x in before[1:,0]]\n",
    "before = np.delete(before,0,0)\n",
    "ss_ind = np.array([],dtype=int)\n",
    "for ind_i, desc in enumerate(before[:,2]):\n",
    "    if \"SS\" in desc:\n",
    "        ss_ind = np.append(ss_ind, ind_i)\n",
    "before_ss = before[ss_ind,:]\n",
    "print(np.shape(before_ss))\n",
    "\n",
    "## get the array of after manual scanning\n",
    "txt_file_name = \"C:\\\\Users\\\\User\\\\Cloud-Drive\\\\BigFiles\\\\OmissionExpOutput\\\\eventDetection\\\\done_on_preProcessed\\\\EDFViewFormat_events\\\\s_32_sleep2_preProcessed_annotations.txt\"\n",
    "after = np.loadtxt(txt_file_name, delimiter=\"\\t\",dtype='object')\n",
    "print(np.shape(after))\n",
    "after[1:,0] = [np.double(x) for x in after[1:,0]]\n",
    "\n",
    "ss_ind = np.array([],dtype=int)\n",
    "for ind_i, desc in enumerate(after[:,2]):\n",
    "    if \"SS\" in desc:\n",
    "        ss_ind = np.append(ss_ind, ind_i)\n",
    "after_ss = after[ss_ind,:]\n",
    "print(np.shape(after_ss))\n",
    "\n",
    "## compare to find rate od TP and FP\n",
    "TP = 0\n",
    "FP = 0\n",
    "for ss_auto in before_ss:\n",
    "    found = False\n",
    "    for ss_manu in after_ss:\n",
    "        if ss_auto[0] == ss_manu[0]:\n",
    "            TP +=1\n",
    "            found = True\n",
    "            break\n",
    "    if found == False:\n",
    "        FP +=1\n",
    "\n",
    "print(TP)\n",
    "print(FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f544ce1a915a9875fad91c894e2c0bcad4b7a79945aa6027ef3ad27810072aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
