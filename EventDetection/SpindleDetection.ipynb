{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.stats\n",
    "import mne\n",
    "from mne.time_frequency import tfr_morlet\n",
    "from mne.stats import permutation_cluster_1samp_test\n",
    "import gc\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "from os.path import exists\n",
    "import mne\n",
    "import numpy as np\n",
    "from mne import create_info\n",
    "from IPython.utils import io\n",
    "import yasa\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matlab\n",
    "import matlab.engine\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def overlap(a, b):\n",
    "    return a[1] >= b[0] and a[0] <= b[1]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = \"C:\\\\Users\\\\User\\\\Cloud-Drive\\\\BigFiles\\\\OmissionExpOutput\\\\\\imported_eventDetectionChan\\\\filter0.35\"\n",
    "import_type = \"eventDetectionChan\"\n",
    "output_dir_name = 'eventDetection\\\\done_on_imported'\n",
    "import_path = f'{pkl_dir}\\\\{import_type}.pkl'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Nir&Andrillon Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05-פבר-23 20:38:03 | WARNING | Hypnogram is SHORTER than data by 23.01 seconds. Padding hypnogram with last value to match data.size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4 10  1]\n",
      "sub:38_2, before exclude sleep stages:537, after:375\n",
      "sub:38_2, before filt:375, after:278, after>1: after:95\n"
     ]
    }
   ],
   "source": [
    "electrode_column_name = 'electrode'\n",
    "header = np.array(['Onset',\"Duration\",\"Annotation\"])\n",
    "\n",
    "def detect_ss_AndrillonNir(file_ids,minMax_sd_ver,output_key,electrodes_names):\n",
    "    andriNir_code_dir = os.getcwd()\n",
    "    andriNir_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\\\\Andrillon_Nir\"\n",
    "    andriNir_aux_output_dir = f\"{andriNir_output_dir}\\\\aux_mats\"\n",
    "\n",
    "    if os.path.exists(andriNir_output_dir):\n",
    "        shutil.rmtree(andriNir_output_dir)\n",
    "    os.mkdir(andriNir_output_dir)\n",
    "    os.mkdir(andriNir_aux_output_dir)\n",
    "\n",
    "    spindles_output_columns =  ['spindleStartTime', 'spindleEndTime', 'peakTime', 'peakEnergy', 'peakEnergyNorm', 'freqSpindle', 'spindleDuration/SR', 'PowerSP', 'PowerAlpha', 'currentStage']\n",
    "\n",
    "    for id in file_ids:\n",
    "        datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "        datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "        scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=True)\n",
    "        ss_allElectrodes = pd.DataFrame()\n",
    "\n",
    "        for electd_i, electrode_name_eventDetect in enumerate(electrodes_names):\n",
    "            curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name_eventDetect)[0][0]\n",
    "\n",
    "            ## create aux files to use in MATLAB\n",
    "            datafile_1elect_eeg = datafile_data[curr_electrode_num,:]\n",
    "            if 2 not in datafile_scoring or 3 not in datafile_scoring:\n",
    "                continue\n",
    "            \n",
    "            DetectionThreshold = minMax_sd_ver[0]\n",
    "            RejectThreshold = minMax_sd_ver[1]\n",
    "            StartEndThreshold = minMax_sd_ver[2]\n",
    "            mat_to_save = {'datafile_data': datafile_1elect_eeg, 'scoring_upsampled': scoring_upsampled, 'sample_freq': configu['sample_freq'], 'electrode_name':electrode_name_eventDetect, 'DetectionThreshold':DetectionThreshold,'RejectThreshold':RejectThreshold, 'StartEndThreshold':StartEndThreshold}\n",
    "            scipy.io.savemat(f\"{andriNir_aux_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\",mat_to_save)\n",
    "\n",
    "            ## run Andrillon & Nir SS detection over all subjects files\n",
    "            eng = matlab.engine.start_matlab()\n",
    "            eng.cd(andriNir_code_dir, nargout=0)\n",
    "            out = eng.batch_useAndrillonNirSSDetection(andriNir_aux_output_dir, andriNir_output_dir,nargout=0)\n",
    "            eng.quit()\n",
    "\n",
    "            ## add the spindles data to the main subject dictionary\n",
    "            try: \n",
    "                spindles_file_name = f\"{andriNir_output_dir}\\\\SS_AN_{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\"\n",
    "                matlabImport = scipy.io.loadmat(spindles_file_name, simplify_cells=True)\n",
    "            except Exception: \n",
    "                print(f\"Error importing spindles sub file at: {allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}\")\n",
    "                continue\n",
    "\n",
    "            ss = matlabImport['spindles']\n",
    "            if np.size(np.shape(ss)) == 1: \n",
    "                temp = np.zeros((1,np.size(ss)),dtype=object)\n",
    "                temp[0] = ss\n",
    "                ss = temp\n",
    "            if np.size(ss) >0 :\n",
    "                df = pd.DataFrame(np.double(ss))\n",
    "                df.columns = spindles_output_columns\n",
    "                tile_electrode = np.tile(f\"{electrode_name_eventDetect}\", ss.shape[0])[None].T\n",
    "                df[electrode_column_name] = tile_electrode\n",
    "                \n",
    "                allsubsdata_perFile[id][f'{output_key}@@{electrode_name_eventDetect}'] = df\n",
    "                if electd_i ==0: ss_allElectrodes =  df\n",
    "                else:  ss_allElectrodes = pd.concat([ss_allElectrodes,df])\n",
    "\n",
    "            # sw = matlabImport['allWaves']\n",
    "            # if np.size(sw) >0 :\n",
    "            #     tile_electrode = np.tile(f\"{electrode_name_eventDetect}\", sw.shape[0])[None].T\n",
    "            #     sw = np.hstack([sw, tile_electrode])\n",
    "            #     allsubsdata_perFile[id][f'SW_algo-AN@@{electrode_name_eventDetect}'] = np.asarray(sw,dtype=object)  \n",
    "            #     sw_allElectrodes = np.concatenate((sw_allElectrodes,sw))\n",
    "    \n",
    "        if not ss_allElectrodes.empty:\n",
    "            #print(f\"sub:{id}, thresh:{matlabImport['minMax_SD_threshold']},  before {len(ss_allElectrodes)}\")\n",
    "            allsubsdata_perFile[id][output_key]  = ss_allElectrodes\n",
    "            #allsubsdata_perFile[id][f'SW_algo-AN_all']  = sw_allElectrodes\n",
    "\n",
    "    minmax_sd_name =  matlabImport['minMax_SD_threshold']\n",
    "    shutil.rmtree(andriNir_aux_output_dir)\n",
    "    return minmax_sd_name\n",
    "def filter_sleep_stages(file_ids,event_key_to_use,after_sleepStage_exclution_key, sleepstages):\n",
    "    for id in file_ids:\n",
    "        if event_key_to_use not in allsubsdata_perFile[id]: continue\n",
    "        event_allElectrodes = allsubsdata_perFile[id][event_key_to_use]\n",
    "        \n",
    "        event_filtered_sleepstages = pd.DataFrame([], columns=event_allElectrodes.columns)\n",
    "        for sleepstage in sleepstages:\n",
    "            event_filtered_sleepstages = pd.concat([event_filtered_sleepstages , event_allElectrodes.loc[(event_allElectrodes['currentStage'] == sleepstage)]])\n",
    "\n",
    "        if len(event_filtered_sleepstages) == 0:  print(f\"sub:{id} - no events in sleepstages{sleepstages}\")\n",
    "        else: print(f\"sub:{id}, before exclude sleep stages:{len(event_allElectrodes)}, after:{len(event_filtered_sleepstages)}\")\n",
    "        \n",
    "        allsubsdata_perFile[id][after_sleepStage_exclution_key] = event_filtered_sleepstages\n",
    "def group_spindles(ss_key_to_use, uniqeElctds_ss_key, uniqeElctd_ss_key):\n",
    "    ## create a spindle array, where each spindle occour once and attribued to the electrode where the spindle was the most powerful\n",
    "    for id in allsubsdata_perFile:\n",
    "        if ss_key_to_use not in allsubsdata_perFile[id]:  continue\n",
    "        filterd_events_allElectrodes = allsubsdata_perFile[id][ss_key_to_use].copy(deep=True)\n",
    "        \n",
    "        filterd_events_allElectrodes.sort_values(by=['spindleStartTime'],inplace=True) \n",
    "        filterd_events_allElectrodes.reset_index(drop=True, inplace=True)\n",
    "        deleted = filterd_events_allElectrodes.copy(deep=True)\n",
    "        filtered = pd.DataFrame([], columns = deleted.columns)\n",
    "        filtered_moreThan1 = pd.DataFrame([], columns = deleted.columns)\n",
    "        simultan = pd.DataFrame([], columns = deleted.columns)\n",
    "\n",
    "        while len(deleted)>0:\n",
    "            if len(simultan)==0:\n",
    "                simultan = pd.concat([simultan, deleted.iloc[[0]]])\n",
    "                deleted.drop(deleted.index[0], axis=0, inplace=True)\n",
    "                still_overlap = True\n",
    "            else:\n",
    "                while still_overlap == True and len(deleted)>0:\n",
    "                    still_overlap = False\n",
    "                    simultan.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "\n",
    "                    ## check now_overlap_all_in_simultan:\n",
    "                    for index, simultan_row in simultan.iterrows():\n",
    "                        simultan_0 = simultan_row['spindleStartTime']\n",
    "                        simultan_1 = simultan_row['spindleEndTime']\n",
    "                        deleted_0 = deleted.iloc[0]['spindleStartTime']\n",
    "                        deleted_1 = deleted.iloc[0]['spindleEndTime']\n",
    "                        if overlap([simultan_0,simultan_1],[deleted_0,deleted_1]):\n",
    "                            simultan = pd.concat([simultan, deleted.iloc[[0]]])\n",
    "                            deleted.drop(deleted.index[0], axis=0, inplace=True)\n",
    "                            still_overlap = True\n",
    "                            break\n",
    "                    if still_overlap: continue\n",
    "                    else:\n",
    "                        ## check max ps and add to filt\n",
    "                        # if len(simultan)>1:\n",
    "                        #     print('hu')\n",
    "                        simultan.sort_values(by=['PowerSP'],ascending=False,inplace=True)\n",
    "                        row_df = simultan.iloc[[0]]\n",
    "                        row_df.iat[0, row_df.columns.get_loc(electrode_column_name)] = np.array(simultan[electrode_column_name])\n",
    "                        filtered = pd.concat([filtered,row_df])\n",
    "                        if len(simultan) > 1: filtered_moreThan1 = pd.concat([filtered_moreThan1,row_df])\n",
    "                        simultan = pd.DataFrame([], columns = deleted.columns)                    \n",
    "            \n",
    "        filtered.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "        filtered_moreThan1.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "        allsubsdata_perFile[id][uniqeElctd_ss_key] = filtered   \n",
    "        allsubsdata_perFile[id][uniqeElctds_ss_key] = filtered_moreThan1   \n",
    "        print(f\"sub:{id}, before filt:{np.shape(allsubsdata_perFile[id][ss_key_to_use])[0]}, after:{np.shape(allsubsdata_perFile[id][uniqeElctd_ss_key])[0]}, after>1: after:{np.shape(allsubsdata_perFile[id][uniqeElctds_ss_key])[0]}\")\n",
    "\n",
    "def edfViewFormat_scoring_dict(score):\n",
    "    if score == 0:\n",
    "        return 'W'\n",
    "    elif score ==1:\n",
    "        return 'N1'\n",
    "    elif score ==2:\n",
    "        return 'N2'\n",
    "    elif score ==3:\n",
    "        return 'N3'\n",
    "    elif score ==4:\n",
    "        return 'TREM'\n",
    "    elif score ==5:\n",
    "        return 'PREM'\n",
    "    elif score ==6:\n",
    "        return 'MOVE'\n",
    "    elif score ==7:\n",
    "        return 'ARTIFACT'\n",
    "    else:\n",
    "        Exception('no such score')\n",
    "def add_edfViewFormat_scoring(key_edfScoringFormat):\n",
    "    for id in allsubsdata_perFile:\n",
    "        curr_file_scoring = allsubsdata_perFile[id]['scoring']\n",
    "        new_format_score = np.zeros((len(curr_file_scoring),3), dtype=object)\n",
    "        for ind, score in enumerate(curr_file_scoring):\n",
    "                new_format_score[ind,:] = [30*ind,30,edfViewFormat_scoring_dict(score)] ## onset (sec), duration, desc\n",
    "\n",
    "        allsubsdata_perFile[id][key_edfScoringFormat] = new_format_score\n",
    "def add_edfViewFormat_ss(file_ids,event_key_for_save, SS_efdViewFormat_key):\n",
    "        for id in file_ids:\n",
    "            if event_key_for_save not in allsubsdata_perFile[id]:\n",
    "                    #print(f\"no {event_key_for_save} for sub {id}\")\n",
    "                    continue\n",
    "            ss_df = allsubsdata_perFile[id][event_key_for_save]\n",
    "            ss_mat_edfFormat = np.zeros((len(ss_df),3), dtype=object)\n",
    "            startTime_arr = np.array(ss_df['spindleStartTime']) / np.double(configu['sample_freq'])\n",
    "            endTime_arr = np.array(ss_df['spindleEndTime']) / np.double(configu['sample_freq'])\n",
    "            duration_arr = endTime_arr - startTime_arr\n",
    "            electd_arr_per_ss = np.array(ss_df[electrode_column_name])\n",
    "            desc = [f\"SS@@{electd_arr[0]}\" for electd_arr in electd_arr_per_ss]\n",
    "            new_format_ss = np.array([startTime_arr,duration_arr,desc]).T ## onset (sec), duration,desc\n",
    "            allsubsdata_perFile[id][SS_efdViewFormat_key] = new_format_ss\n",
    "\n",
    "def save_eventsSepratelyForComparison_edfViewFormat(file_ids,events_types_to_compare,edfViewFormat_output_dir):\n",
    "    # Save events (scoring and ss) per file, in a format suitable for EDF_viewer\n",
    "    for id in file_ids:\n",
    "        for event_type in events_types_to_compare:\n",
    "            filename = f\"{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_events\"\n",
    "            all_events_with_header = np.asarray([header],dtype=object)\n",
    "            events_type_found = [ v for k, v in allsubsdata_perFile[id].items() if ((event_type in k) and (edfviewFormatSuffix in k))]\n",
    "            if np.size(events_type_found) == 0: continue\n",
    "            for events_found in events_type_found:\n",
    "                    for event_found in events_found:\n",
    "                            all_events_with_header = np.concatenate((all_events_with_header,[event_found]),dtype=object)\n",
    "            np.savetxt(f\"{edfViewFormat_output_dir}\\\\{event_type}_{filename}.txt\", all_events_with_header, delimiter='\\t',fmt='%s')\n",
    "    \n",
    "# Save events (scoring and ss) per file, in a format suitable for EDF_viewer\n",
    "def save_eventsAllTogether_edfViewFormat(events_types_for_save,edfViewFormat_output_dir):\n",
    "        for id in file_ids:\n",
    "            filename = f\"{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_events\"\n",
    "            all_events_with_header = np.asarray([header],dtype=object)\n",
    "            for event_type in events_types_for_save:\n",
    "                events_type_found = [ v for k, v in allsubsdata_perFile[id].items() if (event_type in k) and (edfviewFormatSuffix in k)]\n",
    "                if np.size(events_type_found) == 0: continue\n",
    "                for events_found in events_type_found:\n",
    "                        for event_found in events_found:\n",
    "                                all_events_with_header = np.concatenate((all_events_with_header,[event_found]),dtype=object)\n",
    "            np.savetxt(f\"{edfViewFormat_output_dir}\\\\allEvents_{filename}.txt\", all_events_with_header, delimiter='\\t',fmt='%s')\n",
    "\n",
    "testType = 1\n",
    "if(testType==1):\n",
    "    file_ids = ['38_2']\n",
    "    minMax_sd_vers = [[4,10,1]]\n",
    "    electrodes_names_eventDetect = ['Fp2','F3']\n",
    "elif(testType==2):\n",
    "    file_ids = ['32_2','38_2']\n",
    "    minMax_sd_vers = [[3,9,1],[4,10,1]]\n",
    "    electrodes_names_eventDetect = ['Fp2','F3']\n",
    "elif(testType==3):\n",
    "    file_ids = ['32_2','38_2']\n",
    "    minMax_sd_vers = [[3,9,1],[4,10,1]]\n",
    "    electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "else:\n",
    "    file_ids = ['32_2','38_2']\n",
    "    electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "    minMax_sd_vers = [[3,8,1],[4,8,1],[5,8,1],[3,9,1],[4,9,1],[5,9,1],[3,10,1],[4,10,1],[5,10,1]]\n",
    "    #file_ids = allsubsdata_perFile.keys()\n",
    "\n",
    "with open(import_path, \"rb\") as file:\n",
    "    [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "if not os.path.exists(fig_output_dir): os.mkdir(fig_output_dir)\n",
    "\n",
    "edfViewFormat_events_manual_dir = f'{fig_output_dir}\\\\EDFViewFormat_events'\n",
    "edfViewFormat_events_output_dir = f'{fig_output_dir}\\\\EDFViewFormat_events\\\\minmax_sd_tests'\n",
    "if os.path.exists(edfViewFormat_events_output_dir):\n",
    "    shutil.rmtree(edfViewFormat_events_output_dir)\n",
    "os.mkdir(edfViewFormat_events_output_dir)\n",
    "\n",
    "fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "if not os.path.exists(fig_output_dir): os.mkdir(fig_output_dir)\n",
    "\n",
    "for minMax_sd_ver in minMax_sd_vers:\n",
    "    with open(import_path, \"rb\") as file: [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "    \n",
    "    all_electodes_ss_key = 'SS_AN'\n",
    "    minmax_sd_name = detect_ss_AndrillonNir(file_ids,minMax_sd_ver,all_electodes_ss_key,electrodes_names_eventDetect)\n",
    "    print(minmax_sd_name)\n",
    "\n",
    "    after_sleepStage_exclution_key = f\"{all_electodes_ss_key}_n2n3\"\n",
    "    filter_sleep_stages(file_ids,all_electodes_ss_key,after_sleepStage_exclution_key, [2.0,3.0]) \n",
    "\n",
    "    ss_key_to_use = after_sleepStage_exclution_key\n",
    "    multiElectdPerSS_text = \"multiElectd\"\n",
    "    singleElectdPerSS_text = \"singleElectd\"\n",
    "    multiElectdPerSS_key = f\"{ss_key_to_use}_{multiElectdPerSS_text}\"\n",
    "    singleElectdPerSS_key = f\"{ss_key_to_use}_{singleElectdPerSS_text}\"\n",
    "    group_spindles(ss_key_to_use,multiElectdPerSS_key, singleElectdPerSS_key)\n",
    "\n",
    "    edfviewFormatSuffix = 'efdViewFormat'\n",
    "    SS_multiElectdPerSS_efdViewFormat_key = f'{multiElectdPerSS_key}_{edfviewFormatSuffix}'\n",
    "    SS_singleElectdPerSS_efdViewFormat_key = f'{singleElectdPerSS_key}_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_ss(file_ids,multiElectdPerSS_key,SS_multiElectdPerSS_efdViewFormat_key)\n",
    "    add_edfViewFormat_ss(file_ids,singleElectdPerSS_key,SS_singleElectdPerSS_efdViewFormat_key)\n",
    "\n",
    "    events_types_to_compare = [multiElectdPerSS_text,singleElectdPerSS_text]    \n",
    "    edfViewFormat_eventsTest_output_dir = f\"{edfViewFormat_events_output_dir}\\\\{minMax_sd_ver}\"\n",
    "    if os.path.exists(edfViewFormat_eventsTest_output_dir):  shutil.rmtree(edfViewFormat_eventsTest_output_dir)\n",
    "    os.mkdir(edfViewFormat_eventsTest_output_dir)\n",
    "    save_eventsSepratelyForComparison_edfViewFormat(file_ids,events_types_to_compare,edfViewFormat_eventsTest_output_dir)\n",
    "\n",
    "    events_types_to_save = ['scoring', multiElectdPerSS_text,singleElectdPerSS_text]    \n",
    "    save_eventsAllTogether_edfViewFormat(events_types_to_save,edfViewFormat_eventsTest_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "#display(HTML(allsubsdata_perFile[file_ids[0]][singleElectdPerSS_key].to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>eventType</th>\n",
       "      <th>minmax_ver</th>\n",
       "      <th>Positive</th>\n",
       "      <th>TruePos</th>\n",
       "      <th>FalsePos</th>\n",
       "      <th>hitRate</th>\n",
       "      <th>falseDiscoveryRate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38_2</td>\n",
       "      <td>multiElectd</td>\n",
       "      <td>[4, 10, 1]</td>\n",
       "      <td>448</td>\n",
       "      <td>89</td>\n",
       "      <td>6</td>\n",
       "      <td>0.198661</td>\n",
       "      <td>0.063158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38_2</td>\n",
       "      <td>singleElectd</td>\n",
       "      <td>[4, 10, 1]</td>\n",
       "      <td>448</td>\n",
       "      <td>254</td>\n",
       "      <td>24</td>\n",
       "      <td>0.566964</td>\n",
       "      <td>0.086331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id     eventType  minmax_ver  Positive  TruePos  FalsePos   hitRate  \\\n",
       "0  38_2   multiElectd  [4, 10, 1]       448       89         6  0.198661   \n",
       "1  38_2  singleElectd  [4, 10, 1]       448      254        24  0.566964   \n",
       "\n",
       "   falseDiscoveryRate  \n",
       "0            0.063158  \n",
       "1            0.086331  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preform_minmaxSD_comparison(events_types_to_compare,file_ids,minMax_sd_vers):\n",
    "    all_comparisons = pd.DataFrame(columns=['id','eventType','minmax_ver','Positive','TruePos','FalsePos', 'hitRate','falseDiscoveryRate'])\n",
    "    for id in file_ids:\n",
    "        for ver in minMax_sd_vers:\n",
    "            for event_type in events_types_to_compare:\n",
    "                dir_auto_ = f\"{edfViewFormat_events_output_dir}\\\\{ver}\"\n",
    "                manual_detection_filename = f\"{edfViewFormat_events_manual_dir}\\\\sub{id.split('_')[0]}_sleep{id.split('_')[1]}_imported_eventDetectionChan_annotations.txt\"\n",
    "                auto_detection_filename = f\"{dir_auto_}\\\\{event_type}_{id}_events.txt\"\n",
    "                ## get the array of before manual scanning\n",
    "                if os.path.exists(auto_detection_filename) and os.path.exists(manual_detection_filename):\n",
    "                    all_ss_auto = np.loadtxt(auto_detection_filename, delimiter=\"\\t\",dtype='object')\n",
    "                    all_ss_auto = np.delete(all_ss_auto, np.where(all_ss_auto[:, 2] == \"Annotation\")[0], axis=0)\n",
    "                    all_ss_auto[:,[0,1]] = [np.double(x) for x in all_ss_auto[:,[0,1]]]\n",
    "                    ss_ind = np.array([],dtype=int)\n",
    "                    for ind_i, desc in enumerate(all_ss_auto[:,2]):\n",
    "                        if (\"SS\" in desc) or (\"ss\" in desc):\n",
    "                            ss_ind = np.append(ss_ind, ind_i)\n",
    "                    all_ss_auto = all_ss_auto[ss_ind,:]\n",
    "\n",
    "                    ## get the array of after manual scanning\n",
    "                    all_ss_manu = np.loadtxt(manual_detection_filename, delimiter=\"\\t\",dtype='object')\n",
    "                    all_ss_manu = np.delete(all_ss_manu, np.where(all_ss_manu[:, 2] == \"Annotation\")[0], axis=0)\n",
    "                    all_ss_manu[:,[0,1]] = [np.double(x) for x in all_ss_manu[:,[0,1]]]\n",
    "                    ss_ind = np.array([],dtype=int)\n",
    "                    for ind_i, desc in enumerate(all_ss_manu[:,2]):\n",
    "                        if \"SS\" in desc:\n",
    "                            ss_ind = np.append(ss_ind, ind_i)\n",
    "                    all_ss_manu = all_ss_manu[ss_ind,:]\n",
    "\n",
    "                    ## compare to find rate od TP and FP\n",
    "                    TP = 0\n",
    "                    FP = 0\n",
    "                    for ss_auto in all_ss_auto:\n",
    "                        found = False\n",
    "                        for ss_manu in all_ss_manu:\n",
    "                            if overlap([ss_auto[0],ss_auto[0]+ss_auto[1]],[ss_manu[0],ss_manu[0]+ss_manu[1]]):\n",
    "                                TP +=1\n",
    "                                found = True\n",
    "                                break\n",
    "                        if found == False:\n",
    "                            FP +=1\n",
    "\n",
    "                    # FN = 0\n",
    "                    # for ss_manu in all_ss_manu:\n",
    "                    #     found = False\n",
    "                    #     for ss_auto in all_ss_auto:\n",
    "                    #         if overlap([ss_auto[0],ss_auto[0]+ss_auto[1]],[ss_manu[0],ss_manu[0]+ss_manu[1]]):\n",
    "                    #             found = True\n",
    "                    #             break\n",
    "                    #     if found == False:\n",
    "                    #         FN +=1\n",
    "\n",
    "                    Positive = np.shape(all_ss_manu)[0]   \n",
    "                    hitRate = TP/Positive\n",
    "                    #missRate = FN/Positive\n",
    "                    falseDiscoveryRate = FP/(FP+TP)\n",
    "                    \n",
    "                    comparison = [id,event_type, ver, Positive, TP, FP,hitRate, falseDiscoveryRate]\n",
    "                    all_comparisons.loc[len(all_comparisons)] = comparison\n",
    "\n",
    "    all_comparisons = all_comparisons.sort_values('minmax_ver')\n",
    "    all_comparisons = all_comparisons.sort_values('id')\n",
    "    return all_comparisons\n",
    "    \n",
    "events_types_to_compare = [multiElectdPerSS_text,singleElectdPerSS_text]\n",
    "all_comp = preform_minmaxSD_comparison(events_types_to_compare,file_ids,minMax_sd_vers)\n",
    "all_comp.to_csv('ss_comparisons.csv',index=False)\n",
    "all_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deleted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [70], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m## how to df \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[39m## add row to df and empty df\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m x \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame([], columns \u001b[39m=\u001b[39m deleted\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m      5\u001b[0m y\u001b[39m=\u001b[39m deleted\u001b[39m.\u001b[39miloc[[\u001b[39m0\u001b[39m]]\n\u001b[0;32m      6\u001b[0m y2\u001b[39m=\u001b[39m deleted\u001b[39m.\u001b[39miloc[[\u001b[39m1\u001b[39m]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'deleted' is not defined"
     ]
    }
   ],
   "source": [
    "## how to df \n",
    "\n",
    "## add row to df and empty df\n",
    "x = pd.DataFrame([], columns = deleted.columns)\n",
    "y= deleted.iloc[[0]]\n",
    "y2= deleted.iloc[[1]]\n",
    "z = pd.concat([x,y])\n",
    "zz = pd.concat([z,y2])\n",
    "display(zz)\n",
    "\n",
    "## delete first row\n",
    "# zz.drop(zz.index[0], axis=0, inplace=True) \n",
    "# display(zz)\n",
    "\n",
    "## get value at i,j\n",
    "print(deleted.iloc[0]['spindleStartTime'])\n",
    "\n",
    "## get column as array\n",
    "arr = np.array(zz['electrode'])\n",
    "print(arr)\n",
    "\n",
    "## replace inplace value in i,j to array:\n",
    "zz.iat[1, zz.columns.get_loc(electrode_column_name)] = arr\n",
    "zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f544ce1a915a9875fad91c894e2c0bcad4b7a79945aa6027ef3ad27810072aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
