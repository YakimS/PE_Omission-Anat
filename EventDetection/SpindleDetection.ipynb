{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.stats\n",
    "import mne\n",
    "from mne.time_frequency import tfr_morlet\n",
    "from mne.stats import permutation_cluster_1samp_test\n",
    "import gc\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "from os.path import exists\n",
    "import mne\n",
    "import numpy as np\n",
    "from mne import create_info\n",
    "from IPython.utils import io\n",
    "import yasa\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matlab\n",
    "import matlab.engine\n",
    "import os\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = \"C:\\\\Users\\\\User\\\\Cloud-Drive\\\\BigFiles\\\\OmissionExpOutput\\\\\\imported_eventDetectionChan\\\\filter0.35\"\n",
    "import_type = \"eventDetectionChan\"\n",
    "output_dir_name = 'eventDetection\\\\done_on_imported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EMG1', 'EMG2', 'EMG3', 'LOC', 'ROC', 'Fp1', 'Fp2', 'C3', 'C4',\n",
       "       'O1', 'O2', 'F3', 'F4', 'P3', 'P4', 'Cz'], dtype=object)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import_path = f'{pkl_dir}\\\\{import_type}.pkl'\n",
    "\n",
    "with open(import_path, \"rb\") as file:\n",
    "    [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "\n",
    "fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "if not os.path.exists(fig_output_dir):\n",
    "    os.mkdir(fig_output_dir)\n",
    "\n",
    "configu['electrodes_names']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Nir&Andrillon Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Cloud-Drive\\\\BigFiles\\\\OmissionExpOutput\\\\eventDetection\\\\done_on_imported\\\\Andrillon_Nir'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "andriNir_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Access is denied: 'C:\\\\Users\\\\User\\\\Cloud-Drive\\\\BigFiles\\\\OmissionExpOutput\\\\eventDetection\\\\done_on_imported\\\\Andrillon_Nir\\\\aux_mats'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [103], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     os\u001b[39m.\u001b[39mmkdir(andriNir_output_dir)\n\u001b[0;32m     16\u001b[0m deleteAuxANFiles(andriNir_aux_output_dir)\n\u001b[1;32m---> 17\u001b[0m deleteAuxANFiles(andriNir_output_dir)\n\u001b[0;32m     19\u001b[0m electrodes_names_eventDetect \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mF3\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mF4\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mC3\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mC4\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mP3\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mP4\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mO1\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mO2\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     21\u001b[0m all_electodes_ss_key \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSS_algo-AN_all\u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[1;32mIn [103], line 4\u001b[0m, in \u001b[0;36mdeleteAuxANFiles\u001b[1;34m(andriNir_aux_output_dir)\u001b[0m\n\u001b[0;32m      2\u001b[0m all_files_in_outputDir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(andriNir_aux_output_dir)\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m file_in_outputDir \u001b[39min\u001b[39;00m all_files_in_outputDir:\n\u001b[1;32m----> 4\u001b[0m         os\u001b[39m.\u001b[39;49mremove(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin( andriNir_aux_output_dir, file_in_outputDir ))\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Access is denied: 'C:\\\\Users\\\\User\\\\Cloud-Drive\\\\BigFiles\\\\OmissionExpOutput\\\\eventDetection\\\\done_on_imported\\\\Andrillon_Nir\\\\aux_mats'"
     ]
    }
   ],
   "source": [
    "def deleteAuxANFiles(andriNir_aux_output_dir):\n",
    "    all_files_in_outputDir = os.listdir(andriNir_aux_output_dir)\n",
    "    for file_in_outputDir in all_files_in_outputDir:\n",
    "            os.remove(os.path.join( andriNir_aux_output_dir, file_in_outputDir ))\n",
    "\n",
    "\n",
    "andriNir_code_dir = \"C:\\\\Users\\\\User\\\\Cloud-Drive\\\\AnatArzi\\\\scripts\\\\Matlab_scripts - Sharons\\\\Andrillon_Nir_Algo\"\n",
    "andriNir_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\\\\Andrillon_Nir\"\n",
    "andriNir_aux_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\\\\Andrillon_Nir\\\\aux_mats\"\n",
    "spindles_output_columns =  ['spindleStartTime', 'spindleEndTime', 'peakTime', 'peakEnergy', 'peakEnergyNorm', 'freqSpindle', 'spindleDuration/SR', 'PowerSP', 'PowerAlpha', 'currentStage']\n",
    "electrode_column_name = 'electrode'\n",
    "\n",
    "if not os.path.exists(andriNir_output_dir):\n",
    "    os.mkdir(andriNir_output_dir)\n",
    "\n",
    "deleteAuxANFiles(andriNir_aux_output_dir)\n",
    "deleteAuxANFiles(andriNir_output_dir)\n",
    "\n",
    "electrodes_names_eventDetect = ['F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "\n",
    "all_electodes_ss_key = 'SS_algo-AN_all'\n",
    "\n",
    "\n",
    "for id in ['32_2','34_2']:#allsubsdata_perFile:\n",
    "    datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "    datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "    scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=True)\n",
    "    ss_allElectrodes = pd.DataFrame()\n",
    "\n",
    "    for electd_i, electrode_name_eventDetect in enumerate(electrodes_names_eventDetect):\n",
    "        curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name_eventDetect)[0][0]\n",
    "\n",
    "        ## create aux files to use in MATLAB\n",
    "        datafile_1elect_eeg = datafile_data[curr_electrode_num,:]\n",
    "        if 2 not in datafile_scoring or 3 not in datafile_scoring:\n",
    "            continue\n",
    "        \n",
    "        mat_to_save =  {'datafile_data': datafile_1elect_eeg, 'scoring_upsampled': scoring_upsampled, 'sample_freq': configu['sample_freq'], 'electrode_name':electrode_name_eventDetect}\n",
    "        scipy.io.savemat(f\"{andriNir_aux_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\",mat_to_save)\n",
    "\n",
    "        ## run Andrillon & Nir SS detection over all subjects files\n",
    "        eng = matlab.engine.start_matlab()\n",
    "        eng.cd(andriNir_code_dir, nargout=0)\n",
    "        out = eng.batch_useAndrillonNirEventDetection(andriNir_aux_output_dir, andriNir_output_dir,nargout=0)\n",
    "        eng.quit()\n",
    "\n",
    "        ## add the spindles data to the main subject dictionary\n",
    "        try: \n",
    "            spindles_file_name = f\"{andriNir_output_dir}\\\\Spindles_andrillonNir_{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\"\n",
    "            matlabImport = scipy.io.loadmat(spindles_file_name, simplify_cells=True)\n",
    "        except Exception: \n",
    "            print(f\"Error importing spindles sub file at: {allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}\")\n",
    "            continue\n",
    "\n",
    "        ss = matlabImport['spindles']\n",
    "        if np.size(ss) >0 :\n",
    "            df = pd.DataFrame(np.double(ss))\n",
    "            df.columns = spindles_output_columns\n",
    "            tile_electrode = np.tile(f\"{electrode_name_eventDetect}\", ss.shape[0])[None].T\n",
    "            df[electrode_column_name] = tile_electrode\n",
    "            \n",
    "            allsubsdata_perFile[id][f'SS_algo-AN@@{electrode_name_eventDetect}'] = df\n",
    "            if electd_i ==0: ss_allElectrodes =  df\n",
    "            else:  ss_allElectrodes = pd.concat([ss_allElectrodes,df])\n",
    "\n",
    "        # sw = matlabImport['allWaves']\n",
    "        # if np.size(sw) >0 :\n",
    "        #     tile_electrode = np.tile(f\"{electrode_name_eventDetect}\", sw.shape[0])[None].T\n",
    "        #     sw = np.hstack([sw, tile_electrode])\n",
    "        #     allsubsdata_perFile[id][f'SW_algo-AN@@{electrode_name_eventDetect}'] = np.asarray(sw,dtype=object)  \n",
    "        #     sw_allElectrodes = np.concatenate((sw_allElectrodes,sw))\n",
    " \n",
    "    if not ss_allElectrodes.empty:\n",
    "        allsubsdata_perFile[id][all_electodes_ss_key]  = ss_allElectrodes\n",
    "        #allsubsdata_perFile[id][f'SW_algo-AN_all']  = sw_allElectrodes\n",
    "    \n",
    "deleteAuxANFiles(andriNir_aux_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'spindles', 'spindlesOutEnergy', 'spindlesOutDuration', 'rej_cand', 'allWaves', 'slowWaves'])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matlabImport.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub:32_2, before excludeNonN2Events:2231, after:765\n",
      "sub:34_2, before excludeNonN2Events:1667, after:443\n"
     ]
    }
   ],
   "source": [
    "after_n2_exclution_key = f\"{all_electodes_ss_key}_n2\"\n",
    "for id in allsubsdata_perFile:\n",
    "    if all_electodes_ss_key not in allsubsdata_perFile[id]: continue\n",
    "    ss_allElectrodes = allsubsdata_perFile[id][all_electodes_ss_key]\n",
    "    ss_allElectrodes_n2 = ss_allElectrodes.loc[ss_allElectrodes['currentStage'] == 2.0]\n",
    "    if len(ss_allElectrodes_n2) == 0: \n",
    "        continue\n",
    "    else:\n",
    "        print(f\"sub:{id}, before excludeNonN2Events:{len(ss_allElectrodes)}, after:{len(ss_allElectrodes_n2)}\")\n",
    "    allsubsdata_perFile[id][after_n2_exclution_key] = ss_allElectrodes_n2\n",
    "\n",
    "# sub:32_1, before excludeNonN2Events:2419, after:789\n",
    "# sub:32_2, before excludeNonN2Events:2231, after:765\n",
    "# sub:33_1, before excludeNonN2Events:2491, after:1530\n",
    "# sub:33_2, before excludeNonN2Events:3556, after:2758\n",
    "# sub:34_1, before excludeNonN2Events:1363, after:416\n",
    "# sub:34_2, before excludeNonN2Events:1667, after:443\n",
    "# sub:35_1, before excludeNonN2Events:1076, after:190\n",
    "# sub:35_3, before excludeNonN2Events:1742, after:951\n",
    "# sub:35_4, before excludeNonN2Events:668, after:250\n",
    "# sub:38_1, before excludeNonN2Events:2531, after:1379\n",
    "# sub:38_2, before excludeNonN2Events:3091, after:1793\n",
    "# sub:38_3, before excludeNonN2Events:2127, after:1433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub:32_1, before filt:2419, after:532\n",
      "sub:32_2, before filt:2231, after:417\n",
      "sub:33_1, before filt:2491, after:677\n",
      "sub:33_2, before filt:3556, after:711\n",
      "sub:34_1, before filt:1363, after:350\n",
      "sub:34_2, before filt:1667, after:238\n",
      "sub:35_1, before filt:1076, after:395\n",
      "sub:35_3, before filt:1742, after:427\n",
      "sub:35_4, before filt:668, after:151\n",
      "sub:38_1, before filt:2531, after:491\n",
      "sub:38_2, before filt:3091, after:536\n",
      "sub:38_3, before filt:2127, after:395\n"
     ]
    }
   ],
   "source": [
    "## create a spindle array, where each spindle occour once and attribued to the electrode where the spindle was the most powerful\n",
    "\n",
    "ss_key_to_use = all_electodes_ss_key # after_n2_exclution_key\n",
    "uniqeElctd_ss_key = f\"{ss_key_to_use}_uniqeElctd\"\n",
    "\n",
    "def overlap(a, b):\n",
    "    return a[1] >= b[0] and a[0] <= b[1]\n",
    "\n",
    "for id in allsubsdata_perFile:\n",
    "    if ss_key_to_use not in allsubsdata_perFile[id]:  continue\n",
    "    filterd_events_allElectrodes = allsubsdata_perFile[id][ss_key_to_use].copy(deep=True)\n",
    "    \n",
    "    filterd_events_allElectrodes.sort_values(by=['spindleStartTime'],inplace=True) \n",
    "    deleted = filterd_events_allElectrodes.copy(deep=True)\n",
    "    filtered = pd.DataFrame([], columns = deleted.columns)\n",
    "    simultan = pd.DataFrame([], columns = deleted.columns)\n",
    "    while len(deleted)>0:\n",
    "        if len(simultan)==0:\n",
    "            simultan = pd.concat([simultan, deleted.iloc[[0]]])\n",
    "            deleted.drop(deleted.index[0], axis=0, inplace=True)\n",
    "            still_overlap = True\n",
    "        else:\n",
    "            while still_overlap == True:\n",
    "                still_overlap = False\n",
    "                simultan.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "\n",
    "                ## check now_overlap_all_in_simultan:\n",
    "                for index, simultan_row in simultan.iterrows():\n",
    "                    simultan_0 = simultan_row['spindleStartTime']\n",
    "                    simultan_1 = simultan_row['spindleEndTime']\n",
    "                    deleted_0 = deleted.iloc[0]['spindleStartTime']\n",
    "                    deleted_1 = deleted.iloc[0]['spindleEndTime']\n",
    "                    if overlap([simultan_0,simultan_1],[deleted_0,deleted_1]):\n",
    "                        simultan = pd.concat([simultan, deleted.iloc[[0]]])\n",
    "                        deleted.drop(deleted.index[0], axis=0, inplace=True)\n",
    "                        still_overlap = True\n",
    "                        break\n",
    "                if still_overlap: continue\n",
    "                else:\n",
    "                    ## check max ps and add to filt\n",
    "                    simultan.sort_values(by=['PowerSP'],inplace=True)\n",
    "                    row_df = simultan.iloc[[0]]\n",
    "                    row_df.iat[0, row_df.columns.get_loc(electrode_column_name)] = np.array(simultan[electrode_column_name])\n",
    "                    filtered = pd.concat([filtered,row_df])\n",
    "                    simultan = pd.DataFrame([], columns = deleted.columns)                       \n",
    "        \n",
    "    filtered.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "    allsubsdata_perFile[id][uniqeElctd_ss_key] = filtered   \n",
    "    print(f\"sub:{id}, before filt:{np.shape(allsubsdata_perFile[id][ss_key_to_use])[0]}, after:{np.shape(allsubsdata_perFile[id][uniqeElctd_ss_key])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "#display(HTML(allsubsdata_perFile[id][uniqeElctd_ss_key].to_html()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change scoring to fit EDF_viewer events format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_scoring_to_edfViewer(score):\n",
    "    if score == 0:\n",
    "        return 'W'\n",
    "    elif score ==1:\n",
    "        return 'N1'\n",
    "    elif score ==2:\n",
    "        return 'N2'\n",
    "    elif score ==3:\n",
    "        return 'N3'\n",
    "    elif score ==4:\n",
    "        return 'TREM'\n",
    "    elif score ==5:\n",
    "        return 'PREM'\n",
    "    elif score ==6:\n",
    "        return 'MOVE'\n",
    "    elif score ==7:\n",
    "        return 'ARTIFACT'\n",
    "    else:\n",
    "        Exception('no such score')\n",
    "        \n",
    "\n",
    "for id in allsubsdata_perFile:\n",
    "    curr_file_scoring = allsubsdata_perFile[id]['scoring']\n",
    "    new_format_score = np.zeros((len(curr_file_scoring),3), dtype=object)\n",
    "    for ind, score in enumerate(curr_file_scoring):\n",
    "            new_format_score[ind,:] = [30*ind,30,format_scoring_to_edfViewer(score)] ## onset (sec), duration, desc\n",
    "\n",
    "    allsubsdata_perFile[id]['scoring_efdViewFormat'] = new_format_score\n",
    "    # np.savetxt('test2.txt', new_format_score, delimiter='\\t',fmt='%s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change spindles events to fit EDF_viewer events format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no SS_algo-AN_all_uniqeElctd for sub 32_3\n",
      "no SS_algo-AN_all_uniqeElctd for sub 33_3\n",
      "no SS_algo-AN_all_uniqeElctd for sub 33_4\n",
      "no SS_algo-AN_all_uniqeElctd for sub 34_3\n"
     ]
    }
   ],
   "source": [
    "event_key_for_save = uniqeElctd_ss_key\n",
    "SS_efdViewFormat_key = 'SS_efdViewFormat'\n",
    "header = np.array(['Onset',\"Duration\",\"Annotation\"])\n",
    "for id in allsubsdata_perFile:\n",
    "        if event_key_for_save not in allsubsdata_perFile[id]:\n",
    "                print(f\"no {event_key_for_save} for sub {id}\")\n",
    "                continue\n",
    "        ss_df = allsubsdata_perFile[id][uniqeElctd_ss_key]\n",
    "        ss_mat_edfFormat = np.zeros((len(ss_df),3), dtype=object)\n",
    "        startTime_arr = np.array(ss_df['spindleStartTime']) / np.double(configu['sample_freq'])\n",
    "        endTime_arr = np.array(ss_df['spindleEndTime']) / np.double(configu['sample_freq'])\n",
    "        duration_arr = endTime_arr - startTime_arr\n",
    "        electd_arr_per_ss = np.array(ss_df[electrode_column_name])\n",
    "        desc = [f\"SS@@{electd_arr[0]}\" for electd_arr in electd_arr_per_ss]\n",
    "        new_format_ss = np.array([startTime_arr,duration_arr,desc]).T ## onset (sec), duration,desc\n",
    "        #new_format_ss = np.concatenate((new_format_ss,header))\n",
    "        #new_format_ss = np.vstack(new_format_ss, header)\n",
    "        new_format_ss = np.r_[new_format_ss,[header]]\n",
    "        allsubsdata_perFile[id][f'SS_efdViewFormat'] = new_format_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [spindleStartTime spindleEndTime peakTime peakEnergy peakEnergyNorm freqSpindle spindleDuration/SR PowerSP PowerAlpha currentStage];\n",
    "# def EDFViewerformat(key_name):\n",
    "#         curr_file_ss = allsubsdata_perFile[id][key_name]\n",
    "#         new_format_ss = np.zeros((np.shape(curr_file_ss)[0],3), dtype=object)\n",
    "#         for ind, ss in enumerate(new_format_ss):\n",
    "#                 SpindleStartTime = np.double(curr_file_ss[ind,0]) / np.double(configu['sample_freq'])  \n",
    "#                 SpindleDuration = (np.double(curr_file_ss[ind,1])  -np.double(curr_file_ss[ind,0])) / np.double(configu['sample_freq'])\n",
    "#                 if np.size(curr_file_ss[ind,10]) > 1:  \n",
    "#                         event_desc = f\"{key_name.split('_')[0]}@@{curr_file_ss[ind,10][0]}\"\n",
    "#                 else:\n",
    "#                         event_desc = f\"{key_name.split('_')[0]}@@{curr_file_ss[ind,10]}\"\n",
    "#                 new_format_ss[ind,:] = [SpindleStartTime,SpindleDuration,event_desc] ## onset (sec), duration,desc\n",
    "#         allsubsdata_perFile[id][f'{key_name}_efdViewFormat'] = new_format_ss\n",
    "\n",
    "\n",
    "\n",
    "# events_types_for_save = [uniqeElctd_ss_key]\n",
    "# for event_type in events_types_for_save: \n",
    "#         for id in allsubsdata_perFile:\n",
    "#                 if event_type not in allsubsdata_perFile[id]:\n",
    "#                         print(f\"no {event_type} for sub {id}\")\n",
    "#                         continue\n",
    "#                 EDFViewerformat(event_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edfViewFormat_events_output_dir = f'{fig_output_dir}\\\\EDFViewFormat_events'\n",
    "if not os.path.exists(edfViewFormat_events_output_dir):\n",
    "    os.mkdir(edfViewFormat_events_output_dir)\n",
    "\n",
    "events_types_for_save = ['scoring_efdViewFormat','SS']\n",
    "# Save events (scoring and ss) per file, in a format suitable for EDF_viewer\n",
    "for id in allsubsdata_perFile:\n",
    "        filename = f\"{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_events\"\n",
    "        all_events_with_header = np.asarray([['Onset',\"Duration\",\"Annotation\"]],dtype=object)\n",
    "        for event_type in events_types_for_save:\n",
    "                events_type_found = [ v for k, v in allsubsdata_perFile[id].items() if (event_type in k) and ('_efdViewFormat' in k)]\n",
    "                if np.size(events_type_found) == 0: continue\n",
    "                for events_found in events_type_found:\n",
    "                        for event_found in events_found:\n",
    "                                all_events_with_header = np.concatenate((all_events_with_header,[event_found]),dtype=object)\n",
    "        np.savetxt(f\"{edfViewFormat_events_output_dir}\\\\{filename}.txt\", all_events_with_header, delimiter='\\t',fmt='%s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test TP / FP for the algorithm of Andrillon-Nir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 3)\n",
      "(204, 3)\n",
      "(737, 3)\n",
      "(235, 3)\n",
      "156\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "## get the array of before manual scanning\n",
    "txt_file_name = \"C:\\\\Users\\\\User\\\\Cloud-Drive\\\\BigFiles\\\\OmissionExpOutput\\\\eventDetection\\\\done_on_preProcessed\\\\EDFViewFormat_events\\\\32_2_events.txt\"\n",
    "before = np.loadtxt(txt_file_name, delimiter=\"\\t\",dtype='object')\n",
    "print(np.shape(before))\n",
    "before[1:,0] = [np.double(x) for x in before[1:,0]]\n",
    "before = np.delete(before,0,0)\n",
    "ss_ind = np.array([],dtype=int)\n",
    "for ind_i, desc in enumerate(before[:,2]):\n",
    "    if \"SS\" in desc:\n",
    "        ss_ind = np.append(ss_ind, ind_i)\n",
    "before_ss = before[ss_ind,:]\n",
    "print(np.shape(before_ss))\n",
    "\n",
    "## get the array of after manual scanning\n",
    "txt_file_name = \"C:\\\\Users\\\\User\\\\Cloud-Drive\\\\BigFiles\\\\OmissionExpOutput\\\\eventDetection\\\\done_on_preProcessed\\\\EDFViewFormat_events\\\\s_32_sleep2_preProcessed_annotations.txt\"\n",
    "after = np.loadtxt(txt_file_name, delimiter=\"\\t\",dtype='object')\n",
    "print(np.shape(after))\n",
    "after[1:,0] = [np.double(x) for x in after[1:,0]]\n",
    "\n",
    "ss_ind = np.array([],dtype=int)\n",
    "for ind_i, desc in enumerate(after[:,2]):\n",
    "    if \"SS\" in desc:\n",
    "        ss_ind = np.append(ss_ind, ind_i)\n",
    "after_ss = after[ss_ind,:]\n",
    "print(np.shape(after_ss))\n",
    "\n",
    "## compare to find rate od TP and FP\n",
    "TP = 0\n",
    "FP = 0\n",
    "for ss_auto in before_ss:\n",
    "    found = False\n",
    "    for ss_manu in after_ss:\n",
    "        if ss_auto[0] == ss_manu[0]:\n",
    "            TP +=1\n",
    "            found = True\n",
    "            break\n",
    "    if found == False:\n",
    "        FP +=1\n",
    "\n",
    "print(TP)\n",
    "print(FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py:1587\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1586\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1587\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_take_with_is_copy(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   1588\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m   1589\u001b[0m     \u001b[39m# re-raise with different error message\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   3895\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3896\u001b[0m \u001b[39mInternal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   3897\u001b[0m \u001b[39mattribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3900\u001b[0m \u001b[39mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   3901\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3902\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take(indices\u001b[39m=\u001b[39;49mindices, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   3903\u001b[0m \u001b[39m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\generic.py:3886\u001b[0m, in \u001b[0;36mNDFrame._take\u001b[1;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[0;32m   3884\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consolidate_inplace()\n\u001b[1;32m-> 3886\u001b[0m new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mtake(\n\u001b[0;32m   3887\u001b[0m     indices,\n\u001b[0;32m   3888\u001b[0m     axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_block_manager_axis(axis),\n\u001b[0;32m   3889\u001b[0m     verify\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   3890\u001b[0m     convert_indices\u001b[39m=\u001b[39;49mconvert_indices,\n\u001b[0;32m   3891\u001b[0m )\n\u001b[0;32m   3892\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtake\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\internals\\managers.py:965\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify, convert_indices)\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[39mif\u001b[39;00m convert_indices:\n\u001b[1;32m--> 965\u001b[0m     indexer \u001b[39m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[39m=\u001b[39;49mverify)\n\u001b[0;32m    967\u001b[0m new_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\u001b[39m.\u001b[39mtake(indexer)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexers\\utils.py:286\u001b[0m, in \u001b[0;36mmaybe_convert_indices\u001b[1;34m(indices, n, verify)\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[1;32m--> 286\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mindices are out-of-bounds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m indices\n",
      "\u001b[1;31mIndexError\u001b[0m: indices are out-of-bounds",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [66], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m## how to df \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[39m## add row to df and empty df\u001b[39;00m\n\u001b[0;32m      4\u001b[0m x \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame([], columns \u001b[39m=\u001b[39m deleted\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m----> 5\u001b[0m y\u001b[39m=\u001b[39m deleted\u001b[39m.\u001b[39;49miloc[[\u001b[39m0\u001b[39;49m]]\n\u001b[0;32m      6\u001b[0m y2\u001b[39m=\u001b[39m deleted\u001b[39m.\u001b[39miloc[[\u001b[39m1\u001b[39m]]\n\u001b[0;32m      7\u001b[0m z \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([x,y])\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1070\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m   1072\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m-> 1073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py:1616\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1614\u001b[0m \u001b[39m# a list of integers\u001b[39;00m\n\u001b[0;32m   1615\u001b[0m \u001b[39melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[1;32m-> 1616\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_list_axis(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   1618\u001b[0m \u001b[39m# a single integer\u001b[39;00m\n\u001b[0;32m   1619\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1620\u001b[0m     key \u001b[39m=\u001b[39m item_from_zerodim(key)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexing.py:1590\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1587\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_take_with_is_copy(key, axis\u001b[39m=\u001b[39maxis)\n\u001b[0;32m   1588\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m   1589\u001b[0m     \u001b[39m# re-raise with different error message\u001b[39;00m\n\u001b[1;32m-> 1590\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpositional indexers are out-of-bounds\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "## how to df \n",
    "\n",
    "## add row to df and empty df\n",
    "x = pd.DataFrame([], columns = deleted.columns)\n",
    "y= deleted.iloc[[0]]\n",
    "y2= deleted.iloc[[1]]\n",
    "z = pd.concat([x,y])\n",
    "zz = pd.concat([z,y2])\n",
    "display(zz)\n",
    "\n",
    "## delete first row\n",
    "# zz.drop(zz.index[0], axis=0, inplace=True) \n",
    "# display(zz)\n",
    "\n",
    "## get value at i,j\n",
    "print(deleted.iloc[0]['spindleStartTime'])\n",
    "\n",
    "## get column as array\n",
    "arr = np.array(zz['electrode'])\n",
    "print(arr)\n",
    "\n",
    "## replace inplace value in i,j to array:\n",
    "zz.iat[1, zz.columns.get_loc(electrode_column_name)] = arr\n",
    "zz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f544ce1a915a9875fad91c894e2c0bcad4b7a79945aa6027ef3ad27810072aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
