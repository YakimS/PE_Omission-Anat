{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30233"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.stats\n",
    "import mne\n",
    "from mne.time_frequency import tfr_morlet\n",
    "from mne.stats import permutation_cluster_1samp_test\n",
    "import gc\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "from os.path import exists\n",
    "import mne\n",
    "import numpy as np\n",
    "from mne import create_info\n",
    "from IPython.utils import io\n",
    "import yasa\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matlab\n",
    "import matlab.engine\n",
    "import os\n",
    "import shutil\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import freqz\n",
    "from scipy import signal\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    return butter(order, [lowcut, highcut], fs=fs, btype='band')\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def overlap(a, b):\n",
    "    return a[1] >= b[0] and a[0] <= b[1]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = \"C:\\\\Users\\\\User\\\\Cloud-Drive\\\\BigFiles\\\\OmissionExpOutput\\\\\\imported_eventDetectionChan\\\\filter0.35\"\n",
    "import_type = \"eventDetectionChan\"\n",
    "output_dir_name = 'eventDetection\\\\done_on_imported'\n",
    "import_path = f'{pkl_dir}\\\\{import_type}.pkl'\n",
    "\n",
    "electrode_column_name = 'electrode'\n",
    "header = np.array(['Onset',\"Duration\",\"Annotation\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Nir&Andrillon Spindle Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sw_AndrillonNir(file_ids,output_key,electrodes_names):\n",
    "    andriNir_code_dir = os.getcwd()\n",
    "    andriNir_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\\\\Andrillon_Nir\"\n",
    "    andriNir_aux_output_dir = f\"{andriNir_output_dir}\\\\aux_mats\"\n",
    "\n",
    "    if os.path.exists(andriNir_output_dir):\n",
    "        shutil.rmtree(andriNir_output_dir)\n",
    "    os.mkdir(andriNir_output_dir)\n",
    "    os.mkdir(andriNir_aux_output_dir)\n",
    "\n",
    "    sw_output_columns = ['wavest' ,'wavend', 'mdpt', 'poszx', 'period/SR' ,'abs(b)', 'bx', 'c' ,'cx' ,'maxb2c', 'n1', 'n1x', 'nEnd', 'nEndx' ,'p1', 'p1x' ,'meanAmp', 'nump', 'nperiod/SR', 'p2p', 'mxdn', 'mxup' ,'currentStage']\n",
    "\n",
    "    for id in file_ids:\n",
    "        datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "        datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "        scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=True)\n",
    "        sw_allElectrodes = pd.DataFrame()\n",
    "\n",
    "        for electd_i, electrode_name_eventDetect in enumerate(electrodes_names):\n",
    "            curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name_eventDetect)[0][0]\n",
    "\n",
    "            ## create aux files to use in MATLAB\n",
    "            datafile_1elect_eeg = datafile_data[curr_electrode_num,:]\n",
    "            if 2 not in datafile_scoring or 3 not in datafile_scoring:\n",
    "                continue\n",
    "            \n",
    "            mat_to_save = {'datafile_data': datafile_1elect_eeg, 'scoring_upsampled': scoring_upsampled, 'sample_freq': configu['sample_freq'], 'electrode_name':electrode_name_eventDetect}\n",
    "            scipy.io.savemat(f\"{andriNir_aux_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\",mat_to_save)\n",
    "\n",
    "            ## run Andrillon & Nir SW detection over all subjects files\n",
    "            eng = matlab.engine.start_matlab()\n",
    "            eng.cd(andriNir_code_dir, nargout=0)\n",
    "            out = eng.batch_useAndrillonNirSWDetection(andriNir_aux_output_dir, andriNir_output_dir,nargout=0)\n",
    "            eng.quit()\n",
    "\n",
    "            ## add the SW data to the main subject dictionary\n",
    "            try: \n",
    "                sw_file_name = f\"{andriNir_output_dir}\\\\{output_key}_{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\"\n",
    "                matlabImport = scipy.io.loadmat(sw_file_name, simplify_cells=True)\n",
    "            except Exception: \n",
    "                print(f\"Error importing spindles sub file at: {allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}\")\n",
    "                continue\n",
    "\n",
    "            sw = matlabImport['allWaves']\n",
    "            if np.size(np.shape(sw)) == 1: \n",
    "                temp = np.zeros((1,np.size(sw)),dtype=object)\n",
    "                temp[0] = sw\n",
    "                sw = temp\n",
    "            if np.size(sw) >0 :\n",
    "                df = pd.DataFrame(np.double(sw))\n",
    "                df.columns = sw_output_columns\n",
    "                tile_electrode = np.tile(f\"{electrode_name_eventDetect}\", sw.shape[0])[None].T\n",
    "                df[electrode_column_name] = tile_electrode\n",
    "                \n",
    "                allsubsdata_perFile[id][f\"{output_key}@@{electrode_name_eventDetect}\"] = df\n",
    "                if electd_i ==0: sw_allElectrodes =  df\n",
    "                else:  sw_allElectrodes = pd.concat([sw_allElectrodes,df])\n",
    "    \n",
    "        if not sw_allElectrodes.empty:\n",
    "            allsubsdata_perFile[id][output_key]  = sw_allElectrodes\n",
    "    shutil.rmtree(andriNir_aux_output_dir)\n",
    "def detect_kComp_AndrillonNir(file_ids,output_key,electrodes_names):\n",
    "    andriNir_code_dir = os.getcwd()\n",
    "    andriNir_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\\\\Andrillon_Nir\"\n",
    "    andriNir_aux_output_dir = f\"{andriNir_output_dir}\\\\aux_mats\"\n",
    "\n",
    "    if os.path.exists(andriNir_output_dir):\n",
    "        shutil.rmtree(andriNir_output_dir)\n",
    "    os.mkdir(andriNir_output_dir)\n",
    "    os.mkdir(andriNir_aux_output_dir)\n",
    "\n",
    "    kcomp_output_columns = ['wavest' ,'wavend', 'mdpt', 'poszx', 'period' ,'abs(b)', 'bx', 'c' ,'cx' ,'maxb2c', 'n1', 'n1x', 'nEnd', 'nEndx' ,'p1', 'p1x' ,'meanAmp', 'nump', 'nperiod/SR', 'p2p', 'mxdn', 'mxup' ,'currentStage']\n",
    "\n",
    "    for id in file_ids:\n",
    "        datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "        datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "        scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=True)\n",
    "        kcomp_allElectrodes = pd.DataFrame()\n",
    "\n",
    "        for electd_i, electrode_name_eventDetect in enumerate(electrodes_names):\n",
    "            curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name_eventDetect)[0][0]\n",
    "\n",
    "            ## create aux files to use in MATLAB\n",
    "            datafile_1elect_eeg = datafile_data[curr_electrode_num,:]\n",
    "            if 2 not in datafile_scoring or 3 not in datafile_scoring:\n",
    "                continue\n",
    "            \n",
    "            mat_to_save = {'datafile_data': datafile_1elect_eeg, 'scoring_upsampled': scoring_upsampled, 'sample_freq': configu['sample_freq'], 'electrode_name':electrode_name_eventDetect}\n",
    "            scipy.io.savemat(f\"{andriNir_aux_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\",mat_to_save)\n",
    "\n",
    "            ## run Andrillon & Nir Kcomp detection over all subjects files\n",
    "            eng = matlab.engine.start_matlab()\n",
    "            eng.cd(andriNir_code_dir, nargout=0)\n",
    "            out = eng.batch_useAndrillonNirKcompDetection(andriNir_aux_output_dir, andriNir_output_dir,nargout=0)\n",
    "            eng.quit()\n",
    "\n",
    "            ## add the SW data to the main subject dictionary\n",
    "            try: \n",
    "                kcomp_file_name = f\"{andriNir_output_dir}\\\\{output_key}_{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\"\n",
    "                matlabImport = scipy.io.loadmat(kcomp_file_name, simplify_cells=True)\n",
    "            except Exception: \n",
    "                print(f\"Error importing kcomp sub file at: {kcomp_file_name}\")\n",
    "                continue\n",
    "\n",
    "            kComps = matlabImport['kcomps']\n",
    "            if np.size(np.shape(kComps)) == 1: \n",
    "                temp = np.zeros((1,np.size(kComps)),dtype=object)\n",
    "                temp[0] = kComps\n",
    "                kComps = temp\n",
    "            if np.size(kComps) >0 :\n",
    "                df = pd.DataFrame(np.double(kComps))\n",
    "                df.columns = kcomp_output_columns\n",
    "                tile_electrode = np.tile(f\"{electrode_name_eventDetect}\", kComps.shape[0])[None].T\n",
    "                df[electrode_column_name] = tile_electrode\n",
    "                \n",
    "                allsubsdata_perFile[id][f\"{output_key}@@{electrode_name_eventDetect}\"] = df\n",
    "                if electd_i ==0: kcomp_allElectrodes =  df\n",
    "                else:  kcomp_allElectrodes = pd.concat([kcomp_allElectrodes,df])\n",
    "    \n",
    "        if not kcomp_allElectrodes.empty:\n",
    "            allsubsdata_perFile[id][output_key]  = kcomp_allElectrodes\n",
    "\n",
    "        \n",
    "    shutil.rmtree(andriNir_aux_output_dir)\n",
    "def detect_ss_AndrillonNir(file_ids,minMax_sd_ver,output_key,electrodes_names):\n",
    "    andriNir_code_dir = os.getcwd()\n",
    "    andriNir_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\\\\Andrillon_Nir\"\n",
    "    andriNir_aux_output_dir = f\"{andriNir_output_dir}\\\\aux_mats\"\n",
    "\n",
    "    if os.path.exists(andriNir_output_dir):\n",
    "        shutil.rmtree(andriNir_output_dir)\n",
    "    os.mkdir(andriNir_output_dir)\n",
    "    os.mkdir(andriNir_aux_output_dir)\n",
    "\n",
    "    spindles_output_columns =  ['spindleStartTime', 'spindleEndTime', 'peakTime', 'peakEnergy', 'peakEnergyNorm', 'freqSpindle', 'spindleDuration/SR', 'PowerSP', 'PowerAlpha', 'currentStage']\n",
    "\n",
    "    for id in file_ids:\n",
    "        datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "        datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "        scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=True)\n",
    "        ss_allElectrodes = pd.DataFrame()\n",
    "\n",
    "        for electd_i, electrode_name_eventDetect in enumerate(electrodes_names):\n",
    "            curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name_eventDetect)[0][0]\n",
    "\n",
    "            ## create aux files to use in MATLAB\n",
    "            datafile_1elect_eeg = datafile_data[curr_electrode_num,:]\n",
    "            if 2 not in datafile_scoring or 3 not in datafile_scoring:\n",
    "                continue\n",
    "            \n",
    "            DetectionThreshold = minMax_sd_ver[0]\n",
    "            RejectThreshold = minMax_sd_ver[1]\n",
    "            StartEndThreshold = minMax_sd_ver[2]\n",
    "            mat_to_save = {'datafile_data': datafile_1elect_eeg, 'scoring_upsampled': scoring_upsampled, 'sample_freq': configu['sample_freq'], 'electrode_name':electrode_name_eventDetect, 'DetectionThreshold':DetectionThreshold,'RejectThreshold':RejectThreshold, 'StartEndThreshold':StartEndThreshold}\n",
    "            scipy.io.savemat(f\"{andriNir_aux_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\",mat_to_save)\n",
    "\n",
    "            ## run Andrillon & Nir SS detection over all subjects files\n",
    "            eng = matlab.engine.start_matlab()\n",
    "            eng.cd(andriNir_code_dir, nargout=0)\n",
    "            out = eng.batch_useAndrillonNirSSDetection(andriNir_aux_output_dir, andriNir_output_dir,nargout=0)\n",
    "            eng.quit()\n",
    "\n",
    "            ## add the spindles data to the main subject dictionary\n",
    "            try: \n",
    "                spindles_file_name = f\"{andriNir_output_dir}\\\\SS_AN_{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\"\n",
    "                matlabImport = scipy.io.loadmat(spindles_file_name, simplify_cells=True)\n",
    "            except Exception: \n",
    "                print(f\"Error importing spindles sub file at: {allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}\")\n",
    "                continue\n",
    "\n",
    "            ss = matlabImport['spindles']\n",
    "            if np.size(np.shape(ss)) == 1: \n",
    "                temp = np.zeros((1,np.size(ss)),dtype=object)\n",
    "                temp[0] = ss\n",
    "                ss = temp\n",
    "            if np.size(ss) >0 :\n",
    "                df = pd.DataFrame(np.double(ss))\n",
    "                df.columns = spindles_output_columns\n",
    "                tile_electrode = np.tile(f\"{electrode_name_eventDetect}\", ss.shape[0])[None].T\n",
    "                df[electrode_column_name] = tile_electrode\n",
    "                \n",
    "                allsubsdata_perFile[id][f'{output_key}@@{electrode_name_eventDetect}'] = df\n",
    "                if electd_i ==0: ss_allElectrodes =  df\n",
    "                else:  ss_allElectrodes = pd.concat([ss_allElectrodes,df])\n",
    "    \n",
    "        if not ss_allElectrodes.empty:\n",
    "            allsubsdata_perFile[id][output_key]  = ss_allElectrodes\n",
    "\n",
    "    minmax_sd_name =  matlabImport['minMax_SD_threshold']\n",
    "    shutil.rmtree(andriNir_aux_output_dir)\n",
    "    return minmax_sd_name\n",
    "\n",
    "\n",
    "def filter_sleep_stages(file_ids,event_key_to_use,after_sleepStage_exclution_key, sleepstages):\n",
    "    for id in file_ids:\n",
    "        if event_key_to_use not in allsubsdata_perFile[id]: continue\n",
    "        event_allElectrodes = allsubsdata_perFile[id][event_key_to_use]\n",
    "        \n",
    "        event_filtered_sleepstages = pd.DataFrame([], columns=event_allElectrodes.columns)\n",
    "        for sleepstage in sleepstages:\n",
    "            event_filtered_sleepstages = pd.concat([event_filtered_sleepstages , event_allElectrodes.loc[(event_allElectrodes['currentStage'] == sleepstage)]])\n",
    "\n",
    "        if len(event_filtered_sleepstages) == 0:  print(f\"sub:{id} - no events in sleepstages{sleepstages}\")\n",
    "        else: print(f\"sub:{id}, before exclude sleep stages:{len(event_allElectrodes)}, after:{len(event_filtered_sleepstages)}\")\n",
    "        \n",
    "        allsubsdata_perFile[id][after_sleepStage_exclution_key] = event_filtered_sleepstages\n",
    "def filter_kcompCriteria(file_ids,event_key_to_use,afterkcompCriteria_exclution_key):\n",
    "            # % Result Matrix\n",
    "        # %1:  wave beginning (sample)\n",
    "        # %2:  wave end (sample)\n",
    "        # %3:  wave middle point (sample)\n",
    "        # %4:  wave negative half-way (sample)\n",
    "        # %5:  period in seconds\n",
    "        # %6:  positive amplitude peak\n",
    "        # %7:  positive amplitude peak position (sample)\n",
    "        # %8:  negative amplitude peak\n",
    "        # %9:  negative amplitude peak position (sample)\n",
    "        # %10: peak-to-peak amplitude\n",
    "        # %11: 1st pos peak amplitude\n",
    "        # %12: 1st pos peak amplitude position (sample)\n",
    "        # %13: Last pos peak amplitude\n",
    "        # %14: Last pos peak amplitude position (sample)\n",
    "        # %15: 1st neg peak amplitude\n",
    "        # %16: 1st neg peak amplitude position (sample)\n",
    "        # %17: mean wave amplitude\n",
    "        # %18: number of positive peaks\n",
    "        # %19: wave negative half-way period\n",
    "        # %20: 1st peak to last peak period\n",
    "        # %21: determines instantaneous positive 1st segement slope on smoothed signal\n",
    "        # %22: determines maximal negative slope for 2nd segement\n",
    "        # %23: stage (if scored data)\n",
    "    for id in file_ids:\n",
    "        if event_key_to_use not in allsubsdata_perFile[id]: continue\n",
    "        event_allElectrodes = allsubsdata_perFile[id][event_key_to_use]\n",
    "        event_filtered_kcompCrit = pd.DataFrame([], columns=event_allElectrodes.columns)\n",
    "\n",
    "        event_filtered_kcompCrit = event_allElectrodes[\n",
    "                                                        (event_allElectrodes['period'] >= 0.5) &\n",
    "                                                        (event_allElectrodes['n1'] < 45) &\n",
    "                                                        ((event_allElectrodes['currentStage'] == 2.0) | (event_allElectrodes['currentStage'] == 3.0))\n",
    "                                                       ]\n",
    "\n",
    "        if len(event_filtered_kcompCrit) == 0:  print(f\"sub:{id} - no kcomp with criteria\")\n",
    "        else: print(f\"sub:{id}, before wave criteria sleep stages:{len(event_allElectrodes)}, after:{len(event_filtered_kcompCrit)}\")\n",
    "        \n",
    "        allsubsdata_perFile[id][afterkcompCriteria_exclution_key] = event_filtered_kcompCrit\n",
    "def filter_waveCriteria(file_ids,event_key_to_use,after_sleepStage_exclution_key):\n",
    "        # % Result Matrix\n",
    "        # %1:  wave beginning (sample)\n",
    "        # %2:  wave end (sample)\n",
    "        # %3:  wave middle point (sample)\n",
    "        # %4:  wave negative half-way (sample)\n",
    "        # %5:  period in seconds\n",
    "        # %6:  positive amplitude peak\n",
    "        # %7:  positive amplitude peak position (sample)\n",
    "        # %8:  negative amplitude peak\n",
    "        # %9:  negative amplitude peak position (sample)\n",
    "        # %10: peak-to-peak amplitude\n",
    "        # %11: 1st pos peak amplitude\n",
    "        # %12: 1st pos peak amplitude position (sample)\n",
    "        # %13: Last pos peak amplitude\n",
    "        # %14: Last pos peak amplitude position (sample)\n",
    "        # %15: 1st neg peak amplitude\n",
    "        # %16: 1st neg peak amplitude position (sample)\n",
    "        # %17: mean wave amplitude\n",
    "        # %18: number of positive peaks\n",
    "        # %19: wave negative half-way period\n",
    "        # %20: 1st peak to last peak period\n",
    "        # %21: determines instantaneous positive 1st segement slope on smoothed signal\n",
    "        # %22: determines maximal negative slope for 2nd segement\n",
    "        # %23: stage (if scored data)\n",
    "    for id in file_ids:\n",
    "        if event_key_to_use not in allsubsdata_perFile[id]: continue\n",
    "        event_allElectrodes = allsubsdata_perFile[id][event_key_to_use]\n",
    "        event_filtered_waveCrit = pd.DataFrame([], columns=event_allElectrodes.columns)\n",
    "\n",
    "        event_filtered_waveCrit = event_allElectrodes[(event_allElectrodes['maxb2c'] > 75)]\n",
    "\n",
    "        if len(event_filtered_waveCrit) == 0:  print(f\"sub:{id} - no sw with criteria\")\n",
    "        else: print(f\"sub:{id}, before wave criteria sleep stages:{len(event_allElectrodes)}, after:{len(event_filtered_waveCrit)}\")\n",
    "        \n",
    "        allsubsdata_perFile[id][after_sleepStage_exclution_key] = event_filtered_waveCrit\n",
    "def group_spindles(ss_key_to_use, uniqeElctds_ss_key, uniqeElctd_ss_key):\n",
    "    ## create a spindle array, where each spindle occour once and attribued to the electrode where the spindle was the most powerful\n",
    "    for id in allsubsdata_perFile:\n",
    "        if ss_key_to_use not in allsubsdata_perFile[id]:  continue\n",
    "        filterd_events_allElectrodes = allsubsdata_perFile[id][ss_key_to_use].copy(deep=True)\n",
    "        \n",
    "        filterd_events_allElectrodes.sort_values(by=['spindleStartTime'],inplace=True) \n",
    "        filterd_events_allElectrodes.reset_index(drop=True, inplace=True)\n",
    "        deleted = filterd_events_allElectrodes.copy(deep=True)\n",
    "        filtered = pd.DataFrame([], columns = deleted.columns)\n",
    "        filtered_moreThan1 = pd.DataFrame([], columns = deleted.columns)\n",
    "        simultan = pd.DataFrame([], columns = deleted.columns)\n",
    "\n",
    "        while len(deleted)>0:\n",
    "            if len(simultan)==0:\n",
    "                simultan = pd.concat([simultan, deleted.iloc[[0]]])\n",
    "                deleted.drop(deleted.index[0], axis=0, inplace=True)\n",
    "                still_overlap = True\n",
    "            else:\n",
    "                while still_overlap == True and len(deleted)>0:\n",
    "                    still_overlap = False\n",
    "                    simultan.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "\n",
    "                    ## check now_overlap_all_in_simultan:\n",
    "                    for index, simultan_row in simultan.iterrows():\n",
    "                        simultan_0 = simultan_row['spindleStartTime']\n",
    "                        simultan_1 = simultan_row['spindleEndTime']\n",
    "                        deleted_0 = deleted.iloc[0]['spindleStartTime']\n",
    "                        deleted_1 = deleted.iloc[0]['spindleEndTime']\n",
    "                        if overlap([simultan_0,simultan_1],[deleted_0,deleted_1]):\n",
    "                            simultan = pd.concat([simultan, deleted.iloc[[0]]])\n",
    "                            deleted.drop(deleted.index[0], axis=0, inplace=True)\n",
    "                            still_overlap = True\n",
    "                            break\n",
    "                    if still_overlap: continue\n",
    "                    else:\n",
    "                        ## check max ps and add to filt\n",
    "                        # if len(simultan)>1:\n",
    "                        #     print('hu')\n",
    "                        simultan.sort_values(by=['PowerSP'],ascending=False,inplace=True)\n",
    "                        row_df = simultan.iloc[[0]]\n",
    "                        row_df.iat[0, row_df.columns.get_loc(electrode_column_name)] = np.array(simultan[electrode_column_name])\n",
    "                        filtered = pd.concat([filtered,row_df])\n",
    "                        if len(simultan) > 1: filtered_moreThan1 = pd.concat([filtered_moreThan1,row_df])\n",
    "                        simultan = pd.DataFrame([], columns = deleted.columns)                    \n",
    "            \n",
    "        filtered.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "        filtered_moreThan1.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "        allsubsdata_perFile[id][uniqeElctd_ss_key] = filtered   \n",
    "        allsubsdata_perFile[id][uniqeElctds_ss_key] = filtered_moreThan1   \n",
    "        print(f\"sub:{id}, before filt:{np.shape(allsubsdata_perFile[id][ss_key_to_use])[0]}, after:{np.shape(allsubsdata_perFile[id][uniqeElctd_ss_key])[0]}, after>1: after:{np.shape(allsubsdata_perFile[id][uniqeElctds_ss_key])[0]}\")\n",
    "\n",
    "def edfViewFormat_scoring_dict(score):\n",
    "    if score == 0:\n",
    "        return 'W'\n",
    "    elif score ==1:\n",
    "        return 'N1'\n",
    "    elif score ==2:\n",
    "        return 'N2'\n",
    "    elif score ==3:\n",
    "        return 'N3'\n",
    "    elif score ==4:\n",
    "        return 'TREM'\n",
    "    elif score ==5:\n",
    "        return 'PREM'\n",
    "    elif score ==6:\n",
    "        return 'MOVE'\n",
    "    elif score ==7:\n",
    "        return 'ARTIFACT'\n",
    "    else:\n",
    "        Exception('no such score')\n",
    "def add_edfViewFormat_scoring(key_edfScoringFormat):\n",
    "    for id in allsubsdata_perFile:\n",
    "        curr_file_scoring = allsubsdata_perFile[id]['scoring']\n",
    "        new_format_score = np.zeros((len(curr_file_scoring),3), dtype=object)\n",
    "        for ind, score in enumerate(curr_file_scoring):\n",
    "                new_format_score[ind,:] = [30*ind,30,edfViewFormat_scoring_dict(score)] ## onset (sec), duration, desc\n",
    "\n",
    "        allsubsdata_perFile[id][key_edfScoringFormat] = new_format_score\n",
    "def add_edfViewFormat_ss(file_ids,event_key_for_save, SS_efdViewFormat_key):\n",
    "        for id in file_ids:\n",
    "            if event_key_for_save not in allsubsdata_perFile[id]:\n",
    "                    #print(f\"no {event_key_for_save} for sub {id}\")\n",
    "                    continue\n",
    "            ss_df = allsubsdata_perFile[id][event_key_for_save]\n",
    "            startTime_arr = np.array(ss_df['spindleStartTime']) / np.double(configu['sample_freq'])\n",
    "            endTime_arr = np.array(ss_df['spindleEndTime']) / np.double(configu['sample_freq'])\n",
    "            duration_arr = endTime_arr - startTime_arr\n",
    "            electd_arr_per_ss = np.array(ss_df[electrode_column_name])\n",
    "            desc = [f\"SS@@{electd_arr[0]}\" for electd_arr in electd_arr_per_ss] ## NOTICE: this will work only if the electrode column consists of values which are arrays\n",
    "            new_format_ss = np.array([startTime_arr,duration_arr,desc]).T ## onset (sec), duration,desc\n",
    "            allsubsdata_perFile[id][SS_efdViewFormat_key] = new_format_ss\n",
    "def add_edfViewFormat_sw(file_ids,event_key_for_save, SW_efdViewFormat_key):\n",
    "        for id in file_ids:\n",
    "            if event_key_for_save not in allsubsdata_perFile[id]:\n",
    "                    #print(f\"no {event_key_for_save} for sub {id}\")\n",
    "                    continue\n",
    "            sw_df = allsubsdata_perFile[id][event_key_for_save]\n",
    "    \n",
    "            startTime_arr = np.array(sw_df['wavest'])\n",
    "            endTime_arr = np.array(sw_df['wavend'])\n",
    "            duration_arr = (endTime_arr - startTime_arr)  / np.double(configu['sample_freq'])\n",
    "            electd_arr_per_sw = np.array(sw_df[electrode_column_name])\n",
    "            desc = [f\"SW@@{electd_arr}\" for electd_arr in electd_arr_per_sw]\n",
    "            new_format_sw = np.array([startTime_arr / np.double(configu['sample_freq']) ,duration_arr,desc]).T ## onset (sec), duration,desc\n",
    "\n",
    "            allsubsdata_perFile[id][SW_efdViewFormat_key] = new_format_sw\n",
    "def add_edfViewFormat_kcomp(file_ids,event_key_for_save, kcomp_efdViewFormat_key):\n",
    "    for id in file_ids:\n",
    "        if event_key_for_save not in allsubsdata_perFile[id]:\n",
    "                #print(f\"no {event_key_for_save} for sub {id}\")\n",
    "                continue\n",
    "        kcomp_df = allsubsdata_perFile[id][event_key_for_save]\n",
    "\n",
    "        startTime_arr = np.array(kcomp_df['wavest'])\n",
    "        endTime_arr = np.array(kcomp_df['wavend'])\n",
    "        duration_arr = np.round(endTime_arr - startTime_arr,2)\n",
    "        electd_arr_per_sw = np.array(kcomp_df[electrode_column_name])\n",
    "        desc = [f\"kcomp@@{electd_arr}\" for electd_arr in electd_arr_per_sw]\n",
    "        new_format_kcomp = np.array([startTime_arr ,duration_arr,desc]).T ## onset (sec), duration,desc\n",
    "\n",
    "        allsubsdata_perFile[id][kcomp_efdViewFormat_key] = new_format_kcomp\n",
    "def save_eventsSepratelyForComparison_edfViewFormat(file_ids,events_types_to_compare,edfViewFormat_output_dir):\n",
    "    # Save events (scoring and ss) per file, in a format suitable for EDF_viewer\n",
    "    for id in file_ids:\n",
    "        for event_type in events_types_to_compare:\n",
    "            filename = f\"{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_events\"\n",
    "            all_events_with_header = np.asarray([header],dtype=object)\n",
    "            events_type_found = [ v for k, v in allsubsdata_perFile[id].items() if ((event_type in k) and (edfviewFormatSuffix in k))]\n",
    "            if np.size(events_type_found) == 0: continue\n",
    "            for events_found in events_type_found:\n",
    "                    for event_found in events_found:\n",
    "                            all_events_with_header = np.concatenate((all_events_with_header,[event_found]),dtype=object)\n",
    "            np.savetxt(f\"{edfViewFormat_output_dir}\\\\{event_type}_{filename}.txt\", all_events_with_header, delimiter='\\t',fmt='%s')\n",
    "    \n",
    "# Save events (scoring and ss) per file, in a format suitable for EDF_viewer\n",
    "def save_eventsAllTogether_edfViewFormat(events_types_for_save,edfViewFormat_output_dir):\n",
    "        for id in file_ids:\n",
    "            filename = f\"{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_events\"\n",
    "            all_events_with_header = np.asarray([header],dtype=object)\n",
    "            for event_type in events_types_for_save:\n",
    "                events_type_found = [ v for k, v in allsubsdata_perFile[id].items() if (event_type in k) and (edfviewFormatSuffix in k)]\n",
    "                if np.size(events_type_found) == 0: continue\n",
    "                for events_found in events_type_found:\n",
    "                        for event_found in events_found:\n",
    "                                all_events_with_header = np.concatenate((all_events_with_header,[event_found]),dtype=object)\n",
    "            np.savetxt(f\"{edfViewFormat_output_dir}\\\\allEvents_{filename}.txt\", all_events_with_header, delimiter='\\t',fmt='%s')\n",
    "\n",
    "def preform_minmaxSD_comparison(events_types_to_compare,file_ids,minMax_sd_vers):\n",
    "    all_comparisons = pd.DataFrame(columns=['id','eventType','minmax_ver','Positive','TruePos','FalsePos', 'hitRate','falseDiscoveryRate'])\n",
    "    for id in file_ids:\n",
    "        for ver in minMax_sd_vers:\n",
    "            for event_type in events_types_to_compare:\n",
    "                dir_auto_ = f\"{edfViewFormat_events_output_dir}\\\\{ver}\"\n",
    "                manual_detection_filename = f\"{edfViewFormat_events_manual_dir}\\\\sub{id.split('_')[0]}_sleep{id.split('_')[1]}_imported_eventDetectionChan_annotations.txt\"\n",
    "                auto_detection_filename = f\"{dir_auto_}\\\\{event_type}_{id}_events.txt\"\n",
    "                ## get the array of before manual scanning\n",
    "                if os.path.exists(auto_detection_filename) and os.path.exists(manual_detection_filename):\n",
    "                    all_ss_auto = np.loadtxt(auto_detection_filename, delimiter=\"\\t\",dtype='object')\n",
    "                    all_ss_auto = np.delete(all_ss_auto, np.where(all_ss_auto[:, 2] == \"Annotation\")[0], axis=0)\n",
    "                    all_ss_auto[:,[0,1]] = [np.double(x) for x in all_ss_auto[:,[0,1]]]\n",
    "                    ss_ind = np.array([],dtype=int)\n",
    "                    for ind_i, desc in enumerate(all_ss_auto[:,2]):\n",
    "                        if (\"SS\" in desc) or (\"ss\" in desc):\n",
    "                            ss_ind = np.append(ss_ind, ind_i)\n",
    "                    all_ss_auto = all_ss_auto[ss_ind,:]\n",
    "\n",
    "                    ## get the array of after manual scanning\n",
    "                    all_ss_manu = np.loadtxt(manual_detection_filename, delimiter=\"\\t\",dtype='object')\n",
    "                    all_ss_manu = np.delete(all_ss_manu, np.where(all_ss_manu[:, 2] == \"Annotation\")[0], axis=0)\n",
    "                    all_ss_manu[:,[0,1]] = [np.double(x) for x in all_ss_manu[:,[0,1]]]\n",
    "                    ss_ind = np.array([],dtype=int)\n",
    "                    for ind_i, desc in enumerate(all_ss_manu[:,2]):\n",
    "                        if \"SS\" in desc:\n",
    "                            ss_ind = np.append(ss_ind, ind_i)\n",
    "                    all_ss_manu = all_ss_manu[ss_ind,:]\n",
    "\n",
    "                    ## compare to find rate od TP and FP\n",
    "                    TP = 0\n",
    "                    FP = 0\n",
    "                    for ss_auto in all_ss_auto:\n",
    "                        found = False\n",
    "                        for ss_manu in all_ss_manu:\n",
    "                            if overlap([ss_auto[0],ss_auto[0]+ss_auto[1]],[ss_manu[0],ss_manu[0]+ss_manu[1]]):\n",
    "                                TP +=1\n",
    "                                found = True\n",
    "                                break\n",
    "                        if found == False:\n",
    "                            FP +=1\n",
    "\n",
    "                    # FN = 0\n",
    "                    # for ss_manu in all_ss_manu:\n",
    "                    #     found = False\n",
    "                    #     for ss_auto in all_ss_auto:\n",
    "                    #         if overlap([ss_auto[0],ss_auto[0]+ss_auto[1]],[ss_manu[0],ss_manu[0]+ss_manu[1]]):\n",
    "                    #             found = True\n",
    "                    #             break\n",
    "                    #     if found == False:\n",
    "                    #         FN +=1\n",
    "\n",
    "                    Positive = np.shape(all_ss_manu)[0]   \n",
    "                    hitRate = TP/Positive\n",
    "                    #missRate = FN/Positive\n",
    "                    falseDiscoveryRate = FP/(FP+TP)\n",
    "                    \n",
    "                    comparison = [id,event_type, ver, Positive, TP, FP,hitRate, falseDiscoveryRate]\n",
    "                    all_comparisons.loc[len(all_comparisons)] = comparison\n",
    "\n",
    "    all_comparisons = all_comparisons.sort_values('minmax_ver')\n",
    "    all_comparisons = all_comparisons.sort_values('id')\n",
    "    return all_comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS TO GET COMPARISONS OF SPINDLES MINMANX_SD OPTIONS\n",
    "run_comparison_of_spindle_minMax = False\n",
    "if(run_comparison_of_spindle_minMax):\n",
    "    testType = 0\n",
    "    if(testType==1):\n",
    "        file_ids = ['38_2']\n",
    "        minMax_sd_vers = [[4,10,1]]\n",
    "        electrodes_names_eventDetect = ['Fp2','F3']\n",
    "    elif(testType==2):\n",
    "        file_ids = ['32_2','38_2']\n",
    "        minMax_sd_vers = [[3,9,1],[4,10,1]]\n",
    "        electrodes_names_eventDetect = ['Fp2','F3']\n",
    "    elif(testType==3):\n",
    "        file_ids = ['32_2','38_2']\n",
    "        minMax_sd_vers = [[3,9,1],[4,10,1]]\n",
    "        electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "    else:\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "        minMax_sd_vers = [[3,8,1],[4,8,1],[5,8,1],[3,9,1],[4,9,1],[5,9,1],[3,10,1],[4,10,1],[5,10,1]]\n",
    "        #file_ids = allsubsdata_perFile.keys()\n",
    "\n",
    "    with open(import_path, \"rb\") as file:\n",
    "        [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "    fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "    if not os.path.exists(fig_output_dir): os.mkdir(fig_output_dir)\n",
    "\n",
    "    edfViewFormat_events_manual_dir = f'{fig_output_dir}\\\\EDFViewFormat_events'\n",
    "    edfViewFormat_events_output_dir = f'{fig_output_dir}\\\\EDFViewFormat_events\\\\minmax_sd_tests'\n",
    "    if os.path.exists(edfViewFormat_events_output_dir):\n",
    "        shutil.rmtree(edfViewFormat_events_output_dir)\n",
    "    os.mkdir(edfViewFormat_events_output_dir)\n",
    "\n",
    "    fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "    if not os.path.exists(fig_output_dir): os.mkdir(fig_output_dir)\n",
    "\n",
    "    for minMax_sd_ver in minMax_sd_vers:\n",
    "        with open(import_path, \"rb\") as file: [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "        \n",
    "        all_electodes_ss_key = 'SS_AN'\n",
    "        minmax_sd_name = detect_ss_AndrillonNir(file_ids,minMax_sd_ver,all_electodes_ss_key,electrodes_names_eventDetect)\n",
    "        print(minmax_sd_name)\n",
    "\n",
    "        after_sleepStage_exclution_key = f\"{all_electodes_ss_key}_n2n3\"\n",
    "        filter_sleep_stages(file_ids,all_electodes_ss_key,after_sleepStage_exclution_key, [2.0,3.0]) \n",
    "\n",
    "        sw_key_to_use = after_sleepStage_exclution_key\n",
    "        multiElectdPerSS_text = \"multiElectd\"\n",
    "        singleElectdPerSS_text = \"singleElectd\"\n",
    "        multiElectdPerSS_key = f\"{sw_key_to_use}_{multiElectdPerSS_text}\"\n",
    "        singleElectdPerSS_key = f\"{sw_key_to_use}_{singleElectdPerSS_text}\"\n",
    "        group_spindles(sw_key_to_use,multiElectdPerSS_key, singleElectdPerSS_key)\n",
    "\n",
    "        edfviewFormatSuffix = 'efdViewFormat'\n",
    "        SS_multiElectdPerSS_efdViewFormat_key = f'{multiElectdPerSS_key}_{edfviewFormatSuffix}'\n",
    "        SS_singleElectdPerSS_efdViewFormat_key = f'{singleElectdPerSS_key}_{edfviewFormatSuffix}'\n",
    "        add_edfViewFormat_ss(file_ids,multiElectdPerSS_key,SS_multiElectdPerSS_efdViewFormat_key)\n",
    "        add_edfViewFormat_ss(file_ids,singleElectdPerSS_key,SS_singleElectdPerSS_efdViewFormat_key)\n",
    "\n",
    "        events_types_to_compare = [multiElectdPerSS_text,singleElectdPerSS_text]    \n",
    "        edfViewFormat_eventsTest_output_dir = f\"{edfViewFormat_events_output_dir}\\\\{minMax_sd_ver}\"\n",
    "        if os.path.exists(edfViewFormat_eventsTest_output_dir):  shutil.rmtree(edfViewFormat_eventsTest_output_dir)\n",
    "        os.mkdir(edfViewFormat_eventsTest_output_dir)\n",
    "        save_eventsSepratelyForComparison_edfViewFormat(file_ids,events_types_to_compare,edfViewFormat_eventsTest_output_dir)\n",
    "\n",
    "        scoring_edfViewFormat_key = f'scoring_{edfviewFormatSuffix}'\n",
    "        add_edfViewFormat_scoring(scoring_edfViewFormat_key)\n",
    "\n",
    "        events_types_to_save = ['scoring', multiElectdPerSS_text,singleElectdPerSS_text]    \n",
    "        save_eventsAllTogether_edfViewFormat(events_types_to_save,edfViewFormat_eventsTest_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_comparison_of_spindle_minMax = False\n",
    "if(run_comparison_of_spindle_minMax):\n",
    "    events_types_to_compare = [multiElectdPerSS_text,singleElectdPerSS_text]\n",
    "    all_comp = preform_minmaxSD_comparison(events_types_to_compare,file_ids,minMax_sd_vers)\n",
    "    # all_comp.to_csv('ss_comparisons.csv',index=False)\n",
    "    display(all_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN this to get scoring events files only\n",
    "run_scoring_eventFiles = False\n",
    "if(run_scoring_eventFiles):\n",
    "    testType = 1\n",
    "    if(testType==1):\n",
    "        with open(import_path, \"rb\") as file: [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "        file_ids = allsubsdata_perFile.keys()\n",
    "        electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "\n",
    "\n",
    "    fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "    if not os.path.exists(fig_output_dir): os.mkdir(fig_output_dir)\n",
    "\n",
    "    edfViewFormat_events_manual_dir = f'{fig_output_dir}\\\\EDFViewFormat_events'\n",
    "    edfViewFormat_events_output_dir = f'{fig_output_dir}\\\\EDFViewFormat_events\\\\only_scoring'\n",
    "    if os.path.exists(edfViewFormat_events_output_dir):\n",
    "        shutil.rmtree(edfViewFormat_events_output_dir)\n",
    "    os.mkdir(edfViewFormat_events_output_dir)\n",
    "\n",
    "    all_electodes_sw_key = 'SW_AN'\n",
    "\n",
    "    edfviewFormatSuffix = 'efdViewFormat'\n",
    "\n",
    "    scoring_edfViewFormat_key = f'scoring_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_scoring(scoring_edfViewFormat_key)\n",
    "\n",
    "    events_types_to_save = ['scoring']    \n",
    "    save_eventsAllTogether_edfViewFormat(events_types_to_save,edfViewFormat_events_output_dir)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08-מרץ-23 00:12:22 | WARNING | Hypnogram is LONGER than data by 6.99 seconds. Cropping hypnogram to match data.size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub:38_2, before exclude sleep stages:20237, after:11281\n",
      "sub:38_2, before wave criteria sleep stages:11281, after:2324\n"
     ]
    }
   ],
   "source": [
    "## RUN THIS TO GET AN_SlowWaves events\n",
    "run_comparison_of_sw_minMax = True\n",
    "if(run_comparison_of_sw_minMax):\n",
    "    testType = 1\n",
    "    if(testType==1):\n",
    "        file_ids = ['38_2']\n",
    "        electrodes_names_eventDetect = ['Fp2','F3']\n",
    "    elif(testType==2):\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp2','F3']\n",
    "    elif(testType==3):\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "    else:\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "\n",
    "    with open(import_path, \"rb\") as file:  [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "    fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "    if not os.path.exists(fig_output_dir): os.mkdir(fig_output_dir)\n",
    "\n",
    "    edfViewFormat_events_manual_dir = f'{fig_output_dir}\\\\EDFViewFormat_events'\n",
    "    edfViewFormat_events_output_dir = f'{fig_output_dir}\\\\EDFViewFormat_events\\\\sw_AN_tests'\n",
    "    if os.path.exists(edfViewFormat_events_output_dir):\n",
    "        shutil.rmtree(edfViewFormat_events_output_dir)\n",
    "    os.mkdir(edfViewFormat_events_output_dir)\n",
    "\n",
    "    \n",
    "    all_electodes_sw_key = 'SW_AN'\n",
    "    res = detect_sw_AndrillonNir(file_ids,all_electodes_sw_key,electrodes_names_eventDetect)\n",
    "\n",
    "    after_sleepStage_exclution_key = f\"{all_electodes_sw_key}_n2n3\"\n",
    "    filter_sleep_stages(file_ids,all_electodes_sw_key,after_sleepStage_exclution_key, [2.0,3.0]) \n",
    "\n",
    "    sw_key_to_use = after_sleepStage_exclution_key\n",
    "    sw_key_after_SWExclusion = f\"{after_sleepStage_exclution_key}_swExclusion\"\n",
    "    filter_waveCriteria(file_ids,sw_key_to_use,sw_key_after_SWExclusion)\n",
    "\n",
    "    edfviewFormatSuffix = 'efdViewFormat'\n",
    "    SW_multiElectdPerSW_efdViewFormat_key = f'{sw_key_after_SWExclusion}_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_sw(file_ids,sw_key_after_SWExclusion,SW_multiElectdPerSW_efdViewFormat_key)\n",
    "\n",
    "    # events_types_to_compare = [multiElectdPerSW_text,singleElectdPerSW_text]    \n",
    "    # edfViewFormat_eventsTest_output_dir = f\"{edfViewFormat_events_output_dir}\\\\{minMax_sd_ver}\"\n",
    "    # if os.path.exists(edfViewFormat_eventsTest_output_dir):  shutil.rmtree(edfViewFormat_eventsTest_output_dir)\n",
    "    # os.mkdir(edfViewFormat_eventsTest_output_dir)\n",
    "    # save_eventsSepratelyForComparison_edfViewFormat(file_ids,events_types_to_compare,edfViewFormat_eventsTest_output_dir)\n",
    "\n",
    "    scoring_edfViewFormat_key = f'scoring_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_scoring(scoring_edfViewFormat_key)\n",
    "\n",
    "    events_types_to_save = ['scoring', SW_multiElectdPerSW_efdViewFormat_key]    \n",
    "    save_eventsAllTogether_edfViewFormat(events_types_to_save,edfViewFormat_events_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_kcompCriteria(file_ids,event_key_to_use,afterkcompCriteria_exclution_key):\n",
    "            # % Result Matrix\n",
    "        # %1:  wave beginning (sample)\n",
    "        # %2:  wave end (sample)\n",
    "        # %3:  wave middle point (sample)\n",
    "        # %4:  wave negative half-way (sample)\n",
    "        # %5:  period in seconds\n",
    "        # %6:  positive amplitude peak\n",
    "        # %7:  positive amplitude peak position (sample)\n",
    "        # %8:  negative amplitude peak\n",
    "        # %9:  negative amplitude peak position (sample)\n",
    "        # %10: peak-to-peak amplitude\n",
    "        # %11: 1st pos peak amplitude\n",
    "        # %12: 1st pos peak amplitude position (sample)\n",
    "        # %13: Last pos peak amplitude\n",
    "        # %14: Last pos peak amplitude position (sample)\n",
    "        # %15: 1st neg peak amplitude\n",
    "        # %16: 1st neg peak amplitude position (sample)\n",
    "        # %17: mean wave amplitude\n",
    "        # %18: number of positive peaks\n",
    "        # %19: wave negative half-way period\n",
    "        # %20: 1st peak to last peak period\n",
    "        # %21: determines instantaneous positive 1st segement slope on smoothed signal\n",
    "        # %22: determines maximal negative slope for 2nd segement\n",
    "        # %23: stage (if scored data)\n",
    "    for id in file_ids:\n",
    "        if event_key_to_use not in allsubsdata_perFile[id]: continue\n",
    "        event_allElectrodes = allsubsdata_perFile[id][event_key_to_use]\n",
    "        event_filtered_kcompCrit = event_allElectrodes[       (event_allElectrodes['n1'] < 45) \n",
    "                                                        &((event_allElectrodes['currentStage'] == 2.0) | (event_allElectrodes['currentStage'] == 3.0))\n",
    "                                                       ]\n",
    "        \n",
    "    \n",
    "        if len(event_filtered_kcompCrit) == 0:  print(f\"sub:{id} - no kcomp with criteria\")\n",
    "        else: print(f\"sub:{id}, before wave criteria sleep stages:{len(event_allElectrodes)}, after:{len(event_filtered_kcompCrit)}\")\n",
    "        \n",
    "        allsubsdata_perFile[id][afterkcompCriteria_exclution_key] = event_filtered_kcompCrit\n",
    "\n",
    "## RUN THIS TO GET AN_kcomplex events\n",
    "run_comparison_of_kcomp_minMax = False\n",
    "if(run_comparison_of_kcomp_minMax):\n",
    "    testType = 1\n",
    "    if(testType==1):\n",
    "        file_ids = ['38_2']\n",
    "        electrodes_names_eventDetect = ['Fp2']\n",
    "    elif(testType==2):\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp2','F3']\n",
    "    else:\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "\n",
    "    with open(import_path, \"rb\") as file:  [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "    fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "    if not os.path.exists(fig_output_dir): os.mkdir(fig_output_dir)\n",
    "\n",
    "    edfViewFormat_events_manual_dir = f'{fig_output_dir}\\\\EDFViewFormat_events'\n",
    "    edfViewFormat_events_output_dir = f'{fig_output_dir}\\\\EDFViewFormat_events\\\\kcomp_tests_AN'\n",
    "    if os.path.exists(edfViewFormat_events_output_dir):\n",
    "        shutil.rmtree(edfViewFormat_events_output_dir)\n",
    "    os.mkdir(edfViewFormat_events_output_dir)\n",
    "\n",
    "    all_electodes_kcomp_key = 'KComp'\n",
    "    detect_kComp_AndrillonNir(file_ids,all_electodes_kcomp_key,electrodes_names_eventDetect)\n",
    "\n",
    "    after_criteria_key = f\"{all_electodes_kcomp_key}_criteria\"\n",
    "    filter_kcompCriteria(file_ids,all_electodes_kcomp_key,after_criteria_key)\n",
    "\n",
    "    edfviewFormatSuffix = 'efdViewFormat'\n",
    "    kcomp_efdViewFormat_key = f'{after_criteria_key}_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_kcomp(file_ids,after_criteria_key,kcomp_efdViewFormat_key)\n",
    "\n",
    "    scoring_edfViewFormat_key = f'scoring_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_scoring(scoring_edfViewFormat_key)\n",
    "\n",
    "    events_types_to_save = ['scoring', kcomp_efdViewFormat_key]    \n",
    "    save_eventsAllTogether_edfViewFormat(events_types_to_save,edfViewFormat_events_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7838\n",
      "[['0.94' '0.21' 'kcomp@@Fp2']\n",
      " ['1.15' '1.84' 'kcomp@@Fp2']\n",
      " ['4.32' '0.42' 'kcomp@@Fp2']\n",
      " ...\n",
      " ['9177.81' '0.42' 'kcomp@@Fp2']\n",
      " ['9178.78' '0.21' 'kcomp@@Fp2']\n",
      " ['9178.99' '1.03' 'kcomp@@Fp2']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wavest</th>\n",
       "      <th>wavend</th>\n",
       "      <th>mdpt</th>\n",
       "      <th>poszx</th>\n",
       "      <th>period</th>\n",
       "      <th>abs(b)</th>\n",
       "      <th>bx</th>\n",
       "      <th>c</th>\n",
       "      <th>cx</th>\n",
       "      <th>maxb2c</th>\n",
       "      <th>...</th>\n",
       "      <th>p1</th>\n",
       "      <th>p1x</th>\n",
       "      <th>meanAmp</th>\n",
       "      <th>nump</th>\n",
       "      <th>nperiod/SR</th>\n",
       "      <th>p2p</th>\n",
       "      <th>mxdn</th>\n",
       "      <th>mxup</th>\n",
       "      <th>currentStage</th>\n",
       "      <th>electrode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.21</td>\n",
       "      <td>6.971936</td>\n",
       "      <td>1.02</td>\n",
       "      <td>5.045137</td>\n",
       "      <td>1.14</td>\n",
       "      <td>12.017073</td>\n",
       "      <td>...</td>\n",
       "      <td>5.045137</td>\n",
       "      <td>1.14</td>\n",
       "      <td>6.971936</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.040</td>\n",
       "      <td>802.804395</td>\n",
       "      <td>687.519515</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Fp2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.15</td>\n",
       "      <td>2.99</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.96</td>\n",
       "      <td>1.84</td>\n",
       "      <td>61.083939</td>\n",
       "      <td>1.66</td>\n",
       "      <td>79.951886</td>\n",
       "      <td>2.26</td>\n",
       "      <td>141.035826</td>\n",
       "      <td>...</td>\n",
       "      <td>79.951886</td>\n",
       "      <td>2.26</td>\n",
       "      <td>45.178569</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.224</td>\n",
       "      <td>2307.625153</td>\n",
       "      <td>2034.929110</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Fp2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.32</td>\n",
       "      <td>4.74</td>\n",
       "      <td>4.37</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>14.050622</td>\n",
       "      <td>4.40</td>\n",
       "      <td>53.656340</td>\n",
       "      <td>4.57</td>\n",
       "      <td>67.706962</td>\n",
       "      <td>...</td>\n",
       "      <td>53.656340</td>\n",
       "      <td>4.57</td>\n",
       "      <td>14.050622</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.074</td>\n",
       "      <td>2520.911287</td>\n",
       "      <td>1531.321931</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Fp2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.64</td>\n",
       "      <td>5.82</td>\n",
       "      <td>5.68</td>\n",
       "      <td>5.72</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.723316</td>\n",
       "      <td>5.71</td>\n",
       "      <td>6.597173</td>\n",
       "      <td>5.80</td>\n",
       "      <td>8.320489</td>\n",
       "      <td>...</td>\n",
       "      <td>6.597173</td>\n",
       "      <td>5.80</td>\n",
       "      <td>1.723316</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.066</td>\n",
       "      <td>406.329021</td>\n",
       "      <td>145.938325</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Fp2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.82</td>\n",
       "      <td>6.31</td>\n",
       "      <td>5.89</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0.49</td>\n",
       "      <td>16.218847</td>\n",
       "      <td>5.93</td>\n",
       "      <td>67.255603</td>\n",
       "      <td>6.23</td>\n",
       "      <td>83.474450</td>\n",
       "      <td>...</td>\n",
       "      <td>48.738177</td>\n",
       "      <td>6.09</td>\n",
       "      <td>16.218847</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.086</td>\n",
       "      <td>1415.443215</td>\n",
       "      <td>1382.480299</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Fp2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16788</th>\n",
       "      <td>9175.96</td>\n",
       "      <td>9176.98</td>\n",
       "      <td>9176.01</td>\n",
       "      <td>9176.06</td>\n",
       "      <td>1.02</td>\n",
       "      <td>6.695662</td>\n",
       "      <td>9176.03</td>\n",
       "      <td>29.953387</td>\n",
       "      <td>9176.39</td>\n",
       "      <td>36.649049</td>\n",
       "      <td>...</td>\n",
       "      <td>2.223227</td>\n",
       "      <td>9176.12</td>\n",
       "      <td>6.695662</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.128</td>\n",
       "      <td>962.165160</td>\n",
       "      <td>668.175160</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Fp2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16789</th>\n",
       "      <td>9176.98</td>\n",
       "      <td>9177.81</td>\n",
       "      <td>9177.26</td>\n",
       "      <td>9177.53</td>\n",
       "      <td>0.83</td>\n",
       "      <td>41.230901</td>\n",
       "      <td>9177.09</td>\n",
       "      <td>15.089083</td>\n",
       "      <td>9177.75</td>\n",
       "      <td>56.319984</td>\n",
       "      <td>...</td>\n",
       "      <td>12.683360</td>\n",
       "      <td>9177.63</td>\n",
       "      <td>38.672957</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.272</td>\n",
       "      <td>3098.875626</td>\n",
       "      <td>2274.378160</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Fp2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16790</th>\n",
       "      <td>9177.81</td>\n",
       "      <td>9178.23</td>\n",
       "      <td>9177.90</td>\n",
       "      <td>9177.98</td>\n",
       "      <td>0.42</td>\n",
       "      <td>22.681655</td>\n",
       "      <td>9177.91</td>\n",
       "      <td>12.188371</td>\n",
       "      <td>9178.08</td>\n",
       "      <td>34.870026</td>\n",
       "      <td>...</td>\n",
       "      <td>12.188371</td>\n",
       "      <td>9178.08</td>\n",
       "      <td>22.681655</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.066</td>\n",
       "      <td>1989.869484</td>\n",
       "      <td>1689.830890</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Fp2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16792</th>\n",
       "      <td>9178.78</td>\n",
       "      <td>9178.99</td>\n",
       "      <td>9178.86</td>\n",
       "      <td>9178.93</td>\n",
       "      <td>0.21</td>\n",
       "      <td>27.942224</td>\n",
       "      <td>9178.86</td>\n",
       "      <td>1.871429</td>\n",
       "      <td>9178.98</td>\n",
       "      <td>29.813653</td>\n",
       "      <td>...</td>\n",
       "      <td>1.871429</td>\n",
       "      <td>9178.98</td>\n",
       "      <td>27.942224</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.046</td>\n",
       "      <td>3643.961421</td>\n",
       "      <td>2215.146140</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Fp2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16793</th>\n",
       "      <td>9178.99</td>\n",
       "      <td>9180.02</td>\n",
       "      <td>9179.03</td>\n",
       "      <td>9179.07</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.068390</td>\n",
       "      <td>9179.06</td>\n",
       "      <td>41.548489</td>\n",
       "      <td>9179.34</td>\n",
       "      <td>44.616878</td>\n",
       "      <td>...</td>\n",
       "      <td>27.863522</td>\n",
       "      <td>9179.20</td>\n",
       "      <td>3.068390</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.072</td>\n",
       "      <td>625.460464</td>\n",
       "      <td>347.982134</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Fp2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7838 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        wavest   wavend     mdpt    poszx  period     abs(b)       bx  \\\n",
       "1         0.94     1.15     1.00     1.06    0.21   6.971936     1.02   \n",
       "2         1.15     2.99     1.56     1.96    1.84  61.083939     1.66   \n",
       "5         4.32     4.74     4.37     4.42    0.42  14.050622     4.40   \n",
       "7         5.64     5.82     5.68     5.72    0.18   1.723316     5.71   \n",
       "8         5.82     6.31     5.89     5.96    0.49  16.218847     5.93   \n",
       "...        ...      ...      ...      ...     ...        ...      ...   \n",
       "16788  9175.96  9176.98  9176.01  9176.06    1.02   6.695662  9176.03   \n",
       "16789  9176.98  9177.81  9177.26  9177.53    0.83  41.230901  9177.09   \n",
       "16790  9177.81  9178.23  9177.90  9177.98    0.42  22.681655  9177.91   \n",
       "16792  9178.78  9178.99  9178.86  9178.93    0.21  27.942224  9178.86   \n",
       "16793  9178.99  9180.02  9179.03  9179.07    1.03   3.068390  9179.06   \n",
       "\n",
       "               c       cx      maxb2c  ...         p1      p1x    meanAmp  \\\n",
       "1       5.045137     1.14   12.017073  ...   5.045137     1.14   6.971936   \n",
       "2      79.951886     2.26  141.035826  ...  79.951886     2.26  45.178569   \n",
       "5      53.656340     4.57   67.706962  ...  53.656340     4.57  14.050622   \n",
       "7       6.597173     5.80    8.320489  ...   6.597173     5.80   1.723316   \n",
       "8      67.255603     6.23   83.474450  ...  48.738177     6.09  16.218847   \n",
       "...          ...      ...         ...  ...        ...      ...        ...   \n",
       "16788  29.953387  9176.39   36.649049  ...   2.223227  9176.12   6.695662   \n",
       "16789  15.089083  9177.75   56.319984  ...  12.683360  9177.63  38.672957   \n",
       "16790  12.188371  9178.08   34.870026  ...  12.188371  9178.08  22.681655   \n",
       "16792   1.871429  9178.98   29.813653  ...   1.871429  9178.98  27.942224   \n",
       "16793  41.548489  9179.34   44.616878  ...  27.863522  9179.20   3.068390   \n",
       "\n",
       "       nump  nperiod/SR    p2p         mxdn         mxup  currentStage  \\\n",
       "1       1.0       0.024  0.040   802.804395   687.519515           3.0   \n",
       "2       2.0       0.162  0.224  2307.625153  2034.929110           3.0   \n",
       "5       1.0       0.020  0.074  2520.911287  1531.321931           3.0   \n",
       "7       1.0       0.016  0.066   406.329021   145.938325           3.0   \n",
       "8       1.0       0.028  0.086  1415.443215  1382.480299           3.0   \n",
       "...     ...         ...    ...          ...          ...           ...   \n",
       "16788   1.0       0.020  0.128   962.165160   668.175160           2.0   \n",
       "16789   2.0       0.110  0.272  3098.875626  2274.378160           2.0   \n",
       "16790   1.0       0.034  0.066  1989.869484  1689.830890           2.0   \n",
       "16792   1.0       0.030  0.046  3643.961421  2215.146140           2.0   \n",
       "16793   1.0       0.016  0.072   625.460464   347.982134           2.0   \n",
       "\n",
       "       electrode  \n",
       "1            Fp2  \n",
       "2            Fp2  \n",
       "5            Fp2  \n",
       "7            Fp2  \n",
       "8            Fp2  \n",
       "...          ...  \n",
       "16788        Fp2  \n",
       "16789        Fp2  \n",
       "16790        Fp2  \n",
       "16792        Fp2  \n",
       "16793        Fp2  \n",
       "\n",
       "[7838 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "event_allElectrodes = allsubsdata_perFile['38_2']['KComp_criteria']\n",
    "event_filtered_kcompCrit = pd.DataFrame([], columns=event_allElectrodes.columns)\n",
    "\n",
    "neg_events_filtered = event_allElectrodes[       (event_allElectrodes['n1'] < 40) &\n",
    "                                                ((event_allElectrodes['currentStage'] == 2.0) | (event_allElectrodes['currentStage'] == 3.0))\n",
    "                                                ]\n",
    "neg_events_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "exclude_indxs = np.zeros([], dtype=int)\n",
    "for index, row in neg_events_filtered.iterrows():\n",
    "    if index == 0: continue\n",
    "    #print(event_filtered_neg.iloc[index-1]['wavend'] +1)\n",
    "    if neg_events_filtered.iloc[index-1]['n1x'] +1.5 >=neg_events_filtered.iloc[index]['n1x']: # filtered minimas that are seperared by at leaset 1.5s\n",
    "        exclude_indxs = np.append(exclude_indxs, index)\n",
    "\n",
    "ex = neg_events_filtered.drop(exclude_indxs, axis=0)\n",
    "ex.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(np.size(allsubsdata_perFile['38_2']['KComp_criteria'],0))\n",
    "print(allsubsdata_perFile['38_2']['KComp_criteria_efdViewFormat'])\n",
    "display(allsubsdata_perFile['38_2']['KComp_criteria'])\n",
    "a= allsubsdata_perFile['38_2']['KComp_criteria']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07-מרץ-23 16:35:55 | WARNING | Hypnogram is LONGER than data by 6.99 seconds. Cropping hypnogram to match data.size.\n"
     ]
    }
   ],
   "source": [
    "## RUN Sridhar_kcomplex events\n",
    "\n",
    "# Sample rate and desired cutoff frequencies (in Hz).\n",
    "wanted_freq = 100\n",
    "lowcut = 0.25\n",
    "highcut = 6.0\n",
    "\n",
    "number_of_sec_in_epoch = 4\n",
    "distance_in_sec = 1.5\n",
    "distance_in_samples = wanted_freq * distance_in_sec\n",
    "min_negPos_gap = 0.2 # in sec\n",
    "max_negPos_gap = 1.5 # in sec\n",
    "range_for_pos_peak = 1 #in sec\n",
    "\n",
    "kcomp_df_columns = ['pos_i','pos_amp','neg_i','neg_amp',electrode_column_name]\n",
    "kcomp_df =  pd.DataFrame([], columns = kcomp_df_columns)\n",
    "\n",
    "def detect_kComp_Srid(file_ids,output_key,electrodes_names):\n",
    "    for id in file_ids:\n",
    "        kcomp_df =  pd.DataFrame([], columns = kcomp_df_columns)\n",
    "\n",
    "        datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "        datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "        scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'])\n",
    "        \n",
    "        for electrode_name in electrodes_names:\n",
    "            curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name)[0][0]\n",
    "            data = allsubsdata_perFile[id]['data'][curr_electrode_num] ## shape (time)\n",
    "            z = wanted_freq / configu['sample_freq']\n",
    "            resampled = signal.resample(data, int(np.size(data)*z)) ## downsample to 100hz\n",
    "            filtered = butter_bandpass_filter(resampled, lowcut, highcut, wanted_freq, order=6) ## oreder can be changed. see: https://stackoverflow.com/questions/12093594/how-to-implement-band-pass-butterworth-filter-with-scipy-signal-butter\n",
    "\n",
    "            for epoch_i in np.arange(np.size(filtered)/wanted_freq/number_of_sec_in_epoch,dtype=int):\n",
    "                if epoch_i == 0: continue\n",
    "                curr_epoch_orig_i = int(epoch_i*wanted_freq*number_of_sec_in_epoch/z)\n",
    "                if (scoring_upsampled[curr_epoch_orig_i] ==2 or scoring_upsampled[curr_epoch_orig_i] ==3):\n",
    "                    curr_start_time = (epoch_i-1)*wanted_freq*number_of_sec_in_epoch\n",
    "                    curr_end_time = epoch_i*wanted_freq*number_of_sec_in_epoch\n",
    "                    epoch_data = filtered[int(curr_start_time) : int(curr_end_time)]\n",
    "                    scaled_epoch_data = max(epoch_data) - epoch_data; #scale the signal by the maximum value.\n",
    "                    \n",
    "                    curr_minimas_inds, _ = find_peaks(-epoch_data, height=40, distance=distance_in_samples)\n",
    "\n",
    "                    for minima_ind in curr_minimas_inds:\n",
    "                        if epoch_data[minima_ind] < -40:\n",
    "                            start_range = max(0, minima_ind-(range_for_pos_peak *wanted_freq))\n",
    "                            end_range = min((number_of_sec_in_epoch *wanted_freq)-1, minima_ind+(range_for_pos_peak *wanted_freq))               \n",
    "                            \n",
    "                            curr_maximas_inds, _ = find_peaks(epoch_data[start_range:end_range], height=0)\n",
    "                            if np.size(curr_minimas_inds) ==0 or np.size(epoch_data[curr_maximas_inds])==0: continue\n",
    "                            curr_maximas_inds = curr_maximas_inds+start_range\n",
    "                            \n",
    "                            max_ind = curr_maximas_inds[np.argmax(epoch_data[curr_maximas_inds])] \n",
    "                            \n",
    "                            if np.abs(max_ind - minima_ind) < max_negPos_gap*wanted_freq and np.abs(max_ind - minima_ind) > min_negPos_gap*wanted_freq:\n",
    "                                ## TODO: check if the negative peak is below ~45milivolt\n",
    "                                if epoch_data[max_ind] >= 2* epoch_data[minima_ind]:\n",
    "                                    if np.abs(epoch_data[max_ind] - epoch_data[minima_ind]) > 100:\n",
    "                                        new_row = {'pos_i':(curr_start_time+max_ind)/z, \n",
    "                                                            'pos_amp':epoch_data[max_ind], \n",
    "                                                            'neg_i':(curr_start_time+minima_ind)/z, \n",
    "                                                            'neg_amp':epoch_data[minima_ind],\n",
    "                                                            electrode_column_name:electrode_name}\n",
    "                                        #kcomp_df = kcomp_df.append(new_row, ignore_index=True)\n",
    "                                        new_row_df = pd.DataFrame([new_row], columns = kcomp_df_columns)\n",
    "                                        kcomp_df = pd.concat([kcomp_df,new_row_df])\n",
    "\n",
    "        if not kcomp_df.empty:\n",
    "            allsubsdata_perFile[id][output_key]  = kcomp_df\n",
    "def add_edfViewFormat_kcomp_Srid(file_ids,event_key_for_save, kcomp_efdViewFormat_key):\n",
    "    for id in file_ids:\n",
    "        if event_key_for_save not in allsubsdata_perFile[id]:\n",
    "                #print(f\"no {event_key_for_save} for sub {id}\")\n",
    "                continue\n",
    "        kcomp_df = allsubsdata_perFile[id][event_key_for_save]\n",
    "\n",
    "        startTime_arr = np.array(kcomp_df['neg_i'])\n",
    "        endTime_arr = np.array(kcomp_df['pos_i']) \n",
    "        duration_arr = np.round((endTime_arr - startTime_arr)/configu['sample_freq'],2)\n",
    "        electd_arr_per_sw = np.array(kcomp_df[electrode_column_name])\n",
    "        desc = [f\"kcomp@@{electd_arr}\" for electd_arr in electd_arr_per_sw]\n",
    "        new_format_kcomp = np.array([startTime_arr /configu['sample_freq'],duration_arr,desc]).T ## onset (sec), duration,desc\n",
    "\n",
    "        allsubsdata_perFile[id][kcomp_efdViewFormat_key] = new_format_kcomp\n",
    "\n",
    "## RUN THIS TO GET Sridhar_kcomplex events\n",
    "run_kcomp_srid = False\n",
    "if(run_kcomp_srid):\n",
    "    testType = 1\n",
    "    if(testType==1):\n",
    "        file_ids = ['38_2']\n",
    "        electrodes_names_eventDetect = ['Fp2']\n",
    "    elif(testType==2):\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp2','F3']\n",
    "    else:\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "\n",
    "    with open(import_path, \"rb\") as file:  [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "    fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "    if not os.path.exists(fig_output_dir): os.mkdir(fig_output_dir)\n",
    "\n",
    "    edfViewFormat_events_manual_dir = f'{fig_output_dir}\\\\EDFViewFormat_events'\n",
    "    edfViewFormat_events_output_dir = f'{fig_output_dir}\\\\EDFViewFormat_events\\\\kcomp_tests_Sridhar'\n",
    "    if os.path.exists(edfViewFormat_events_output_dir):\n",
    "        shutil.rmtree(edfViewFormat_events_output_dir)\n",
    "    os.mkdir(edfViewFormat_events_output_dir)\n",
    "\n",
    "    all_electodes_kcompSrid_key = 'KComp_Srid'\n",
    "    res = detect_kComp_Srid(file_ids,all_electodes_kcompSrid_key,electrodes_names_eventDetect)\n",
    "\n",
    "    edfviewFormatSuffix = 'efdViewFormat'\n",
    "    kcomp_efdViewFormat_key = f'{all_electodes_kcompSrid_key}_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_kcomp_Srid(file_ids,all_electodes_kcompSrid_key,kcomp_efdViewFormat_key)\n",
    "\n",
    "    scoring_edfViewFormat_key = f'scoring_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_scoring(scoring_edfViewFormat_key)\n",
    "\n",
    "    events_types_to_save = ['scoring', kcomp_efdViewFormat_key]    \n",
    "    save_eventsAllTogether_edfViewFormat(events_types_to_save,edfViewFormat_events_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08-מרץ-23 03:43:18 | WARNING | Hypnogram is LONGER than data by 3.59 seconds. Cropping hypnogram to match data.size.\n",
      "08-מרץ-23 03:43:19 | WARNING | Hypnogram is LONGER than data by 6.99 seconds. Cropping hypnogram to match data.size.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>pos_i</th>\n",
       "      <th>pos_amp</th>\n",
       "      <th>neg_i</th>\n",
       "      <th>neg_amp</th>\n",
       "      <th>electrode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>301625.0</td>\n",
       "      <td>126.403798</td>\n",
       "      <td>301375.0</td>\n",
       "      <td>-118.909419</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>306680.0</td>\n",
       "      <td>72.792779</td>\n",
       "      <td>306265.0</td>\n",
       "      <td>-68.405855</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>311660.0</td>\n",
       "      <td>198.979489</td>\n",
       "      <td>311370.0</td>\n",
       "      <td>-97.359740</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>325185.0</td>\n",
       "      <td>112.552360</td>\n",
       "      <td>324930.0</td>\n",
       "      <td>-71.395604</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>336840.0</td>\n",
       "      <td>121.246554</td>\n",
       "      <td>336590.0</td>\n",
       "      <td>-101.798268</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>4428520.0</td>\n",
       "      <td>146.594144</td>\n",
       "      <td>4428215.0</td>\n",
       "      <td>-96.804175</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0</td>\n",
       "      <td>4430895.0</td>\n",
       "      <td>96.296364</td>\n",
       "      <td>4430730.0</td>\n",
       "      <td>-106.690510</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0</td>\n",
       "      <td>4433405.0</td>\n",
       "      <td>61.609141</td>\n",
       "      <td>4433215.0</td>\n",
       "      <td>-119.192816</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>4495285.0</td>\n",
       "      <td>84.974502</td>\n",
       "      <td>4495010.0</td>\n",
       "      <td>-104.821969</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "      <td>4496270.0</td>\n",
       "      <td>118.972152</td>\n",
       "      <td>4496030.0</td>\n",
       "      <td>-49.009223</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index      pos_i     pos_amp      neg_i     neg_amp electrode\n",
       "0        0   301625.0  126.403798   301375.0 -118.909419        F4\n",
       "1        0   306680.0   72.792779   306265.0  -68.405855        F4\n",
       "2        0   311660.0  198.979489   311370.0  -97.359740        F4\n",
       "3        0   325185.0  112.552360   324930.0  -71.395604        F4\n",
       "4        0   336840.0  121.246554   336590.0 -101.798268        F4\n",
       "..     ...        ...         ...        ...         ...       ...\n",
       "131      0  4428520.0  146.594144  4428215.0  -96.804175        F4\n",
       "132      0  4430895.0   96.296364  4430730.0 -106.690510        F4\n",
       "133      0  4433405.0   61.609141  4433215.0 -119.192816        F4\n",
       "134      0  4495285.0   84.974502  4495010.0 -104.821969        F4\n",
       "135      0  4496270.0  118.972152  4496030.0  -49.009223        F4\n",
       "\n",
       "[136 rows x 6 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN ShaYKM_kcomplex events\n",
    "\n",
    "# Sample rate and desired cutoff frequencies (in Hz).\n",
    "wanted_freq = 100\n",
    "\n",
    "distance_in_sec = 1.5\n",
    "distance_in_sw = wanted_freq * distance_in_sec\n",
    "min_negPos_gap = 0.2 # in sec\n",
    "max_negPos_gap = 1.5 # in sec\n",
    "range_for_pos_peak = 1 #in sec\n",
    "\n",
    "kcomps_dur_min = 0.5 # sec\n",
    "\n",
    "sw_min_width = 0.3 # sec\n",
    "\n",
    "kcomp_df_columns = ['pos_i','pos_amp','neg_i','neg_amp',electrode_column_name]\n",
    "kcomp_df =  pd.DataFrame([], columns = kcomp_df_columns)\n",
    "\n",
    "def detect_kComp_shaYKM(file_ids,output_key,electrodes_names):\n",
    "    for id in file_ids:\n",
    "        kcomp_df =  pd.DataFrame([], columns = kcomp_df_columns)\n",
    "\n",
    "        datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "        datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "        scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'])\n",
    "        \n",
    "        for electrode_name in electrodes_names:\n",
    "            curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name)[0][0]\n",
    "            data = allsubsdata_perFile[id]['data'][curr_electrode_num] ## shape (time)\n",
    "            z = wanted_freq / configu['sample_freq']\n",
    "            resampled = signal.resample(data, int(np.size(data)*z)) ## downsample to 100hz\n",
    "\n",
    "            smoothing_window = 3\n",
    "            smoothen  = scipy.signal.medfilt(resampled, smoothing_window)\n",
    "            lowcut = 0.25\n",
    "            highcut = 3.0\n",
    "            filer_order = 3\n",
    "            filtered_sw = butter_bandpass_filter(smoothen, lowcut, highcut, wanted_freq, order=filer_order) ## oreder can be changed. see: https://stackoverflow.com/questions/12093594/how-to-implement-band-pass-butterworth-filter-with-scipy-signal-butter\n",
    "            \n",
    "            smoothing_window = 3\n",
    "            smoothen  = scipy.signal.medfilt(resampled, smoothing_window)\n",
    "            lowcut = 0.5\n",
    "            highcut = 6.0\n",
    "            filer_order = 3\n",
    "            filtered_transiantMinima = butter_bandpass_filter(smoothen, lowcut, highcut, wanted_freq, order=filer_order)\n",
    "            \n",
    "            ## find all standalone minimas\n",
    "            curr_maximas_inds, _ = find_peaks(filtered_sw, threshold=0,width=sw_min_width*wanted_freq, distance=distance_in_sw)\n",
    "            for maxima_ind in curr_maximas_inds:\n",
    "                if (scoring_upsampled[int(maxima_ind/z)] ==2):\n",
    "                    # plt.plot(resampled[max(minima_ind-200,0):min(minima_ind+200,np.size(resampled)-1)])\n",
    "                    # plt.plot(filtered_sw[max(minima_ind-200,0):min(minima_ind+200,np.size(resampled)-1)])\n",
    "                    # plt.plot(filtered_notSW[max(minima_ind-200,0):min(minima_ind+200,np.size(resampled)-1)])\n",
    "                    # plt.show()\n",
    "                    maxima = filtered_sw[maxima_ind]\n",
    "\n",
    "                    range_for_neg_peak = 1 #in sec\n",
    "                    minima_scope = filtered_transiantMinima[maxima_ind - int(range_for_neg_peak*wanted_freq) : maxima_ind]\n",
    "                    distance_in_preSW = 1\n",
    "                    curr_minimas_inds, _ = find_peaks(-minima_scope, threshold=0,distance=distance_in_preSW)\n",
    "\n",
    "                    if np.size(curr_minimas_inds) ==0 : continue\n",
    "\n",
    "                    min_minima_ind_inScope = curr_minimas_inds[np.argmin(minima_scope[curr_minimas_inds])]\n",
    "                    minima = minima_scope[min_minima_ind_inScope]\n",
    "                    minima_ind = min_minima_ind_inScope + (maxima_ind - int(range_for_neg_peak*wanted_freq))\n",
    "\n",
    "                    if ((maxima_ind - minima_ind) > kcomps_dur_min *wanted_freq )and (minima < maxima-100) and (minima<-45) and (maxima > 0.5*abs(minima)): ## found a k-comp!\n",
    "                        new_row =           {'pos_i':maxima_ind/z, \n",
    "                                            'pos_amp':maxima, \n",
    "                                            'neg_i':minima_ind/z, \n",
    "                                            'neg_amp':minima,\n",
    "                                            electrode_column_name:electrode_name}\n",
    "                        new_row_df = pd.DataFrame([new_row], columns = kcomp_df_columns)\n",
    "                        kcomp_df = pd.concat([kcomp_df,new_row_df])\n",
    "            \n",
    "        if not kcomp_df.empty:\n",
    "            allsubsdata_perFile[id][output_key]  = kcomp_df\n",
    "        else: print(f'no events: {output_key}')\n",
    "            \n",
    "def add_edfViewFormat_kcomp_shaYKM(file_ids,event_key_for_save, kcomp_efdViewFormat_key):\n",
    "    for id in file_ids:\n",
    "        if event_key_for_save not in allsubsdata_perFile[id]:\n",
    "                #print(f\"no {event_key_for_save} for sub {id}\")\n",
    "                continue\n",
    "        kcomp_df = allsubsdata_perFile[id][event_key_for_save]\n",
    "\n",
    "        startTime_arr = np.array(kcomp_df['neg_i'])\n",
    "        endTime_arr = np.array(kcomp_df['pos_i']) \n",
    "        duration_arr = np.round((endTime_arr - startTime_arr)/configu['sample_freq'],2)\n",
    "        electd_arr_per_sw = np.array(kcomp_df[electrode_column_name])\n",
    "        desc = [f\"kcomp@@{electd_arr}\" for electd_arr in electd_arr_per_sw]\n",
    "        new_format_kcomp = np.array([startTime_arr /configu['sample_freq'],duration_arr,desc]).T ## onset (sec), duration,desc\n",
    "\n",
    "        allsubsdata_perFile[id][kcomp_efdViewFormat_key] = new_format_kcomp\n",
    "\n",
    "\n",
    "run_kcomp_shaYKM = True\n",
    "if(run_kcomp_shaYKM):\n",
    "    testType = 2\n",
    "    if(testType==1):\n",
    "        file_ids = ['32_2']\n",
    "        electrodes_names_eventDetect = ['F4']\n",
    "    elif(testType==2):\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['F4']\n",
    "    else:\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "\n",
    "    with open(import_path, \"rb\") as file:  [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "    fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "    if not os.path.exists(fig_output_dir): os.mkdir(fig_output_dir)\n",
    "\n",
    "    edfViewFormat_events_manual_dir = f'{fig_output_dir}\\\\EDFViewFormat_events'\n",
    "    edfViewFormat_events_output_dir = f'{fig_output_dir}\\\\EDFViewFormat_events\\\\kcomp_tests_ShaYKM'\n",
    "    if os.path.exists(edfViewFormat_events_output_dir):\n",
    "        shutil.rmtree(edfViewFormat_events_output_dir)\n",
    "    os.mkdir(edfViewFormat_events_output_dir)\n",
    "\n",
    "    all_electodes_kcompSrid_key = 'KComp_shaYKM'\n",
    "    res = detect_kComp_shaYKM(file_ids,all_electodes_kcompSrid_key,electrodes_names_eventDetect)\n",
    "\n",
    "    edfviewFormatSuffix = 'efdViewFormat'\n",
    "    kcomp_efdViewFormat_key = f'{all_electodes_kcompSrid_key}_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_kcomp_shaYKM(file_ids,all_electodes_kcompSrid_key,kcomp_efdViewFormat_key)\n",
    "\n",
    "    scoring_edfViewFormat_key = f'scoring_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_scoring(scoring_edfViewFormat_key)\n",
    "\n",
    "    events_types_to_save = ['scoring', kcomp_efdViewFormat_key]    \n",
    "    save_eventsAllTogether_edfViewFormat(events_types_to_save,edfViewFormat_events_output_dir)\n",
    "\n",
    "allsubsdata_perFile['32_2'][all_electodes_kcompSrid_key].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f544ce1a915a9875fad91c894e2c0bcad4b7a79945aa6027ef3ad27810072aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
