{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.stats\n",
    "import mne\n",
    "from mne.time_frequency import tfr_morlet\n",
    "from mne.stats import permutation_cluster_1samp_test\n",
    "import gc\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "from os.path import exists\n",
    "import mne\n",
    "import numpy as np\n",
    "from mne import create_info\n",
    "from IPython.utils import io\n",
    "import yasa\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matlab\n",
    "import matlab.engine\n",
    "import os\n",
    "import shutil\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import freqz\n",
    "from scipy import signal\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    return butter(order, [lowcut, highcut], fs=fs, btype='band')\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def overlap(a, b):\n",
    "    return a[1] >= b[0] and a[0] <= b[1]\n",
    "\n",
    "def restart_output_dir(path):\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    os.mkdir(path)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = \"C:\\\\Users\\\\User\\\\Cloud-Drive\\\\BigFiles\\\\OmissionExpOutput\\\\eventDetection\\\\imported_eventDetectionChan\\\\no_filters\"\n",
    "import_type = \"eventDetectionChan\"\n",
    "event_detection_dir_name = 'eventDetection'\n",
    "import_path = f'{pkl_dir}\\\\{import_type}_3.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(import_path, \"rb\") as file:  [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "\n",
    "main_output_dir = f\"{configu['outputs_dir_path']}\\\\{event_detection_dir_name}\"\n",
    "edf_output_dir = f\"{main_output_dir}\\\\EDFViewFormat_events\"\n",
    "csv_output_dir = f\"{main_output_dir}\\\\detected_csv\"\n",
    "manual_detection_dir = f\"{edf_output_dir}\\\\manual_annotation\"\n",
    "if not os.path.exists(main_output_dir): os.mkdir(main_output_dir)\n",
    "if not os.path.exists(edf_output_dir): os.mkdir(edf_output_dir)\n",
    "\n",
    "\n",
    "edfviewFormatSuffix = 'efdViewFormat'\n",
    "multiElectd_text = \"multiElectd\"\n",
    "singleElectd_text = \"singleElectd\"\n",
    "scoring_edfViewFormat_key = f'scoring_{edfviewFormatSuffix}'\n",
    "electrode_column_name = 'electrode'\n",
    "header = np.array(['Onset',\"Duration\",\"Annotation\"])\n",
    "spindles_output_columns =  ['spindleStartTime', 'spindleEndTime', 'peakTime', 'peakEnergy', 'peakEnergyNorm', 'freqSpindle', 'spindleDuration/SR', 'PowerSP', 'PowerAlpha', 'sleepStage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_electrode_num = np.where(configu['electrodes_names'] == 'C3')[0][0]\n",
    "# data = allsubsdata_perFile['32_2']['data'][curr_electrode_num]\n",
    "\n",
    "# epochnum = 4\n",
    "# epochstart = 30 * configu['sample_freq'] * epochnum\n",
    "# epochEnd = epochstart + 30*configu['sample_freq']\n",
    "# # plt.plot(data[324930-100:100+325175])\n",
    "# plt.plot(data[epochstart:epochEnd])\n",
    "# plt.rcParams[\"figure.figsize\"] = (40,2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SS\n",
    "run_comparison_of_spindle_minMax = False\n",
    "run_csvsave_ss = True\n",
    "\n",
    "# Scoring\n",
    "run_scoring_eventFiles = False\n",
    "\n",
    "## Slow waves\n",
    "run_comparison_of_sw_minMax = False\n",
    "\n",
    "# k-comp\n",
    "run_kcomp_shaYKM = False\n",
    "run_kcomp_shaYKM_comparison = False\n",
    "run_csvsave_kcomp = True\n",
    "\n",
    "\n",
    "testType = 0\n",
    "if(testType==1):\n",
    "    file_ids = ['32_2']\n",
    "    minMax_sd_vers = [[4,10,1]]\n",
    "    electrodes_names_detect = ['Fp2','F3']\n",
    "    electrodes_names_detect =  ['Fp2','F3']\n",
    "    electrodes_names_detect = ['Fp2','F3']\n",
    "elif(testType==2):\n",
    "    file_ids = ['32_2','38_2']\n",
    "    minMax_sd_vers = [[3,9,1],[4,10,1]]\n",
    "    electrodes_names_detect = ['Fp2','F3']\n",
    "    electrodes_names_detect =  ['Fp2','F3']\n",
    "    electrodes_names_detect = ['Fp2','F3']\n",
    "elif(testType==3):\n",
    "    file_ids = ['32_2','38_2']\n",
    "    minMax_sd_vers = [[3,9,1],[3,10,1],[4,10,1]]\n",
    "    electrodes_names_detect = configu['electrodes_names']\n",
    "elif(testType==4):\n",
    "    file_ids = ['32_2','38_2']\n",
    "    electrodes_names_detect =  configu['electrodes_names']\n",
    "    minMax_sd_vers = [[3,8,1],[4,8,1],[3,9,1],[4,9,1],[3,10,1],[4,10,1]]\n",
    "else:\n",
    "    subs = configu['subs'] \n",
    "    file_ids = allsubsdata_perFile.keys()\n",
    "    electrodes_names_detect = configu['electrodes_names']\n",
    "    csv_output_final_ver = [4,10,1]\n",
    "    minMax_sd_vers = [[3,8,1],[4,8,1],[3,9,1],[4,9,1],[3,10,1],[4,10,1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sleep_stages(file_ids,event_key_to_use,after_sleepStage_exclution_key, sleepstages):\n",
    "    for id in file_ids:\n",
    "        if event_key_to_use not in allsubsdata_perFile[id]: continue\n",
    "        event_allElectrodes = allsubsdata_perFile[id][event_key_to_use]\n",
    "        \n",
    "        event_filtered_sleepstages = pd.DataFrame([], columns=event_allElectrodes.columns)\n",
    "        for sleepstage in sleepstages:\n",
    "            event_filtered_sleepstages = pd.concat([event_filtered_sleepstages , event_allElectrodes.loc[(event_allElectrodes['sleepStage'] == sleepstage)]])\n",
    "\n",
    "        if len(event_filtered_sleepstages) == 0:  print(f\"sub:{id} - no events in sleepstages{sleepstages}\")\n",
    "        else: print(f\"sub:{id}, before exclude sleep stages:{len(event_allElectrodes)}, after:{len(event_filtered_sleepstages)}\")\n",
    "        \n",
    "        allsubsdata_perFile[id][after_sleepStage_exclution_key] = event_filtered_sleepstages\n",
    "\n",
    "def edfViewFormat_scoring_dict(score):\n",
    "    if score == 0:\n",
    "        return 'W'\n",
    "    elif score ==1:\n",
    "        return 'N1'\n",
    "    elif score ==2:\n",
    "        return 'N2'\n",
    "    elif score ==3:\n",
    "        return 'N3'\n",
    "    elif score ==4:\n",
    "        return 'TREM'\n",
    "    elif score ==5:\n",
    "        return 'PREM'\n",
    "    elif score ==6:\n",
    "        return 'MOVE'\n",
    "    elif score ==7:\n",
    "        return 'ARTIFACT'\n",
    "    else:\n",
    "        Exception('no such score')\n",
    "def add_edfViewFormat_scoring(key_edfScoringFormat):\n",
    "    for id in allsubsdata_perFile:\n",
    "        curr_file_scoring = allsubsdata_perFile[id]['scoring']\n",
    "        new_format_score = np.zeros((len(curr_file_scoring),3), dtype=object)\n",
    "        for ind, score in enumerate(curr_file_scoring):\n",
    "                new_format_score[ind,:] = [30*ind,30,edfViewFormat_scoring_dict(score)] ## onset (sec), duration, desc\n",
    "\n",
    "        allsubsdata_perFile[id][key_edfScoringFormat] = new_format_score\n",
    "\n",
    "# Save events (scoring and ss) per file, in a format suitable for EDF_viewer\n",
    "def save_eventsAllTogether_edfViewFormat(events_types_for_save,edfViewFormat_output_dir):\n",
    "        for id in file_ids:\n",
    "            filename = f\"{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_events\"\n",
    "            all_events_with_header = np.asarray([header],dtype=object)\n",
    "            for event_type in events_types_for_save:\n",
    "                events_type_found = [ v for k, v in allsubsdata_perFile[id].items() if (event_type in k) and (edfviewFormatSuffix in k)]\n",
    "                if np.size(events_type_found) == 0: continue\n",
    "                for events_found in events_type_found:\n",
    "                        for event_found in events_found:\n",
    "                                all_events_with_header = np.concatenate((all_events_with_header,[event_found]),dtype=object)\n",
    "            np.savetxt(f\"{edfViewFormat_output_dir}\\\\allEvents_{filename}.txt\", all_events_with_header, delimiter='\\t',fmt='%s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(run_scoring_eventFiles):\n",
    "    edfViewFormat_onlyScoring_output_dir = f'{edf_output_dir}\\\\only_scoring'\n",
    "    restart_output_dir(edfViewFormat_onlyScoring_output_dir)\n",
    "\n",
    "    add_edfViewFormat_scoring(scoring_edfViewFormat_key)\n",
    "\n",
    "    events_types_to_save = ['scoring']    \n",
    "    save_eventsAllTogether_edfViewFormat(events_types_to_save,edfViewFormat_onlyScoring_output_dir)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28-מרץ-23 15:45:35 | WARNING | Hypnogram is LONGER than data by 6.27 seconds. Cropping hypnogram to match data.size.\n",
      "28-מרץ-23 15:46:39 | WARNING | Hypnogram is LONGER than data by 6.99 seconds. Cropping hypnogram to match data.size.\n",
      "28-מרץ-23 15:48:00 | WARNING | Hypnogram is LONGER than data by 5.58 seconds. Cropping hypnogram to match data.size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub:38_1, before exclude sleep stages:2606, after:2091\n",
      "sub:38_2, before exclude sleep stages:2777, after:2080\n",
      "sub:38_3, before exclude sleep stages:2140, after:1837\n",
      "sub:38_1, before filt:2091, after:596, after>1:454\n",
      "sub:38_2, before filt:2080, after:551, after>1:412\n",
      "sub:38_3, before filt:1837, after:438, after>1:350\n"
     ]
    }
   ],
   "source": [
    "spindles_output_columns =  ['spindleStartTime', 'spindleEndTime', 'peakTime', 'peakEnergy', 'peakEnergyNorm', 'freqSpindle', 'spindleDuration/SR', 'PowerSP', 'PowerAlpha', 'sleepStage']\n",
    "\n",
    "def detect_ss_AndrillonNir(file_ids,minMax_sd_ver,output_key,electrodes_names):\n",
    "    andriNir_code_dir = os.getcwd()\n",
    "    andriNir_output_dir = f\"{edf_output_dir}\\\\Andrillon_Nir\"\n",
    "    andriNir_aux_output_dir = f\"{andriNir_output_dir}\\\\aux_mats\"\n",
    "\n",
    "    if os.path.exists(andriNir_output_dir):\n",
    "        shutil.rmtree(andriNir_output_dir)\n",
    "    os.mkdir(andriNir_output_dir)\n",
    "    os.mkdir(andriNir_aux_output_dir)\n",
    "\n",
    "    for id in file_ids:\n",
    "        datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "        datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "        scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=True)\n",
    "        ss_allElectrodes = pd.DataFrame()\n",
    "\n",
    "        for electd_i, electrode_name_eventDetect in enumerate(electrodes_names):\n",
    "            curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name_eventDetect)[0][0]\n",
    "\n",
    "            ## create aux files to use in MATLAB\n",
    "            datafile_1elect_eeg = datafile_data[curr_electrode_num,:]\n",
    "            if 2 not in datafile_scoring or 3 not in datafile_scoring:\n",
    "                continue\n",
    "            \n",
    "            DetectionThreshold = minMax_sd_ver[0]\n",
    "            RejectThreshold = minMax_sd_ver[1]\n",
    "            StartEndThreshold = minMax_sd_ver[2]\n",
    "            mat_to_save = {'datafile_data': datafile_1elect_eeg, 'scoring_upsampled': scoring_upsampled, 'sample_freq': configu['sample_freq'], 'electrode_name':electrode_name_eventDetect, 'DetectionThreshold':DetectionThreshold,'RejectThreshold':RejectThreshold, 'StartEndThreshold':StartEndThreshold}\n",
    "            scipy.io.savemat(f\"{andriNir_aux_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\",mat_to_save)\n",
    "\n",
    "            ## run Andrillon & Nir SS detection over all subjects files\n",
    "            eng = matlab.engine.start_matlab()\n",
    "            eng.cd(andriNir_code_dir, nargout=0) # type: ignore\n",
    "            out = eng.batch_useAndrillonNirSSDetection(andriNir_aux_output_dir, andriNir_output_dir,nargout=0)\n",
    "            eng.quit() # type: ignore\n",
    "\n",
    "            ## add the spindles data to the main subject dictionary\n",
    "            try: \n",
    "                spindles_file_name = f\"{andriNir_output_dir}\\\\SS_AN_{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\"\n",
    "                matlabImport = scipy.io.loadmat(spindles_file_name, simplify_cells=True)\n",
    "            except Exception: \n",
    "                print(f\"Error importing spindles sub file at: {allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}\")\n",
    "                continue\n",
    "\n",
    "            ss = matlabImport['spindles']\n",
    "            if np.size(np.shape(ss)) == 1: \n",
    "                temp = np.zeros((1,np.size(ss)),dtype=object)\n",
    "                temp[0] = ss\n",
    "                ss = temp\n",
    "            if np.size(ss) >0 :\n",
    "                df = pd.DataFrame(np.double(ss)) # type: ignore\n",
    "                df.columns = spindles_output_columns\n",
    "                tile_electrode = np.tile(f\"{electrode_name_eventDetect}\", ss.shape[0])[None].T\n",
    "                df[electrode_column_name] = tile_electrode\n",
    "                \n",
    "                allsubsdata_perFile[id][f'{output_key}@@{electrode_name_eventDetect}'] = df\n",
    "                if electd_i ==0: ss_allElectrodes =  df\n",
    "                else:  ss_allElectrodes = pd.concat([ss_allElectrodes,df])\n",
    "    \n",
    "        if not ss_allElectrodes.empty:\n",
    "            allsubsdata_perFile[id][output_key]  = ss_allElectrodes\n",
    "\n",
    "    minmax_sd_name =  matlabImport['minMax_SD_threshold']\n",
    "    shutil.rmtree(andriNir_aux_output_dir)\n",
    "    return minmax_sd_name\n",
    "def group_spindles(ss_key_to_use, uniqeElctds_ss_key, uniqeElctd_ss_key):\n",
    "    # The grouping is done such that you get 1 event per time frame.\n",
    "    # In the single condition, only one electrode is sufficiant to include event.\n",
    "    #  In the multi condition event will be includede only if it appears in sevral electrodes\n",
    "    # The picked electrode is the one where the event is with the most power\n",
    "    for id in allsubsdata_perFile:\n",
    "        if ss_key_to_use not in allsubsdata_perFile[id]:  continue\n",
    "        filterd_events_allElectrodes = allsubsdata_perFile[id][ss_key_to_use].copy(deep=True)\n",
    "        \n",
    "        filterd_events_allElectrodes.sort_values(by=['spindleStartTime'],inplace=True) \n",
    "        filterd_events_allElectrodes.reset_index(drop=True, inplace=True)\n",
    "        deleted = filterd_events_allElectrodes.copy(deep=True)\n",
    "        filtered = pd.DataFrame([], columns = deleted.columns)\n",
    "        filtered_moreThan1 = pd.DataFrame([], columns = deleted.columns)\n",
    "        simultan = pd.DataFrame([], columns = deleted.columns)\n",
    "\n",
    "        while len(deleted)>0:\n",
    "            if len(simultan)==0:\n",
    "                simultan = pd.concat([simultan, deleted.iloc[[0]]])\n",
    "                deleted.drop(deleted.index[0], axis=0, inplace=True)\n",
    "                still_overlap = True\n",
    "            else:\n",
    "                while still_overlap == True and len(deleted)>0:\n",
    "                    still_overlap = False\n",
    "                    simultan.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "\n",
    "                    ## check now_overlap_all_in_simultan:\n",
    "                    for index, simultan_row in simultan.iterrows():\n",
    "                        simultan_0 = simultan_row['spindleStartTime']\n",
    "                        simultan_1 = simultan_row['spindleEndTime']\n",
    "                        deleted_0 = deleted.iloc[0]['spindleStartTime']\n",
    "                        deleted_1 = deleted.iloc[0]['spindleEndTime']\n",
    "                        if overlap([simultan_0,simultan_1],[deleted_0,deleted_1]):\n",
    "                            simultan = pd.concat([simultan, deleted.iloc[[0]]])\n",
    "                            deleted.drop(deleted.index[0], axis=0, inplace=True)\n",
    "                            still_overlap = True\n",
    "                            break\n",
    "                    if still_overlap: continue\n",
    "                    else:\n",
    "                        ## check max ps and add to filt\n",
    "                        # if len(simultan)>1:\n",
    "                        #     print('hu')\n",
    "                        simultan.sort_values(by=['PowerSP'],ascending=False,inplace=True)\n",
    "                        row_df = simultan.iloc[[0]]\n",
    "                        row_df.iat[0, row_df.columns.get_loc(electrode_column_name)] = np.unique(np.array(simultan[electrode_column_name]))\n",
    "                        filtered = pd.concat([filtered,row_df])\n",
    "                        if len(simultan) > 1: filtered_moreThan1 = pd.concat([filtered_moreThan1,row_df])\n",
    "                        simultan = pd.DataFrame([], columns = deleted.columns)                    \n",
    "            \n",
    "        filtered.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "        filtered_moreThan1.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "        allsubsdata_perFile[id][uniqeElctd_ss_key] = filtered   \n",
    "        allsubsdata_perFile[id][uniqeElctds_ss_key] = filtered_moreThan1   \n",
    "        print(f\"sub:{id}, before filt:{np.shape(allsubsdata_perFile[id][ss_key_to_use])[0]}, after:{np.shape(allsubsdata_perFile[id][uniqeElctd_ss_key])[0]}, after>1:{np.shape(allsubsdata_perFile[id][uniqeElctds_ss_key])[0]}\")\n",
    "\n",
    "def add_edfViewFormat_ss(file_ids,event_key_for_save, SS_efdViewFormat_key):\n",
    "    for id in file_ids:\n",
    "        if event_key_for_save not in allsubsdata_perFile[id]:\n",
    "                #print(f\"no {event_key_for_save} for sub {id}\")\n",
    "                continue\n",
    "        ss_df = allsubsdata_perFile[id][event_key_for_save]\n",
    "\n",
    "        startTime_arr = np.array(ss_df['spindleStartTime'])\n",
    "        endTime_arr = np.array(ss_df['spindleEndTime'])\n",
    "        duration_arr = (endTime_arr - startTime_arr)  / np.double(configu['sample_freq'])\n",
    "        electd_arr_per_ss = np.array(ss_df[electrode_column_name])\n",
    "        desc = [f\"ss@@{electd_arr}\" for electd_arr in electd_arr_per_ss]\n",
    "        new_format_ss = np.array([startTime_arr / np.double(configu['sample_freq']) ,duration_arr,desc]).T ## onset (sec), duration,desc\n",
    "\n",
    "        allsubsdata_perFile[id][SS_efdViewFormat_key] = new_format_ss\n",
    "\n",
    "def save_eventsSepratelyForComparison_edfViewFormat(file_ids,events_types_to_compare,edfViewFormat_output_dir):\n",
    "    # Save events (scoring and ss) per file, in a format suitable for EDF_viewer\n",
    "    for id in file_ids:\n",
    "        for event_type in events_types_to_compare:\n",
    "            filename = f\"{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_events\"\n",
    "            all_events_with_header = np.asarray([header],dtype=object)\n",
    "            events_type_found = [ v for k, v in allsubsdata_perFile[id].items() if ((event_type in k) and (edfviewFormatSuffix in k))]\n",
    "            if np.size(events_type_found) == 0: continue\n",
    "            for events_found in events_type_found:\n",
    "                    for event_found in events_found:\n",
    "                            all_events_with_header = np.concatenate((all_events_with_header,[event_found]),dtype=object)\n",
    "            np.savetxt(f\"{edfViewFormat_output_dir}\\\\{event_type}_{filename}.txt\", all_events_with_header, delimiter='\\t',fmt='%s')\n",
    " \n",
    "def save_ss_csv(subs,event_key_no_electd_cond,ss_ver,output_dir):\n",
    "     for sub in subs:\n",
    "        df_curr_sub = pd.DataFrame([],columns=np.append(spindles_output_columns,['sub','subfile','eventType','numElectrCond']))\n",
    "        for file_i in range(1,6):\n",
    "            curr_subfile_key = f\"{sub}_{file_i}\"\n",
    "            for elecd_cond in [multiElectd_text,singleElectd_text]:\n",
    "                event_key = f\"{event_key_no_electd_cond}_{elecd_cond}\"\n",
    "                if (curr_subfile_key in allsubsdata_perFile.keys()) and (event_key in allsubsdata_perFile[curr_subfile_key].keys()):\n",
    "                        curr_file_ss_df = allsubsdata_perFile[curr_subfile_key][event_key]\n",
    "                        new_df = pd.DataFrame(curr_file_ss_df)\n",
    "                        new_df['sub'] = sub\n",
    "                        new_df['subfile'] = file_i\n",
    "                        new_df['eventType'] = 'SS'\n",
    "                        new_df['numElectrCond'] = elecd_cond\n",
    "                        df_curr_sub = pd.concat([df_curr_sub,new_df])\n",
    "\n",
    "        if(df_curr_sub.size>0):\n",
    "            df_curr_sub.to_csv(f'{output_dir}//sub-{sub}_SS-{ss_ver}.csv')\n",
    "                            \n",
    "if(run_comparison_of_spindle_minMax):\n",
    "    edfViewFormat_minMaxSSTests_output_dir = f'{edf_output_dir}\\\\minmax_sd_tests'\n",
    "    restart_output_dir(edfViewFormat_minMaxSSTests_output_dir)\n",
    "\n",
    "    for minMax_sd_ver in minMax_sd_vers:    \n",
    "        with open(import_path, \"rb\") as file:  [allsubsdata_perFile, configu] = pickle.load(file) # it is important, becuase the minmaxVers override eachother\n",
    "\n",
    "        all_electodes_ss_key = 'SS_AN'\n",
    "        minmax_sd_name = detect_ss_AndrillonNir(file_ids,minMax_sd_ver,all_electodes_ss_key,electrodes_names_detect)\n",
    "        print(minmax_sd_name)\n",
    "\n",
    "        after_sleepStage_exclution_key = f\"{all_electodes_ss_key}_n2n3\"\n",
    "        filter_sleep_stages(file_ids,all_electodes_ss_key,after_sleepStage_exclution_key, [2.0,3.0]) \n",
    "\n",
    "        ss_key_to_use = after_sleepStage_exclution_key\n",
    "        multiElectdPerSS_key = f\"{ss_key_to_use}_{multiElectd_text}\"\n",
    "        singleElectdPerSS_key = f\"{ss_key_to_use}_{singleElectd_text}\"\n",
    "        group_spindles(ss_key_to_use,multiElectdPerSS_key, singleElectdPerSS_key)\n",
    "\n",
    "        SS_multiElectdPerSS_efdViewFormat_key = f'{multiElectdPerSS_key}_{edfviewFormatSuffix}'\n",
    "        SS_singleElectdPerSS_efdViewFormat_key = f'{singleElectdPerSS_key}_{edfviewFormatSuffix}'\n",
    "        add_edfViewFormat_ss(file_ids,multiElectdPerSS_key,SS_multiElectdPerSS_efdViewFormat_key)\n",
    "        add_edfViewFormat_ss(file_ids,singleElectdPerSS_key,SS_singleElectdPerSS_efdViewFormat_key)\n",
    "\n",
    "        events_types_to_compare = [multiElectd_text,singleElectd_text]    \n",
    "        edfViewFormat_eventsTest_output_dir = f\"{edfViewFormat_minMaxSSTests_output_dir}\\\\{minMax_sd_ver}\"\n",
    "        restart_output_dir(edfViewFormat_eventsTest_output_dir)\n",
    "        save_eventsSepratelyForComparison_edfViewFormat(file_ids,events_types_to_compare,edfViewFormat_eventsTest_output_dir)\n",
    "\n",
    "        add_edfViewFormat_scoring(scoring_edfViewFormat_key)\n",
    "\n",
    "        events_types_to_save = ['scoring', multiElectd_text,singleElectd_text]    \n",
    "        save_eventsAllTogether_edfViewFormat(events_types_to_save,edfViewFormat_eventsTest_output_dir)\n",
    "\n",
    "if(run_csvsave_ss):\n",
    "    all_electodes_ss_key = 'SS_AN'\n",
    "    minmax_sd_name = detect_ss_AndrillonNir(file_ids,csv_output_final_ver,all_electodes_ss_key,electrodes_names_detect)\n",
    "\n",
    "    after_sleepStage_exclution_key = f\"{all_electodes_ss_key}_n2n3\"\n",
    "    filter_sleep_stages(file_ids,all_electodes_ss_key,after_sleepStage_exclution_key, [2.0,3.0]) \n",
    "\n",
    "    ss_key_to_use = after_sleepStage_exclution_key\n",
    "    multiElectdPerSS_key = f\"{ss_key_to_use}_{multiElectd_text}\"\n",
    "    singleElectdPerSS_key = f\"{ss_key_to_use}_{singleElectd_text}\"\n",
    "    group_spindles(ss_key_to_use,multiElectdPerSS_key, singleElectdPerSS_key)\n",
    "\n",
    "    save_ss_csv(subs,ss_key_to_use,csv_output_final_ver,csv_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preform_minmaxSD_comparison(events_types_to_compare,file_ids,minMax_sd_vers):\n",
    "    all_comparisons = pd.DataFrame(columns=['id','eventType','minmax_ver','Positive','TruePos','FalsePos', 'hitRate','falseDiscoveryRate'])\n",
    "    for id in file_ids:\n",
    "        for ver in minMax_sd_vers:\n",
    "            for event_type in events_types_to_compare:\n",
    "                dir_auto_ = f\"{edfViewFormat_minMaxSSTests_output_dir}\\\\{ver}\"\n",
    "                manual_detection_filename = f\"{manual_detection_dir}\\\\{id}_imported_manual-ss.txt\"\n",
    "                auto_detection_filename = f\"{dir_auto_}\\\\{event_type}_{id}_events.txt\"\n",
    "                ## get the array of before manual scanning\n",
    "                if os.path.exists(auto_detection_filename) and os.path.exists(manual_detection_filename):\n",
    "                    all_ss_auto = np.loadtxt(auto_detection_filename, delimiter=\"\\t\",dtype='object')\n",
    "                    all_ss_auto = np.delete(all_ss_auto, np.where(all_ss_auto[:, 2] == \"Annotation\")[0], axis=0)\n",
    "                    all_ss_auto[:,[0,1]] = [np.double(x) for x in all_ss_auto[:,[0,1]]]\n",
    "                    ss_ind = np.array([],dtype=int)\n",
    "                    for ind_i, desc in enumerate(all_ss_auto[:,2]):\n",
    "                        if (\"SS\" in desc) or (\"ss\" in desc):\n",
    "                            ss_ind = np.append(ss_ind, ind_i)\n",
    "                    all_ss_auto = all_ss_auto[ss_ind,:]\n",
    "\n",
    "                    ## get the array of after manual scanning\n",
    "                    all_ss_manu = np.loadtxt(manual_detection_filename, delimiter=\"\\t\",dtype='object')\n",
    "                    all_ss_manu = np.delete(all_ss_manu, np.where(all_ss_manu[:, 2] == \"Annotation\")[0], axis=0)\n",
    "                    all_ss_manu[:,[0,1]] = [np.double(x) for x in all_ss_manu[:,[0,1]]]\n",
    "                    ss_ind = np.array([],dtype=int)\n",
    "                    for ind_i, desc in enumerate(all_ss_manu[:,2]):\n",
    "                        if \"SS\" in desc:\n",
    "                            ss_ind = np.append(ss_ind, ind_i)\n",
    "                    all_ss_manu = all_ss_manu[ss_ind,:]\n",
    "\n",
    "                    ## compare to find rate od TP and FP\n",
    "                    TP = 0\n",
    "                    FP = 0\n",
    "                    for ss_auto in all_ss_auto:\n",
    "                        found = False\n",
    "                        for ss_manu in all_ss_manu:\n",
    "                            if overlap([ss_auto[0],ss_auto[0]+ss_auto[1]],[ss_manu[0],ss_manu[0]+ss_manu[1]]):\n",
    "                                TP +=1\n",
    "                                found = True\n",
    "                                break\n",
    "                        if found == False:\n",
    "                            FP +=1\n",
    "\n",
    "                    # FN = 0\n",
    "                    # for ss_manu in all_ss_manu:\n",
    "                    #     found = False\n",
    "                    #     for ss_auto in all_ss_auto:\n",
    "                    #         if overlap([ss_auto[0],ss_auto[0]+ss_auto[1]],[ss_manu[0],ss_manu[0]+ss_manu[1]]):\n",
    "                    #             found = True\n",
    "                    #             break\n",
    "                    #     if found == False:\n",
    "                    #         FN +=1\n",
    "\n",
    "                    Positive = np.shape(all_ss_manu)[0]   \n",
    "                    hitRate = TP/Positive\n",
    "                    #missRate = FN/Positive\n",
    "                    falseDiscoveryRate = FP/(FP+TP)\n",
    "                    \n",
    "                    comparison = [id,event_type, ver, Positive, TP, FP,hitRate, falseDiscoveryRate]\n",
    "                    all_comparisons.loc[len(all_comparisons)] = comparison\n",
    "\n",
    "    all_comparisons = all_comparisons.sort_values('minmax_ver')\n",
    "    all_comparisons = all_comparisons.sort_values('id')\n",
    "    return all_comparisons\n",
    "if(run_comparison_of_spindle_minMax):\n",
    "    events_types_to_compare = [multiElectd_text,singleElectd_text]\n",
    "    all_comp = preform_minmaxSD_comparison(events_types_to_compare,file_ids,minMax_sd_vers)\n",
    "    # all_comp.to_csv('ss_comparisons.csv',index=False)\n",
    "    display(all_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sw_AndrillonNir(file_ids,output_key,electrodes_names):\n",
    "    andriNir_code_dir = os.getcwd()\n",
    "    andriNir_output_dir = f\"{configu['outputs_dir_path']}\\\\{main_dir_name}\\\\Andrillon_Nir\"\n",
    "    andriNir_aux_output_dir = f\"{andriNir_output_dir}\\\\aux_mats\"\n",
    "\n",
    "    if os.path.exists(andriNir_output_dir):\n",
    "        shutil.rmtree(andriNir_output_dir)\n",
    "    os.mkdir(andriNir_output_dir)\n",
    "    os.mkdir(andriNir_aux_output_dir)\n",
    "\n",
    "    sw_output_columns = ['wavest' ,'wavend', 'mdpt', 'poszx', 'period/SR' ,'abs(b)', 'bx', 'c' ,'cx' ,'maxb2c', 'n1', 'n1x', 'nEnd', 'nEndx' ,'p1', 'p1x' ,'meanAmp', 'nump', 'nperiod/SR', 'p2p', 'mxdn', 'mxup' ,'sleepStage']\n",
    "\n",
    "    for id in file_ids:\n",
    "        datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "        datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "        scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=True)\n",
    "        sw_allElectrodes = pd.DataFrame()\n",
    "\n",
    "        for electd_i, electrode_name_eventDetect in enumerate(electrodes_names):\n",
    "            curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name_eventDetect)[0][0]\n",
    "\n",
    "            ## create aux files to use in MATLAB\n",
    "            datafile_1elect_eeg = datafile_data[curr_electrode_num,:]\n",
    "            if 2 not in datafile_scoring or 3 not in datafile_scoring:\n",
    "                continue\n",
    "            \n",
    "            mat_to_save = {'datafile_data': datafile_1elect_eeg, 'scoring_upsampled': scoring_upsampled, 'sample_freq': configu['sample_freq'], 'electrode_name':electrode_name_eventDetect}\n",
    "            scipy.io.savemat(f\"{andriNir_aux_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\",mat_to_save)\n",
    "\n",
    "            ## run Andrillon & Nir SW detection over all subjects files\n",
    "            eng = matlab.engine.start_matlab()\n",
    "            eng.cd(andriNir_code_dir, nargout=0)\n",
    "            out = eng.batch_useAndrillonNirSWDetection(andriNir_aux_output_dir, andriNir_output_dir,nargout=0)\n",
    "            eng.quit()\n",
    "\n",
    "            ## add the SW data to the main subject dictionary\n",
    "            try: \n",
    "                sw_file_name = f\"{andriNir_output_dir}\\\\{output_key}_{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\"\n",
    "                matlabImport = scipy.io.loadmat(sw_file_name, simplify_cells=True)\n",
    "            except Exception: \n",
    "                print(f\"Error importing spindles sub file at: {allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}\")\n",
    "                continue\n",
    "\n",
    "            sw = matlabImport['allWaves']\n",
    "            if np.size(np.shape(sw)) == 1: \n",
    "                temp = np.zeros((1,np.size(sw)),dtype=object)\n",
    "                temp[0] = sw\n",
    "                sw = temp\n",
    "            if np.size(sw) >0 :\n",
    "                df = pd.DataFrame(np.double(sw))\n",
    "                df.columns = sw_output_columns\n",
    "                tile_electrode = np.tile(f\"{electrode_name_eventDetect}\", sw.shape[0])[None].T\n",
    "                df[electrode_column_name] = tile_electrode\n",
    "                \n",
    "                allsubsdata_perFile[id][f\"{output_key}@@{electrode_name_eventDetect}\"] = df\n",
    "                if electd_i ==0: sw_allElectrodes =  df\n",
    "                else:  sw_allElectrodes = pd.concat([sw_allElectrodes,df])\n",
    "    \n",
    "        if not sw_allElectrodes.empty:\n",
    "            allsubsdata_perFile[id][output_key]  = sw_allElectrodes\n",
    "    shutil.rmtree(andriNir_aux_output_dir)\n",
    "def add_edfViewFormat_sw(file_ids,event_key_for_save, SW_efdViewFormat_key):\n",
    "        for id in file_ids:\n",
    "            if event_key_for_save not in allsubsdata_perFile[id]:\n",
    "                    #print(f\"no {event_key_for_save} for sub {id}\")\n",
    "                    continue\n",
    "            sw_df = allsubsdata_perFile[id][event_key_for_save]\n",
    "    \n",
    "            startTime_arr = np.array(sw_df['wavest'])\n",
    "            endTime_arr = np.array(sw_df['wavend'])\n",
    "            duration_arr = (endTime_arr - startTime_arr)  / np.double(configu['sample_freq'])\n",
    "            electd_arr_per_sw = np.array(sw_df[electrode_column_name])\n",
    "            desc = [f\"SW@@{electd_arr}\" for electd_arr in electd_arr_per_sw]\n",
    "            new_format_sw = np.array([startTime_arr / np.double(configu['sample_freq']) ,duration_arr,desc]).T ## onset (sec), duration,desc\n",
    "\n",
    "            allsubsdata_perFile[id][SW_efdViewFormat_key] = new_format_sw\n",
    "def filter_waveCriteria(file_ids,event_key_to_use,after_sleepStage_exclution_key):\n",
    "        # % Result Matrix\n",
    "        # %1:  wave beginning (sample)\n",
    "        # %2:  wave end (sample)\n",
    "        # %3:  wave middle point (sample)\n",
    "        # %4:  wave negative half-way (sample)\n",
    "        # %5:  period in seconds\n",
    "        # %6:  positive amplitude peak\n",
    "        # %7:  positive amplitude peak position (sample)\n",
    "        # %8:  negative amplitude peak\n",
    "        # %9:  negative amplitude peak position (sample)\n",
    "        # %10: peak-to-peak amplitude\n",
    "        # %11: 1st pos peak amplitude\n",
    "        # %12: 1st pos peak amplitude position (sample)\n",
    "        # %13: Last pos peak amplitude\n",
    "        # %14: Last pos peak amplitude position (sample)\n",
    "        # %15: 1st neg peak amplitude\n",
    "        # %16: 1st neg peak amplitude position (sample)\n",
    "        # %17: mean wave amplitude\n",
    "        # %18: number of positive peaks\n",
    "        # %19: wave negative half-way period\n",
    "        # %20: 1st peak to last peak period\n",
    "        # %21: determines instantaneous positive 1st segement slope on smoothed signal\n",
    "        # %22: determines maximal negative slope for 2nd segement\n",
    "        # %23: stage (if scored data)\n",
    "    for id in file_ids:\n",
    "        if event_key_to_use not in allsubsdata_perFile[id]: continue\n",
    "        event_allElectrodes = allsubsdata_perFile[id][event_key_to_use]\n",
    "        event_filtered_waveCrit = pd.DataFrame([], columns=event_allElectrodes.columns)\n",
    "\n",
    "        event_filtered_waveCrit = event_allElectrodes[(event_allElectrodes['maxb2c'] > 75)]\n",
    "\n",
    "        if len(event_filtered_waveCrit) == 0:  print(f\"sub:{id} - no sw with criteria\")\n",
    "        else: print(f\"sub:{id}, before wave criteria sleep stages:{len(event_allElectrodes)}, after:{len(event_filtered_waveCrit)}\")\n",
    "        \n",
    "        allsubsdata_perFile[id][after_sleepStage_exclution_key] = event_filtered_waveCrit\n",
    "\n",
    "if(run_comparison_of_sw_minMax):\n",
    "    edfViewFormat_swEvents_output_dir = f'{edf_output_dir}\\\\sw_AN_tests'\n",
    "    restart_output_dir(edfViewFormat_swEvents_output_dir)\n",
    "    \n",
    "    all_electodes_sw_key = 'SW_AN'\n",
    "    res = detect_sw_AndrillonNir(file_ids,all_electodes_sw_key,electrodes_names_detect)\n",
    "\n",
    "    after_sleepStage_exclution_key = f\"{all_electodes_sw_key}_n2n3\"\n",
    "    filter_sleep_stages(file_ids,all_electodes_sw_key,after_sleepStage_exclution_key, [2.0,3.0]) \n",
    "\n",
    "    sw_key_to_use = after_sleepStage_exclution_key\n",
    "    sw_key_after_SWExclusion = f\"{after_sleepStage_exclution_key}_swExclusion\"\n",
    "    filter_waveCriteria(file_ids,sw_key_to_use,sw_key_after_SWExclusion)\n",
    "\n",
    "    SW_multiElectdPerSW_efdViewFormat_key = f'{sw_key_after_SWExclusion}_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_sw(file_ids,sw_key_after_SWExclusion,SW_multiElectdPerSW_efdViewFormat_key)\n",
    "\n",
    "    # events_types_to_compare = [multiElectdPerSW_text,singleElectdPerSW_text]    \n",
    "    # edfViewFormat_eventsTest_output_dir = f\"{edfViewFormat_events_output_dir}\\\\{minMax_sd_ver}\"\n",
    "    # if os.path.exists(edfViewFormat_eventsTest_output_dir):  shutil.rmtree(edfViewFormat_eventsTest_output_dir)\n",
    "    # os.mkdir(edfViewFormat_eventsTest_output_dir)\n",
    "    # save_eventsSepratelyForComparison_edfViewFormat(file_ids,events_types_to_compare,edfViewFormat_eventsTest_output_dir)\n",
    "\n",
    "    add_edfViewFormat_scoring(scoring_edfViewFormat_key)\n",
    "\n",
    "    events_types_to_save = ['scoring', SW_multiElectdPerSW_efdViewFormat_key]    \n",
    "    save_eventsAllTogether_edfViewFormat(events_types_to_save,edfViewFormat_swEvents_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN ShaYKM_kcomplex events - find max first\n",
    "\n",
    "# Sample rate and desired cutoff frequencies (in Hz).\n",
    "new_freq = 100\n",
    "kcomp_df_columns = ['pos_i','pos_amp','neg_i','neg_amp','power',electrode_column_name]\n",
    "kcomp_df =  pd.DataFrame([], columns = kcomp_df_columns)\n",
    "\n",
    "sw_lowcut = 0.3\n",
    "sw_highcut = 3\n",
    "sw_filer_order = 3\n",
    "\n",
    "negPeak_lowcut = 0.5 # 0.5 is much better (than 1) for hits\n",
    "negPeak_highcut = 6\n",
    "negPeak_filer_order = 2 # 3 is worst for hits, better for FP\n",
    "\n",
    "sw_min_width = 0.3 # sec\n",
    "\n",
    "distance_sw = 0.5 #sec # 0.38 was the minimal found before.\n",
    "range_negPeak = 1 #in sec #ok\n",
    "minima_top_threshold = -45\n",
    "min_amp_diff_between_peaks = 85 #75 was ok before\n",
    "# maxima/abs(minima) > 0.5* applied\n",
    "\n",
    "def detect_kComp_shaYKM(file_ids,output_key,electrodes_names):\n",
    "    for id in file_ids:\n",
    "        kcomp_df =  pd.DataFrame([], columns = kcomp_df_columns)\n",
    "\n",
    "        datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "        datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "        scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'])\n",
    "        \n",
    "        for electrode_name in electrodes_names:\n",
    "            curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name)[0][0]\n",
    "            data = datafile_data[curr_electrode_num] ## shape (time)\n",
    "            z = new_freq / configu['sample_freq']\n",
    "            resampled = signal.resample(data, int(np.size(data)*z)) ## downsample to 100hz\n",
    "\n",
    "            filtered_sw = butter_bandpass_filter(resampled, sw_lowcut, sw_highcut, new_freq, order=sw_filer_order) ## oreder can be changed. see: https://stackoverflow.com/questions/12093594/how-to-implement-band-pass-butterworth-filter-with-scipy-signal-butter\n",
    "            filtered_transiantMinima = butter_bandpass_filter(resampled, negPeak_lowcut, negPeak_highcut, new_freq, order=negPeak_filer_order)\n",
    "            \n",
    "            ## find all standalone maximas\n",
    "            curr_maximas_inds, _ = find_peaks(filtered_sw, height=(-1*minima_top_threshold)/2,width=sw_min_width*new_freq, distance=distance_sw*new_freq)\n",
    "            for maxima_ind in curr_maximas_inds:\n",
    "                if (scoring_upsampled[int(maxima_ind/z)] ==2):\n",
    "\n",
    "                    maxima = filtered_sw[maxima_ind]                \n",
    "                    minima_scope = filtered_transiantMinima[maxima_ind - int(range_negPeak*new_freq) : maxima_ind]                 \n",
    "                    curr_minimas_inds, _ = find_peaks(-1*minima_scope, height=(-1*minima_top_threshold))\n",
    "                    if np.size(curr_minimas_inds) ==0 : continue\n",
    "\n",
    "                    min_minima_ind_inScope = curr_minimas_inds[np.argmin(minima_scope[curr_minimas_inds])]\n",
    "                    minima = minima_scope[min_minima_ind_inScope]\n",
    "                    minima_ind = min_minima_ind_inScope + (maxima_ind - int(range_negPeak*new_freq))\n",
    "\n",
    "                    if (minima < maxima-min_amp_diff_between_peaks) and (minima<minima_top_threshold) and (maxima > 0.5*abs(minima)): ## found a k-comp!\n",
    "                        new_row =           {'pos_i':maxima_ind/z, \n",
    "                                            'pos_amp':maxima, \n",
    "                                            'neg_i':minima_ind/z, \n",
    "                                            'neg_amp':minima,\n",
    "                                            'power': maxima-minima,\n",
    "                                            electrode_column_name:electrode_name}\n",
    "                        new_row_df = pd.DataFrame([new_row], columns = kcomp_df_columns)\n",
    "                        kcomp_df = pd.concat([kcomp_df,new_row_df])\n",
    "                        # print(f\"{maxima_ind/z/configu['sample_freq']/60}\")\n",
    "                        # plt.plot(resampled[max(maxima_ind-100,0):min(maxima_ind+100,np.size(resampled)-1)])\n",
    "                        # plt.plot(filtered_sw[max(maxima_ind-100,0):min(maxima_ind+100,np.size(resampled)-1)])\n",
    "                        # plt.plot(filtered_transiantMinima[max(maxima_ind-100,0):min(maxima_ind+100,np.size(resampled)-1)])\n",
    "                        # plt.show()\n",
    "            \n",
    "        if not kcomp_df.empty:\n",
    "            allsubsdata_perFile[id][output_key]  = kcomp_df\n",
    "        else: print(f'no events: {output_key}')\n",
    "\n",
    "def group_kcomp(kcomp_key_to_use, uniqeElctds_kcomp_key, uniqeElctd_kcomp_key):\n",
    "    # The grouping is done such that you get 1 event per time frame.\n",
    "    # In the single condition, only one electrode is sufficiant to include event.\n",
    "    #  In the multi condition event will be includede only if it appears in sevral electrodes\n",
    "    # The picked electrode is the one where the event is with the most power\n",
    "    for id in allsubsdata_perFile:\n",
    "        if kcomp_key_to_use not in allsubsdata_perFile[id]:  continue\n",
    "        filterd_events_allElectrodes = allsubsdata_perFile[id][kcomp_key_to_use].copy(deep=True)\n",
    "        \n",
    "        filterd_events_allElectrodes.sort_values(by=['neg_i'],inplace=True) \n",
    "        filterd_events_allElectrodes.reset_index(drop=True, inplace=True)\n",
    "        deleted = filterd_events_allElectrodes.copy(deep=True)\n",
    "        filtered = pd.DataFrame([], columns = deleted.columns)\n",
    "        filtered_moreThan1 = pd.DataFrame([], columns = deleted.columns)\n",
    "        simultan = pd.DataFrame([], columns = deleted.columns)\n",
    "\n",
    "        while len(deleted)>0:\n",
    "            if len(simultan)==0:\n",
    "                simultan = pd.concat([simultan, deleted.iloc[[0]]])\n",
    "                deleted.drop(deleted.index[0], axis=0, inplace=True)\n",
    "                still_overlap = True\n",
    "            else:\n",
    "                while still_overlap == True and len(deleted)>0:\n",
    "                    still_overlap = False\n",
    "                    simultan.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "\n",
    "                    ## check now_overlap_all_in_simultan:\n",
    "                    for index, simultan_row in simultan.iterrows():\n",
    "                        simultan_0 = simultan_row['neg_i']\n",
    "                        simultan_1 = simultan_row['pos_i']\n",
    "                        deleted_0 = deleted.iloc[0]['neg_i']\n",
    "                        deleted_1 = deleted.iloc[0]['pos_i']\n",
    "                        if overlap([simultan_0,simultan_1],[deleted_0,deleted_1]):\n",
    "                            simultan = pd.concat([simultan, deleted.iloc[[0]]])\n",
    "                            deleted.drop(deleted.index[0], axis=0, inplace=True)\n",
    "                            still_overlap = True\n",
    "                            break\n",
    "                    if still_overlap: continue\n",
    "                    else:\n",
    "                        ## check max ps and add to filt\n",
    "                        # if len(simultan)>1:\n",
    "                        #     print('hu')\n",
    "                        simultan.sort_values(by=['power'],ascending=False,inplace=True)\n",
    "                        row_df = simultan.iloc[[0]]\n",
    "                        row_df.iat[0, row_df.columns.get_loc(electrode_column_name)] = np.unique(np.array(simultan[electrode_column_name]))\n",
    "                        filtered = pd.concat([filtered,row_df])\n",
    "                        if len(simultan) > 1: filtered_moreThan1 = pd.concat([filtered_moreThan1,row_df])\n",
    "                        simultan = pd.DataFrame([], columns = deleted.columns)                    \n",
    "            \n",
    "        filtered.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "        filtered_moreThan1.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "        allsubsdata_perFile[id][uniqeElctd_kcomp_key] = filtered   \n",
    "        allsubsdata_perFile[id][uniqeElctds_kcomp_key] = filtered_moreThan1   \n",
    "        print(f\"sub:{id}, before filt:{np.shape(allsubsdata_perFile[id][kcomp_key_to_use])[0]}, after:{np.shape(allsubsdata_perFile[id][uniqeElctd_kcomp_key])[0]}, after>1:{np.shape(allsubsdata_perFile[id][uniqeElctds_kcomp_key])[0]}\")\n",
    "def add_edfViewFormat_kcomp_shaYKM(file_ids,event_key_for_save, kcomp_efdViewFormat_key):\n",
    "    for id in file_ids:\n",
    "        if event_key_for_save not in allsubsdata_perFile[id]:\n",
    "                #print(f\"no {event_key_for_save} for sub {id}\")\n",
    "                continue\n",
    "        kcomp_df = allsubsdata_perFile[id][event_key_for_save]\n",
    "\n",
    "        startTime_arr = np.array(kcomp_df['neg_i'])\n",
    "        endTime_arr = np.array(kcomp_df['pos_i']) \n",
    "        duration_arr = np.round((endTime_arr - startTime_arr)/configu['sample_freq'],2)\n",
    "        electd_arr_per_sw = np.array(kcomp_df[electrode_column_name])\n",
    "        desc = [f\"Ykcomp@@{electd_arr[0]}\" for electd_arr in electd_arr_per_sw]\n",
    "        new_format_kcomp = np.array([startTime_arr /configu['sample_freq'],duration_arr,desc]).T ## onset (sec), duration,desc\n",
    "\n",
    "        allsubsdata_perFile[id][kcomp_efdViewFormat_key] = new_format_kcomp\n",
    "\n",
    "def save_eventsSepratelyForComparison_edfViewFormat(file_ids,events_types_to_compare,edfViewFormat_output_dir):\n",
    "    # Save events (scoring and ss) per file, in a format suitable for EDF_viewer\n",
    "    for id in file_ids:\n",
    "        for event_type in events_types_to_compare:\n",
    "            filename = f\"{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_events\"\n",
    "            all_events_with_header = np.asarray([header],dtype=object)\n",
    "            events_type_found = [ v for k, v in allsubsdata_perFile[id].items() if ((event_type in k) and (edfviewFormatSuffix in k))]\n",
    "            if np.size(events_type_found) == 0: continue\n",
    "            for events_found in events_type_found:\n",
    "                    for event_found in events_found:\n",
    "                            all_events_with_header = np.concatenate((all_events_with_header,[event_found]),dtype=object)\n",
    "            np.savetxt(f\"{edfViewFormat_output_dir}\\\\{event_type}_{filename}.txt\", all_events_with_header, delimiter='\\t',fmt='%s')\n",
    "\n",
    "if(run_kcomp_shaYKM):\n",
    "    edfViewFormat_kcompEvents_output_dir = f'{edf_output_dir}\\\\kcomp_tests_ShaYKM'\n",
    "    restart_output_dir(edfViewFormat_kcompEvents_output_dir)\n",
    "\n",
    "    all_electodes_kcompSrid_key = 'KComp_shaYKM'\n",
    "    res = detect_kComp_shaYKM(file_ids,all_electodes_kcompSrid_key,electrodes_names_detect)\n",
    "\n",
    "    kcomp_key_to_use = all_electodes_kcompSrid_key\n",
    "    multiElectdPerKC_key = f\"{kcomp_key_to_use}_{multiElectd_text}\"\n",
    "    singleElectdPerKC_key = f\"{kcomp_key_to_use}_{singleElectd_text}\"\n",
    "    group_kcomp(kcomp_key_to_use,multiElectdPerKC_key, singleElectdPerKC_key)\n",
    "\n",
    "    kcomp_single_efdViewFormat_key = f'{singleElectdPerKC_key}_{edfviewFormatSuffix}'\n",
    "    kcomp_multi_efdViewFormat_key = f'{multiElectdPerKC_key}_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_kcomp_shaYKM(file_ids,multiElectdPerKC_key,kcomp_multi_efdViewFormat_key)\n",
    "    add_edfViewFormat_kcomp_shaYKM(file_ids,singleElectdPerKC_key,kcomp_single_efdViewFormat_key)\n",
    "\n",
    "    add_edfViewFormat_scoring(scoring_edfViewFormat_key)\n",
    "\n",
    "    events_to_save_seperately = [scoring_edfViewFormat_key, multiElectd_text,singleElectd_text] \n",
    "    save_eventsSepratelyForComparison_edfViewFormat(file_ids,events_to_save_seperately,edfViewFormat_kcompEvents_output_dir)\n",
    "\n",
    "    events_types_to_save = ['scoring', kcomp_single_efdViewFormat_key, kcomp_multi_efdViewFormat_key]    \n",
    "    save_eventsAllTogether_edfViewFormat(events_types_to_save,edfViewFormat_kcompEvents_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28-מרץ-23 15:49:29 | WARNING | Hypnogram is LONGER than data by 6.27 seconds. Cropping hypnogram to match data.size.\n",
      "28-מרץ-23 15:49:42 | WARNING | Hypnogram is LONGER than data by 6.99 seconds. Cropping hypnogram to match data.size.\n",
      "28-מרץ-23 15:49:50 | WARNING | Hypnogram is LONGER than data by 5.58 seconds. Cropping hypnogram to match data.size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub:38_1, before filt:5836, after:1387, after>1:1100\n",
      "sub:38_2, before filt:3580, after:844, after>1:664\n",
      "sub:38_3, before filt:3107, after:749, after>1:558\n"
     ]
    }
   ],
   "source": [
    "def save_kcomp_csv(subs,event_key_no_electd_cond,output_dir):\n",
    "     for sub in subs:\n",
    "        df_curr_sub = pd.DataFrame([],columns=np.append(kcomp_df_columns,['sub','subfile','eventType','numElectrCond']))\n",
    "        for file_i in range(1,6):\n",
    "            curr_subfile_key = f\"{sub}_{file_i}\"\n",
    "            for elecd_cond in [multiElectd_text,singleElectd_text]:\n",
    "                event_key = f\"{event_key_no_electd_cond}_{elecd_cond}\"\n",
    "                if (curr_subfile_key in allsubsdata_perFile.keys()) and (event_key in allsubsdata_perFile[curr_subfile_key].keys()):\n",
    "                        curr_file_kcomp_df = allsubsdata_perFile[curr_subfile_key][event_key]\n",
    "                        new_df = pd.DataFrame(curr_file_kcomp_df)\n",
    "                        new_df['sub'] = sub\n",
    "                        new_df['subfile'] = file_i\n",
    "                        new_df['eventType'] = 'KComp'\n",
    "                        new_df['numElectrCond'] = elecd_cond\n",
    "                        df_curr_sub = pd.concat([df_curr_sub,new_df])\n",
    "\n",
    "        if(df_curr_sub.size>0):\n",
    "            df_curr_sub.to_csv(f'{output_dir}//sub-{sub}_KComp.csv')\n",
    "\n",
    "if(run_csvsave_kcomp):\n",
    "    all_electodes_kcompSrid_key = 'KComp_shaYKM'\n",
    "    res = detect_kComp_shaYKM(file_ids,all_electodes_kcompSrid_key,electrodes_names_detect)\n",
    "\n",
    "    kcomp_key_to_use = all_electodes_kcompSrid_key\n",
    "    multiElectdPerKC_key = f\"{kcomp_key_to_use}_{multiElectd_text}\"\n",
    "    singleElectdPerKC_key = f\"{kcomp_key_to_use}_{singleElectd_text}\"\n",
    "    group_kcomp(kcomp_key_to_use,multiElectdPerKC_key, singleElectdPerKC_key)\n",
    "\n",
    "    save_kcomp_csv(subs,kcomp_key_to_use,csv_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preform_KC_comparison(events_types_to_compare,file_ids):\n",
    "    all_comparisons = pd.DataFrame(columns=['id','eventType','Positive','TruePos','FalsePos', 'hitRate','falseDiscoveryRate'])\n",
    "    for id in file_ids:\n",
    "        for event_type in events_types_to_compare:\n",
    "            manual_detection_filename = f\"{manual_detection_dir}\\\\{id}_imported_manual-kcomp.txt\"\n",
    "            auto_detection_filename = f\"{edfViewFormat_kcompEvents_output_dir}\\\\{event_type}_{id}_events.txt\"\n",
    "            ## get the array of before manual scanning\n",
    "            if os.path.exists(auto_detection_filename) and os.path.exists(manual_detection_filename):\n",
    "                all_kc_auto = np.loadtxt(auto_detection_filename, delimiter=\"\\t\",dtype='object')\n",
    "                all_kc_auto = np.delete(all_kc_auto, np.where(all_kc_auto[:, 2] == \"Annotation\")[0], axis=0)\n",
    "                all_kc_auto[:,[0,1]] = [np.double(x) for x in all_kc_auto[:,[0,1]]]\n",
    "                kc_ind = np.array([],dtype=int)\n",
    "                for ind_i, desc in enumerate(all_kc_auto[:,2]):\n",
    "                    if (\"kcomp\" in desc) or (\"KC\" in desc):\n",
    "                        kc_ind = np.append(kc_ind, ind_i)\n",
    "                all_kc_auto = all_kc_auto[kc_ind,:]\n",
    "\n",
    "                ## get the array of after manual scanning\n",
    "                all_kc_manu = np.loadtxt(manual_detection_filename, delimiter=\"\\t\",dtype='object')\n",
    "                all_kc_manu = np.delete(all_kc_manu, np.where(all_kc_manu[:, 2] == \"Annotation\")[0], axis=0)\n",
    "                all_kc_manu[:,[0,1]] = [np.double(x) for x in all_kc_manu[:,[0,1]]]\n",
    "                kc_ind = np.array([],dtype=int)\n",
    "                for ind_i, desc in enumerate(all_kc_manu[:,2]):\n",
    "                    if \"kcomp\" in desc:\n",
    "                        kc_ind = np.append(kc_ind, ind_i)\n",
    "                all_kc_manu = all_kc_manu[kc_ind,:]\n",
    "\n",
    "                ## compare to find rate od TP and FP\n",
    "                TP = 0\n",
    "                FP = 0\n",
    "                for kc_auto in all_kc_auto:\n",
    "                    found = False\n",
    "                    for kc_manu in all_kc_manu:\n",
    "                        if overlap([kc_auto[0],kc_auto[0]+kc_auto[1]],[kc_manu[0],kc_manu[0]+kc_manu[1]]):\n",
    "                            TP +=1\n",
    "                            found = True\n",
    "                            break\n",
    "                    if found == False:\n",
    "                        FP +=1\n",
    "\n",
    "                # FN = 0\n",
    "                # for ss_manu in all_ss_manu:\n",
    "                #     found = False\n",
    "                #     for ss_auto in all_ss_auto:\n",
    "                #         if overlap([ss_auto[0],ss_auto[0]+ss_auto[1]],[ss_manu[0],ss_manu[0]+ss_manu[1]]):\n",
    "                #             found = True\n",
    "                #             break\n",
    "                #     if found == False:\n",
    "                #         FN +=1\n",
    "\n",
    "                Positive = np.shape(all_kc_manu)[0]   \n",
    "                hitRate = TP/Positive\n",
    "                #missRate = FN/Positive\n",
    "                falseDiscoveryRate = FP/(FP+TP)\n",
    "                \n",
    "                comparison = [id,event_type, Positive, TP, FP,hitRate, falseDiscoveryRate]\n",
    "                all_comparisons.loc[len(all_comparisons)] = comparison\n",
    "\n",
    "    all_comparisons = all_comparisons.sort_values('id')\n",
    "    return all_comparisons\n",
    "if(run_kcomp_shaYKM_comparison):\n",
    "    events_types_to_compare = [multiElectd_text,singleElectd_text]\n",
    "    all_comp = preform_KC_comparison(events_types_to_compare,file_ids)\n",
    "    # all_comp.to_csv('ss_comparisons.csv',index=False)\n",
    "    display(all_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f544ce1a915a9875fad91c894e2c0bcad4b7a79945aa6027ef3ad27810072aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
