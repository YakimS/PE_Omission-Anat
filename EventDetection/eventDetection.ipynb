{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30233"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.stats\n",
    "import mne\n",
    "from mne.time_frequency import tfr_morlet\n",
    "from mne.stats import permutation_cluster_1samp_test\n",
    "import gc\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "from os.path import exists\n",
    "import mne\n",
    "import numpy as np\n",
    "from mne import create_info\n",
    "from IPython.utils import io\n",
    "import yasa\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matlab\n",
    "import matlab.engine\n",
    "import os\n",
    "import shutil\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import freqz\n",
    "from scipy import signal\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    return butter(order, [lowcut, highcut], fs=fs, btype='band')\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def overlap(a, b):\n",
    "    return a[1] >= b[0] and a[0] <= b[1]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = \"C:\\\\Users\\\\User\\\\Cloud-Drive\\\\BigFiles\\\\OmissionExpOutput\\\\\\imported_eventDetectionChan\\\\filter0.35\"\n",
    "import_type = \"eventDetectionChan\"\n",
    "output_dir_name = 'eventDetection\\\\done_on_imported'\n",
    "import_path = f'{pkl_dir}\\\\{import_type}.pkl'\n",
    "\n",
    "electrode_column_name = 'electrode'\n",
    "header = np.array(['Onset',\"Duration\",\"Annotation\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Nir&Andrillon Spindle Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sleep_stages(file_ids,event_key_to_use,after_sleepStage_exclution_key, sleepstages):\n",
    "    for id in file_ids:\n",
    "        if event_key_to_use not in allsubsdata_perFile[id]: continue\n",
    "        event_allElectrodes = allsubsdata_perFile[id][event_key_to_use]\n",
    "        \n",
    "        event_filtered_sleepstages = pd.DataFrame([], columns=event_allElectrodes.columns)\n",
    "        for sleepstage in sleepstages:\n",
    "            event_filtered_sleepstages = pd.concat([event_filtered_sleepstages , event_allElectrodes.loc[(event_allElectrodes['currentStage'] == sleepstage)]])\n",
    "\n",
    "        if len(event_filtered_sleepstages) == 0:  print(f\"sub:{id} - no events in sleepstages{sleepstages}\")\n",
    "        else: print(f\"sub:{id}, before exclude sleep stages:{len(event_allElectrodes)}, after:{len(event_filtered_sleepstages)}\")\n",
    "        \n",
    "        allsubsdata_perFile[id][after_sleepStage_exclution_key] = event_filtered_sleepstages\n",
    "def filter_kcompCriteria(file_ids,event_key_to_use,afterkcompCriteria_exclution_key):\n",
    "            # % Result Matrix\n",
    "        # %1:  wave beginning (sample)\n",
    "        # %2:  wave end (sample)\n",
    "        # %3:  wave middle point (sample)\n",
    "        # %4:  wave negative half-way (sample)\n",
    "        # %5:  period in seconds\n",
    "        # %6:  positive amplitude peak\n",
    "        # %7:  positive amplitude peak position (sample)\n",
    "        # %8:  negative amplitude peak\n",
    "        # %9:  negative amplitude peak position (sample)\n",
    "        # %10: peak-to-peak amplitude\n",
    "        # %11: 1st pos peak amplitude\n",
    "        # %12: 1st pos peak amplitude position (sample)\n",
    "        # %13: Last pos peak amplitude\n",
    "        # %14: Last pos peak amplitude position (sample)\n",
    "        # %15: 1st neg peak amplitude\n",
    "        # %16: 1st neg peak amplitude position (sample)\n",
    "        # %17: mean wave amplitude\n",
    "        # %18: number of positive peaks\n",
    "        # %19: wave negative half-way period\n",
    "        # %20: 1st peak to last peak period\n",
    "        # %21: determines instantaneous positive 1st segement slope on smoothed signal\n",
    "        # %22: determines maximal negative slope for 2nd segement\n",
    "        # %23: stage (if scored data)\n",
    "    for id in file_ids:\n",
    "        if event_key_to_use not in allsubsdata_perFile[id]: continue\n",
    "        event_allElectrodes = allsubsdata_perFile[id][event_key_to_use]\n",
    "        event_filtered_kcompCrit = pd.DataFrame([], columns=event_allElectrodes.columns)\n",
    "\n",
    "        event_filtered_kcompCrit = event_allElectrodes[\n",
    "                                                        (event_allElectrodes['period'] >= 0.5) &\n",
    "                                                        (event_allElectrodes['n1'] < 45) &\n",
    "                                                        ((event_allElectrodes['currentStage'] == 2.0) | (event_allElectrodes['currentStage'] == 3.0))\n",
    "                                                       ]\n",
    "\n",
    "        if len(event_filtered_kcompCrit) == 0:  print(f\"sub:{id} - no kcomp with criteria\")\n",
    "        else: print(f\"sub:{id}, before wave criteria sleep stages:{len(event_allElectrodes)}, after:{len(event_filtered_kcompCrit)}\")\n",
    "        \n",
    "        allsubsdata_perFile[id][afterkcompCriteria_exclution_key] = event_filtered_kcompCrit\n",
    "def filter_waveCriteria(file_ids,event_key_to_use,after_sleepStage_exclution_key):\n",
    "        # % Result Matrix\n",
    "        # %1:  wave beginning (sample)\n",
    "        # %2:  wave end (sample)\n",
    "        # %3:  wave middle point (sample)\n",
    "        # %4:  wave negative half-way (sample)\n",
    "        # %5:  period in seconds\n",
    "        # %6:  positive amplitude peak\n",
    "        # %7:  positive amplitude peak position (sample)\n",
    "        # %8:  negative amplitude peak\n",
    "        # %9:  negative amplitude peak position (sample)\n",
    "        # %10: peak-to-peak amplitude\n",
    "        # %11: 1st pos peak amplitude\n",
    "        # %12: 1st pos peak amplitude position (sample)\n",
    "        # %13: Last pos peak amplitude\n",
    "        # %14: Last pos peak amplitude position (sample)\n",
    "        # %15: 1st neg peak amplitude\n",
    "        # %16: 1st neg peak amplitude position (sample)\n",
    "        # %17: mean wave amplitude\n",
    "        # %18: number of positive peaks\n",
    "        # %19: wave negative half-way period\n",
    "        # %20: 1st peak to last peak period\n",
    "        # %21: determines instantaneous positive 1st segement slope on smoothed signal\n",
    "        # %22: determines maximal negative slope for 2nd segement\n",
    "        # %23: stage (if scored data)\n",
    "    for id in file_ids:\n",
    "        if event_key_to_use not in allsubsdata_perFile[id]: continue\n",
    "        event_allElectrodes = allsubsdata_perFile[id][event_key_to_use]\n",
    "        event_filtered_waveCrit = pd.DataFrame([], columns=event_allElectrodes.columns)\n",
    "\n",
    "        event_filtered_waveCrit = event_allElectrodes[(event_allElectrodes['maxb2c'] > 75)]\n",
    "\n",
    "        if len(event_filtered_waveCrit) == 0:  print(f\"sub:{id} - no sw with criteria\")\n",
    "        else: print(f\"sub:{id}, before wave criteria sleep stages:{len(event_allElectrodes)}, after:{len(event_filtered_waveCrit)}\")\n",
    "        \n",
    "        allsubsdata_perFile[id][after_sleepStage_exclution_key] = event_filtered_waveCrit\n",
    "\n",
    "def edfViewFormat_scoring_dict(score):\n",
    "    if score == 0:\n",
    "        return 'W'\n",
    "    elif score ==1:\n",
    "        return 'N1'\n",
    "    elif score ==2:\n",
    "        return 'N2'\n",
    "    elif score ==3:\n",
    "        return 'N3'\n",
    "    elif score ==4:\n",
    "        return 'TREM'\n",
    "    elif score ==5:\n",
    "        return 'PREM'\n",
    "    elif score ==6:\n",
    "        return 'MOVE'\n",
    "    elif score ==7:\n",
    "        return 'ARTIFACT'\n",
    "    else:\n",
    "        Exception('no such score')\n",
    "def add_edfViewFormat_scoring(key_edfScoringFormat):\n",
    "    for id in allsubsdata_perFile:\n",
    "        curr_file_scoring = allsubsdata_perFile[id]['scoring']\n",
    "        new_format_score = np.zeros((len(curr_file_scoring),3), dtype=object)\n",
    "        for ind, score in enumerate(curr_file_scoring):\n",
    "                new_format_score[ind,:] = [30*ind,30,edfViewFormat_scoring_dict(score)] ## onset (sec), duration, desc\n",
    "\n",
    "        allsubsdata_perFile[id][key_edfScoringFormat] = new_format_score\n",
    "\n",
    "   \n",
    "# Save events (scoring and ss) per file, in a format suitable for EDF_viewer\n",
    "def save_eventsAllTogether_edfViewFormat(events_types_for_save,edfViewFormat_output_dir):\n",
    "        for id in file_ids:\n",
    "            filename = f\"{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_events\"\n",
    "            all_events_with_header = np.asarray([header],dtype=object)\n",
    "            for event_type in events_types_for_save:\n",
    "                events_type_found = [ v for k, v in allsubsdata_perFile[id].items() if (event_type in k) and (edfviewFormatSuffix in k)]\n",
    "                if np.size(events_type_found) == 0: continue\n",
    "                for events_found in events_type_found:\n",
    "                        for event_found in events_found:\n",
    "                                all_events_with_header = np.concatenate((all_events_with_header,[event_found]),dtype=object)\n",
    "            np.savetxt(f\"{edfViewFormat_output_dir}\\\\allEvents_{filename}.txt\", all_events_with_header, delimiter='\\t',fmt='%s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS TO GET COMPARISONS OF SPINDLES MINMANX_SD OPTIONS\n",
    "run_comparison_of_spindle_minMax = False\n",
    "\n",
    "def detect_ss_AndrillonNir(file_ids,minMax_sd_ver,output_key,electrodes_names):\n",
    "    andriNir_code_dir = os.getcwd()\n",
    "    andriNir_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\\\\Andrillon_Nir\"\n",
    "    andriNir_aux_output_dir = f\"{andriNir_output_dir}\\\\aux_mats\"\n",
    "\n",
    "    if os.path.exists(andriNir_output_dir):\n",
    "        shutil.rmtree(andriNir_output_dir)\n",
    "    os.mkdir(andriNir_output_dir)\n",
    "    os.mkdir(andriNir_aux_output_dir)\n",
    "\n",
    "    spindles_output_columns =  ['spindleStartTime', 'spindleEndTime', 'peakTime', 'peakEnergy', 'peakEnergyNorm', 'freqSpindle', 'spindleDuration/SR', 'PowerSP', 'PowerAlpha', 'currentStage']\n",
    "\n",
    "    for id in file_ids:\n",
    "        datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "        datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "        scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=True)\n",
    "        ss_allElectrodes = pd.DataFrame()\n",
    "\n",
    "        for electd_i, electrode_name_eventDetect in enumerate(electrodes_names):\n",
    "            curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name_eventDetect)[0][0]\n",
    "\n",
    "            ## create aux files to use in MATLAB\n",
    "            datafile_1elect_eeg = datafile_data[curr_electrode_num,:]\n",
    "            if 2 not in datafile_scoring or 3 not in datafile_scoring:\n",
    "                continue\n",
    "            \n",
    "            DetectionThreshold = minMax_sd_ver[0]\n",
    "            RejectThreshold = minMax_sd_ver[1]\n",
    "            StartEndThreshold = minMax_sd_ver[2]\n",
    "            mat_to_save = {'datafile_data': datafile_1elect_eeg, 'scoring_upsampled': scoring_upsampled, 'sample_freq': configu['sample_freq'], 'electrode_name':electrode_name_eventDetect, 'DetectionThreshold':DetectionThreshold,'RejectThreshold':RejectThreshold, 'StartEndThreshold':StartEndThreshold}\n",
    "            scipy.io.savemat(f\"{andriNir_aux_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\",mat_to_save)\n",
    "\n",
    "            ## run Andrillon & Nir SS detection over all subjects files\n",
    "            eng = matlab.engine.start_matlab()\n",
    "            eng.cd(andriNir_code_dir, nargout=0)\n",
    "            out = eng.batch_useAndrillonNirSSDetection(andriNir_aux_output_dir, andriNir_output_dir,nargout=0)\n",
    "            eng.quit()\n",
    "\n",
    "            ## add the spindles data to the main subject dictionary\n",
    "            try: \n",
    "                spindles_file_name = f\"{andriNir_output_dir}\\\\SS_AN_{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\"\n",
    "                matlabImport = scipy.io.loadmat(spindles_file_name, simplify_cells=True)\n",
    "            except Exception: \n",
    "                print(f\"Error importing spindles sub file at: {allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}\")\n",
    "                continue\n",
    "\n",
    "            ss = matlabImport['spindles']\n",
    "            if np.size(np.shape(ss)) == 1: \n",
    "                temp = np.zeros((1,np.size(ss)),dtype=object)\n",
    "                temp[0] = ss\n",
    "                ss = temp\n",
    "            if np.size(ss) >0 :\n",
    "                df = pd.DataFrame(np.double(ss))\n",
    "                df.columns = spindles_output_columns\n",
    "                tile_electrode = np.tile(f\"{electrode_name_eventDetect}\", ss.shape[0])[None].T\n",
    "                df[electrode_column_name] = tile_electrode\n",
    "                \n",
    "                allsubsdata_perFile[id][f'{output_key}@@{electrode_name_eventDetect}'] = df\n",
    "                if electd_i ==0: ss_allElectrodes =  df\n",
    "                else:  ss_allElectrodes = pd.concat([ss_allElectrodes,df])\n",
    "    \n",
    "        if not ss_allElectrodes.empty:\n",
    "            allsubsdata_perFile[id][output_key]  = ss_allElectrodes\n",
    "\n",
    "    minmax_sd_name =  matlabImport['minMax_SD_threshold']\n",
    "    shutil.rmtree(andriNir_aux_output_dir)\n",
    "    return minmax_sd_name\n",
    "def group_spindles(ss_key_to_use, uniqeElctds_ss_key, uniqeElctd_ss_key):\n",
    "    # The grouping is done such that you get 1 event per time frame.\n",
    "    # In the single condition, only one electrode is sufficiant to include event.\n",
    "    #  In the multi condition event will be includede only if it appears in sevral electrodes\n",
    "    # The picked electrode is the one where the event is with the most power\n",
    "    for id in allsubsdata_perFile:\n",
    "        if ss_key_to_use not in allsubsdata_perFile[id]:  continue\n",
    "        filterd_events_allElectrodes = allsubsdata_perFile[id][ss_key_to_use].copy(deep=True)\n",
    "        \n",
    "        filterd_events_allElectrodes.sort_values(by=['spindleStartTime'],inplace=True) \n",
    "        filterd_events_allElectrodes.reset_index(drop=True, inplace=True)\n",
    "        deleted = filterd_events_allElectrodes.copy(deep=True)\n",
    "        filtered = pd.DataFrame([], columns = deleted.columns)\n",
    "        filtered_moreThan1 = pd.DataFrame([], columns = deleted.columns)\n",
    "        simultan = pd.DataFrame([], columns = deleted.columns)\n",
    "\n",
    "        while len(deleted)>0:\n",
    "            if len(simultan)==0:\n",
    "                simultan = pd.concat([simultan, deleted.iloc[[0]]])\n",
    "                deleted.drop(deleted.index[0], axis=0, inplace=True)\n",
    "                still_overlap = True\n",
    "            else:\n",
    "                while still_overlap == True and len(deleted)>0:\n",
    "                    still_overlap = False\n",
    "                    simultan.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "\n",
    "                    ## check now_overlap_all_in_simultan:\n",
    "                    for index, simultan_row in simultan.iterrows():\n",
    "                        simultan_0 = simultan_row['spindleStartTime']\n",
    "                        simultan_1 = simultan_row['spindleEndTime']\n",
    "                        deleted_0 = deleted.iloc[0]['spindleStartTime']\n",
    "                        deleted_1 = deleted.iloc[0]['spindleEndTime']\n",
    "                        if overlap([simultan_0,simultan_1],[deleted_0,deleted_1]):\n",
    "                            simultan = pd.concat([simultan, deleted.iloc[[0]]])\n",
    "                            deleted.drop(deleted.index[0], axis=0, inplace=True)\n",
    "                            still_overlap = True\n",
    "                            break\n",
    "                    if still_overlap: continue\n",
    "                    else:\n",
    "                        ## check max ps and add to filt\n",
    "                        # if len(simultan)>1:\n",
    "                        #     print('hu')\n",
    "                        simultan.sort_values(by=['PowerSP'],ascending=False,inplace=True)\n",
    "                        row_df = simultan.iloc[[0]]\n",
    "                        row_df.iat[0, row_df.columns.get_loc(electrode_column_name)] = np.array(simultan[electrode_column_name])\n",
    "                        filtered = pd.concat([filtered,row_df])\n",
    "                        if len(simultan) > 1: filtered_moreThan1 = pd.concat([filtered_moreThan1,row_df])\n",
    "                        simultan = pd.DataFrame([], columns = deleted.columns)                    \n",
    "            \n",
    "        filtered.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "        filtered_moreThan1.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "        allsubsdata_perFile[id][uniqeElctd_ss_key] = filtered   \n",
    "        allsubsdata_perFile[id][uniqeElctds_ss_key] = filtered_moreThan1   \n",
    "        print(f\"sub:{id}, before filt:{np.shape(allsubsdata_perFile[id][ss_key_to_use])[0]}, after:{np.shape(allsubsdata_perFile[id][uniqeElctd_ss_key])[0]}, after>1: after:{np.shape(allsubsdata_perFile[id][uniqeElctds_ss_key])[0]}\")\n",
    "\n",
    "def add_edfViewFormat_ss(file_ids,event_key_for_save, SS_efdViewFormat_key):\n",
    "    if event_key_for_save not in allsubsdata_perFile[id]:\n",
    "            #print(f\"no {event_key_for_save} for sub {id}\")\n",
    "            continue\n",
    "    kcomp_df = allsubsdata_perFile[id][event_key_for_save]\n",
    "\n",
    "    startTime_arr = np.array(kcomp_df['wavest'])\n",
    "    endTime_arr = np.array(kcomp_df['wavend'])\n",
    "    duration_arr = np.round(endTime_arr - startTime_arr,2)\n",
    "    electd_arr_per_sw = np.array(kcomp_df[electrode_column_name])\n",
    "    desc = [f\"kcomp@@{electd_arr}\" for electd_arr in electd_arr_per_sw]\n",
    "    new_format_kcomp = np.array([startTime_arr ,duration_arr,desc]).T ## onset (sec), duration,desc\n",
    "\n",
    "    allsubsdata_perFile[id][kcomp_efdViewFormat_key] = new_format_kcomp\n",
    "\n",
    "def save_eventsSepratelyForComparison_edfViewFormat(file_ids,events_types_to_compare,edfViewFormat_output_dir):\n",
    "    # Save events (scoring and ss) per file, in a format suitable for EDF_viewer\n",
    "    for id in file_ids:\n",
    "        for event_type in events_types_to_compare:\n",
    "            filename = f\"{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_events\"\n",
    "            all_events_with_header = np.asarray([header],dtype=object)\n",
    "            events_type_found = [ v for k, v in allsubsdata_perFile[id].items() if ((event_type in k) and (edfviewFormatSuffix in k))]\n",
    "            if np.size(events_type_found) == 0: continue\n",
    "            for events_found in events_type_found:\n",
    "                    for event_found in events_found:\n",
    "                            all_events_with_header = np.concatenate((all_events_with_header,[event_found]),dtype=object)\n",
    "            np.savetxt(f\"{edfViewFormat_output_dir}\\\\{event_type}_{filename}.txt\", all_events_with_header, delimiter='\\t',fmt='%s')\n",
    " \n",
    "if(run_comparison_of_spindle_minMax):\n",
    "    testType = 0\n",
    "    if(testType==1):\n",
    "        file_ids = ['38_2']\n",
    "        minMax_sd_vers = [[4,10,1]]\n",
    "        electrodes_names_eventDetect = ['Fp2','F3']\n",
    "    elif(testType==2):\n",
    "        file_ids = ['32_2','38_2']\n",
    "        minMax_sd_vers = [[3,9,1],[4,10,1]]\n",
    "        electrodes_names_eventDetect = ['Fp2','F3']\n",
    "    elif(testType==3):\n",
    "        file_ids = ['32_2','38_2']\n",
    "        minMax_sd_vers = [[3,9,1],[4,10,1]]\n",
    "        electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "    else:\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "        minMax_sd_vers = [[3,8,1],[4,8,1],[5,8,1],[3,9,1],[4,9,1],[5,9,1],[3,10,1],[4,10,1],[5,10,1]]\n",
    "        #file_ids = allsubsdata_perFile.keys()\n",
    "\n",
    "    with open(import_path, \"rb\") as file:\n",
    "        [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "    fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "    if not os.path.exists(fig_output_dir): os.mkdir(fig_output_dir)\n",
    "\n",
    "    edfViewFormat_events_manual_dir = f'{fig_output_dir}\\\\EDFViewFormat_events'\n",
    "    edfViewFormat_events_output_dir = f'{fig_output_dir}\\\\EDFViewFormat_events\\\\minmax_sd_tests'\n",
    "    if os.path.exists(edfViewFormat_events_output_dir):\n",
    "        shutil.rmtree(edfViewFormat_events_output_dir)\n",
    "    os.mkdir(edfViewFormat_events_output_dir)\n",
    "\n",
    "    fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "    if not os.path.exists(fig_output_dir): os.mkdir(fig_output_dir)\n",
    "\n",
    "    for minMax_sd_ver in minMax_sd_vers:\n",
    "        with open(import_path, \"rb\") as file: [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "        \n",
    "        all_electodes_ss_key = 'SS_AN'\n",
    "        minmax_sd_name = detect_ss_AndrillonNir(file_ids,minMax_sd_ver,all_electodes_ss_key,electrodes_names_eventDetect)\n",
    "        print(minmax_sd_name)\n",
    "\n",
    "        after_sleepStage_exclution_key = f\"{all_electodes_ss_key}_n2n3\"\n",
    "        filter_sleep_stages(file_ids,all_electodes_ss_key,after_sleepStage_exclution_key, [2.0,3.0]) \n",
    "\n",
    "        sw_key_to_use = after_sleepStage_exclution_key\n",
    "        multiElectdPerSS_text = \"multiElectd\"\n",
    "        singleElectdPerSS_text = \"singleElectd\"\n",
    "        multiElectdPerSS_key = f\"{sw_key_to_use}_{multiElectdPerSS_text}\"\n",
    "        singleElectdPerSS_key = f\"{sw_key_to_use}_{singleElectdPerSS_text}\"\n",
    "        group_spindles(sw_key_to_use,multiElectdPerSS_key, singleElectdPerSS_key)\n",
    "\n",
    "        edfviewFormatSuffix = 'efdViewFormat'\n",
    "        SS_multiElectdPerSS_efdViewFormat_key = f'{multiElectdPerSS_key}_{edfviewFormatSuffix}'\n",
    "        SS_singleElectdPerSS_efdViewFormat_key = f'{singleElectdPerSS_key}_{edfviewFormatSuffix}'\n",
    "        add_edfViewFormat_ss(file_ids,multiElectdPerSS_key,SS_multiElectdPerSS_efdViewFormat_key)\n",
    "        add_edfViewFormat_ss(file_ids,singleElectdPerSS_key,SS_singleElectdPerSS_efdViewFormat_key)\n",
    "\n",
    "        events_types_to_compare = [multiElectdPerSS_text,singleElectdPerSS_text]    \n",
    "        edfViewFormat_eventsTest_output_dir = f\"{edfViewFormat_events_output_dir}\\\\{minMax_sd_ver}\"\n",
    "        if os.path.exists(edfViewFormat_eventsTest_output_dir):  shutil.rmtree(edfViewFormat_eventsTest_output_dir)\n",
    "        os.mkdir(edfViewFormat_eventsTest_output_dir)\n",
    "        save_eventsSepratelyForComparison_edfViewFormat(file_ids,events_types_to_compare,edfViewFormat_eventsTest_output_dir)\n",
    "\n",
    "        scoring_edfViewFormat_key = f'scoring_{edfviewFormatSuffix}'\n",
    "        add_edfViewFormat_scoring(scoring_edfViewFormat_key)\n",
    "\n",
    "        events_types_to_save = ['scoring', multiElectdPerSS_text,singleElectdPerSS_text]    \n",
    "        save_eventsAllTogether_edfViewFormat(events_types_to_save,edfViewFormat_eventsTest_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preform_minmaxSD_comparison(events_types_to_compare,file_ids,minMax_sd_vers):\n",
    "    all_comparisons = pd.DataFrame(columns=['id','eventType','minmax_ver','Positive','TruePos','FalsePos', 'hitRate','falseDiscoveryRate'])\n",
    "    for id in file_ids:\n",
    "        for ver in minMax_sd_vers:\n",
    "            for event_type in events_types_to_compare:\n",
    "                dir_auto_ = f\"{edfViewFormat_events_output_dir}\\\\{ver}\"\n",
    "                manual_detection_filename = f\"{edfViewFormat_events_manual_dir}\\\\sub{id.split('_')[0]}_sleep{id.split('_')[1]}_imported_eventDetectionChan_annotations.txt\"\n",
    "                auto_detection_filename = f\"{dir_auto_}\\\\{event_type}_{id}_events.txt\"\n",
    "                ## get the array of before manual scanning\n",
    "                if os.path.exists(auto_detection_filename) and os.path.exists(manual_detection_filename):\n",
    "                    all_ss_auto = np.loadtxt(auto_detection_filename, delimiter=\"\\t\",dtype='object')\n",
    "                    all_ss_auto = np.delete(all_ss_auto, np.where(all_ss_auto[:, 2] == \"Annotation\")[0], axis=0)\n",
    "                    all_ss_auto[:,[0,1]] = [np.double(x) for x in all_ss_auto[:,[0,1]]]\n",
    "                    ss_ind = np.array([],dtype=int)\n",
    "                    for ind_i, desc in enumerate(all_ss_auto[:,2]):\n",
    "                        if (\"SS\" in desc) or (\"ss\" in desc):\n",
    "                            ss_ind = np.append(ss_ind, ind_i)\n",
    "                    all_ss_auto = all_ss_auto[ss_ind,:]\n",
    "\n",
    "                    ## get the array of after manual scanning\n",
    "                    all_ss_manu = np.loadtxt(manual_detection_filename, delimiter=\"\\t\",dtype='object')\n",
    "                    all_ss_manu = np.delete(all_ss_manu, np.where(all_ss_manu[:, 2] == \"Annotation\")[0], axis=0)\n",
    "                    all_ss_manu[:,[0,1]] = [np.double(x) for x in all_ss_manu[:,[0,1]]]\n",
    "                    ss_ind = np.array([],dtype=int)\n",
    "                    for ind_i, desc in enumerate(all_ss_manu[:,2]):\n",
    "                        if \"SS\" in desc:\n",
    "                            ss_ind = np.append(ss_ind, ind_i)\n",
    "                    all_ss_manu = all_ss_manu[ss_ind,:]\n",
    "\n",
    "                    ## compare to find rate od TP and FP\n",
    "                    TP = 0\n",
    "                    FP = 0\n",
    "                    for ss_auto in all_ss_auto:\n",
    "                        found = False\n",
    "                        for ss_manu in all_ss_manu:\n",
    "                            if overlap([ss_auto[0],ss_auto[0]+ss_auto[1]],[ss_manu[0],ss_manu[0]+ss_manu[1]]):\n",
    "                                TP +=1\n",
    "                                found = True\n",
    "                                break\n",
    "                        if found == False:\n",
    "                            FP +=1\n",
    "\n",
    "                    # FN = 0\n",
    "                    # for ss_manu in all_ss_manu:\n",
    "                    #     found = False\n",
    "                    #     for ss_auto in all_ss_auto:\n",
    "                    #         if overlap([ss_auto[0],ss_auto[0]+ss_auto[1]],[ss_manu[0],ss_manu[0]+ss_manu[1]]):\n",
    "                    #             found = True\n",
    "                    #             break\n",
    "                    #     if found == False:\n",
    "                    #         FN +=1\n",
    "\n",
    "                    Positive = np.shape(all_ss_manu)[0]   \n",
    "                    hitRate = TP/Positive\n",
    "                    #missRate = FN/Positive\n",
    "                    falseDiscoveryRate = FP/(FP+TP)\n",
    "                    \n",
    "                    comparison = [id,event_type, ver, Positive, TP, FP,hitRate, falseDiscoveryRate]\n",
    "                    all_comparisons.loc[len(all_comparisons)] = comparison\n",
    "\n",
    "    all_comparisons = all_comparisons.sort_values('minmax_ver')\n",
    "    all_comparisons = all_comparisons.sort_values('id')\n",
    "    return all_comparisons\n",
    "if(run_comparison_of_spindle_minMax):\n",
    "    events_types_to_compare = [multiElectdPerSS_text,singleElectdPerSS_text]\n",
    "    all_comp = preform_minmaxSD_comparison(events_types_to_compare,file_ids,minMax_sd_vers)\n",
    "    # all_comp.to_csv('ss_comparisons.csv',index=False)\n",
    "    display(all_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN this to get scoring events files only\n",
    "run_scoring_eventFiles = False\n",
    "if(run_scoring_eventFiles):\n",
    "    testType = 1\n",
    "    if(testType==1):\n",
    "        with open(import_path, \"rb\") as file: [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "        file_ids = allsubsdata_perFile.keys()\n",
    "        electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "\n",
    "\n",
    "    fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "    if not os.path.exists(fig_output_dir): os.mkdir(fig_output_dir)\n",
    "\n",
    "    edfViewFormat_events_manual_dir = f'{fig_output_dir}\\\\EDFViewFormat_events'\n",
    "    edfViewFormat_events_output_dir = f'{fig_output_dir}\\\\EDFViewFormat_events\\\\only_scoring'\n",
    "    if os.path.exists(edfViewFormat_events_output_dir):\n",
    "        shutil.rmtree(edfViewFormat_events_output_dir)\n",
    "    os.mkdir(edfViewFormat_events_output_dir)\n",
    "\n",
    "    all_electodes_sw_key = 'SW_AN'\n",
    "\n",
    "    edfviewFormatSuffix = 'efdViewFormat'\n",
    "\n",
    "    scoring_edfViewFormat_key = f'scoring_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_scoring(scoring_edfViewFormat_key)\n",
    "\n",
    "    events_types_to_save = ['scoring']    \n",
    "    save_eventsAllTogether_edfViewFormat(events_types_to_save,edfViewFormat_events_output_dir)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08-מרץ-23 00:12:22 | WARNING | Hypnogram is LONGER than data by 6.99 seconds. Cropping hypnogram to match data.size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub:38_2, before exclude sleep stages:20237, after:11281\n",
      "sub:38_2, before wave criteria sleep stages:11281, after:2324\n"
     ]
    }
   ],
   "source": [
    "## RUN THIS TO GET AN_SlowWaves events\n",
    "run_comparison_of_sw_minMax = True\n",
    "\n",
    "def detect_sw_AndrillonNir(file_ids,output_key,electrodes_names):\n",
    "    andriNir_code_dir = os.getcwd()\n",
    "    andriNir_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\\\\Andrillon_Nir\"\n",
    "    andriNir_aux_output_dir = f\"{andriNir_output_dir}\\\\aux_mats\"\n",
    "\n",
    "    if os.path.exists(andriNir_output_dir):\n",
    "        shutil.rmtree(andriNir_output_dir)\n",
    "    os.mkdir(andriNir_output_dir)\n",
    "    os.mkdir(andriNir_aux_output_dir)\n",
    "\n",
    "    sw_output_columns = ['wavest' ,'wavend', 'mdpt', 'poszx', 'period/SR' ,'abs(b)', 'bx', 'c' ,'cx' ,'maxb2c', 'n1', 'n1x', 'nEnd', 'nEndx' ,'p1', 'p1x' ,'meanAmp', 'nump', 'nperiod/SR', 'p2p', 'mxdn', 'mxup' ,'currentStage']\n",
    "\n",
    "    for id in file_ids:\n",
    "        datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "        datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "        scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=True)\n",
    "        sw_allElectrodes = pd.DataFrame()\n",
    "\n",
    "        for electd_i, electrode_name_eventDetect in enumerate(electrodes_names):\n",
    "            curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name_eventDetect)[0][0]\n",
    "\n",
    "            ## create aux files to use in MATLAB\n",
    "            datafile_1elect_eeg = datafile_data[curr_electrode_num,:]\n",
    "            if 2 not in datafile_scoring or 3 not in datafile_scoring:\n",
    "                continue\n",
    "            \n",
    "            mat_to_save = {'datafile_data': datafile_1elect_eeg, 'scoring_upsampled': scoring_upsampled, 'sample_freq': configu['sample_freq'], 'electrode_name':electrode_name_eventDetect}\n",
    "            scipy.io.savemat(f\"{andriNir_aux_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\",mat_to_save)\n",
    "\n",
    "            ## run Andrillon & Nir SW detection over all subjects files\n",
    "            eng = matlab.engine.start_matlab()\n",
    "            eng.cd(andriNir_code_dir, nargout=0)\n",
    "            out = eng.batch_useAndrillonNirSWDetection(andriNir_aux_output_dir, andriNir_output_dir,nargout=0)\n",
    "            eng.quit()\n",
    "\n",
    "            ## add the SW data to the main subject dictionary\n",
    "            try: \n",
    "                sw_file_name = f\"{andriNir_output_dir}\\\\{output_key}_{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\"\n",
    "                matlabImport = scipy.io.loadmat(sw_file_name, simplify_cells=True)\n",
    "            except Exception: \n",
    "                print(f\"Error importing spindles sub file at: {allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}\")\n",
    "                continue\n",
    "\n",
    "            sw = matlabImport['allWaves']\n",
    "            if np.size(np.shape(sw)) == 1: \n",
    "                temp = np.zeros((1,np.size(sw)),dtype=object)\n",
    "                temp[0] = sw\n",
    "                sw = temp\n",
    "            if np.size(sw) >0 :\n",
    "                df = pd.DataFrame(np.double(sw))\n",
    "                df.columns = sw_output_columns\n",
    "                tile_electrode = np.tile(f\"{electrode_name_eventDetect}\", sw.shape[0])[None].T\n",
    "                df[electrode_column_name] = tile_electrode\n",
    "                \n",
    "                allsubsdata_perFile[id][f\"{output_key}@@{electrode_name_eventDetect}\"] = df\n",
    "                if electd_i ==0: sw_allElectrodes =  df\n",
    "                else:  sw_allElectrodes = pd.concat([sw_allElectrodes,df])\n",
    "    \n",
    "        if not sw_allElectrodes.empty:\n",
    "            allsubsdata_perFile[id][output_key]  = sw_allElectrodes\n",
    "    shutil.rmtree(andriNir_aux_output_dir)\n",
    "def add_edfViewFormat_sw(file_ids,event_key_for_save, SW_efdViewFormat_key):\n",
    "        for id in file_ids:\n",
    "            if event_key_for_save not in allsubsdata_perFile[id]:\n",
    "                    #print(f\"no {event_key_for_save} for sub {id}\")\n",
    "                    continue\n",
    "            sw_df = allsubsdata_perFile[id][event_key_for_save]\n",
    "    \n",
    "            startTime_arr = np.array(sw_df['wavest'])\n",
    "            endTime_arr = np.array(sw_df['wavend'])\n",
    "            duration_arr = (endTime_arr - startTime_arr)  / np.double(configu['sample_freq'])\n",
    "            electd_arr_per_sw = np.array(sw_df[electrode_column_name])\n",
    "            desc = [f\"SW@@{electd_arr}\" for electd_arr in electd_arr_per_sw]\n",
    "            new_format_sw = np.array([startTime_arr / np.double(configu['sample_freq']) ,duration_arr,desc]).T ## onset (sec), duration,desc\n",
    "\n",
    "            allsubsdata_perFile[id][SW_efdViewFormat_key] = new_format_sw\n",
    "\n",
    "if(run_comparison_of_sw_minMax):\n",
    "    testType = 1\n",
    "    if(testType==1):\n",
    "        file_ids = ['38_2']\n",
    "        electrodes_names_eventDetect = ['Fp2','F3']\n",
    "    elif(testType==2):\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp2','F3']\n",
    "    elif(testType==3):\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "    else:\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "\n",
    "    with open(import_path, \"rb\") as file:  [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "    fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "    if not os.path.exists(fig_output_dir): os.mkdir(fig_output_dir)\n",
    "\n",
    "    edfViewFormat_events_manual_dir = f'{fig_output_dir}\\\\EDFViewFormat_events'\n",
    "    edfViewFormat_events_output_dir = f'{fig_output_dir}\\\\EDFViewFormat_events\\\\sw_AN_tests'\n",
    "    if os.path.exists(edfViewFormat_events_output_dir):\n",
    "        shutil.rmtree(edfViewFormat_events_output_dir)\n",
    "    os.mkdir(edfViewFormat_events_output_dir)\n",
    "\n",
    "    \n",
    "    all_electodes_sw_key = 'SW_AN'\n",
    "    res = detect_sw_AndrillonNir(file_ids,all_electodes_sw_key,electrodes_names_eventDetect)\n",
    "\n",
    "    after_sleepStage_exclution_key = f\"{all_electodes_sw_key}_n2n3\"\n",
    "    filter_sleep_stages(file_ids,all_electodes_sw_key,after_sleepStage_exclution_key, [2.0,3.0]) \n",
    "\n",
    "    sw_key_to_use = after_sleepStage_exclution_key\n",
    "    sw_key_after_SWExclusion = f\"{after_sleepStage_exclution_key}_swExclusion\"\n",
    "    filter_waveCriteria(file_ids,sw_key_to_use,sw_key_after_SWExclusion)\n",
    "\n",
    "    edfviewFormatSuffix = 'efdViewFormat'\n",
    "    SW_multiElectdPerSW_efdViewFormat_key = f'{sw_key_after_SWExclusion}_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_sw(file_ids,sw_key_after_SWExclusion,SW_multiElectdPerSW_efdViewFormat_key)\n",
    "\n",
    "    # events_types_to_compare = [multiElectdPerSW_text,singleElectdPerSW_text]    \n",
    "    # edfViewFormat_eventsTest_output_dir = f\"{edfViewFormat_events_output_dir}\\\\{minMax_sd_ver}\"\n",
    "    # if os.path.exists(edfViewFormat_eventsTest_output_dir):  shutil.rmtree(edfViewFormat_eventsTest_output_dir)\n",
    "    # os.mkdir(edfViewFormat_eventsTest_output_dir)\n",
    "    # save_eventsSepratelyForComparison_edfViewFormat(file_ids,events_types_to_compare,edfViewFormat_eventsTest_output_dir)\n",
    "\n",
    "    scoring_edfViewFormat_key = f'scoring_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_scoring(scoring_edfViewFormat_key)\n",
    "\n",
    "    events_types_to_save = ['scoring', SW_multiElectdPerSW_efdViewFormat_key]    \n",
    "    save_eventsAllTogether_edfViewFormat(events_types_to_save,edfViewFormat_events_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-מרץ-23 11:55:07 | WARNING | Hypnogram is LONGER than data by 3.59 seconds. Cropping hypnogram to match data.size.\n",
      "09-מרץ-23 11:55:12 | WARNING | Hypnogram is LONGER than data by 6.99 seconds. Cropping hypnogram to match data.size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub:32_2, before filt:1132, after:370, after>1: after:261\n",
      "sub:38_2, before filt:2311, after:838, after>1: after:495\n"
     ]
    }
   ],
   "source": [
    "## RUN ShaYKM_kcomplex events - find max first\n",
    "\n",
    "# Sample rate and desired cutoff frequencies (in Hz).\n",
    "new_freq = 100\n",
    "\n",
    "kcomp_df_columns = ['pos_i','pos_amp','neg_i','neg_amp','power',electrode_column_name]\n",
    "kcomp_df =  pd.DataFrame([], columns = kcomp_df_columns)\n",
    "\n",
    "def detect_kComp_shaYKM(file_ids,output_key,electrodes_names):\n",
    "    for id in file_ids:\n",
    "        kcomp_df =  pd.DataFrame([], columns = kcomp_df_columns)\n",
    "\n",
    "        datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "        datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "        scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'])\n",
    "        \n",
    "        for electrode_name in electrodes_names:\n",
    "            curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name)[0][0]\n",
    "            data = allsubsdata_perFile[id]['data'][curr_electrode_num] ## shape (time)\n",
    "            z = new_freq / configu['sample_freq']\n",
    "            resampled = signal.resample(data, int(np.size(data)*z)) ## downsample to 100hz\n",
    "\n",
    "            lowcut = 0.3\n",
    "            highcut = 3.0\n",
    "            filer_order = 3\n",
    "            smoothing_window = 3\n",
    "            smoothen  = scipy.signal.medfilt(resampled, smoothing_window)\n",
    "            filtered_sw = butter_bandpass_filter(smoothen, lowcut, highcut, new_freq, order=filer_order) ## oreder can be changed. see: https://stackoverflow.com/questions/12093594/how-to-implement-band-pass-butterworth-filter-with-scipy-signal-butter\n",
    "            \n",
    "            lowcut = 0.5\n",
    "            highcut = 6.0\n",
    "            filer_order = 2\n",
    "            filtered_transiantMinima = butter_bandpass_filter(resampled, lowcut, highcut, new_freq, order=filer_order)\n",
    "            \n",
    "            ## find all standalone maximas\n",
    "            distance_sw = 0.8 #sec\n",
    "            sw_min_width = 0.3 # sec\n",
    "            curr_maximas_inds, _ = find_peaks(filtered_sw, threshold=0,width=sw_min_width*new_freq, distance=distance_sw*new_freq)\n",
    "            for maxima_ind in curr_maximas_inds:\n",
    "                if (scoring_upsampled[int(maxima_ind/z)] ==2):\n",
    "                    # plt.plot(resampled[max(minima_ind-200,0):min(minima_ind+200,np.size(resampled)-1)])\n",
    "                    # plt.plot(filtered_sw[max(minima_ind-200,0):min(minima_ind+200,np.size(resampled)-1)])\n",
    "                    # plt.plot(filtered_notSW[max(minima_ind-200,0):min(minima_ind+200,np.size(resampled)-1)])\n",
    "                    # plt.show()\n",
    "                    maxima = filtered_sw[maxima_ind]\n",
    "\n",
    "                    range_for_neg_peak = 1 #in sec\n",
    "                    minima_scope = filtered_transiantMinima[maxima_ind - int(range_for_neg_peak*new_freq) : maxima_ind]\n",
    "                    distance_in_preSW = 1\n",
    "                    curr_minimas_inds, _ = find_peaks(-minima_scope, threshold=0,distance=distance_in_preSW*new_freq)\n",
    "\n",
    "                    if np.size(curr_minimas_inds) ==0 : continue\n",
    "\n",
    "                    min_minima_ind_inScope = curr_minimas_inds[np.argmin(minima_scope[curr_minimas_inds])]\n",
    "                    minima = minima_scope[min_minima_ind_inScope]\n",
    "                    minima_ind = min_minima_ind_inScope + (maxima_ind - int(range_for_neg_peak*new_freq))\n",
    "\n",
    "                    if (minima < maxima-75) and (minima<-40) and (maxima > 0.5*abs(minima)): ## found a k-comp!\n",
    "                        new_row =           {'pos_i':maxima_ind/z, \n",
    "                                            'pos_amp':maxima, \n",
    "                                            'neg_i':minima_ind/z, \n",
    "                                            'neg_amp':minima,\n",
    "                                            'power': maxima-minima,\n",
    "                                            electrode_column_name:electrode_name}\n",
    "                        new_row_df = pd.DataFrame([new_row], columns = kcomp_df_columns)\n",
    "                        kcomp_df = pd.concat([kcomp_df,new_row_df])\n",
    "            \n",
    "        if not kcomp_df.empty:\n",
    "            allsubsdata_perFile[id][output_key]  = kcomp_df\n",
    "        else: print(f'no events: {output_key}')\n",
    "\n",
    "def group_kcomp(kcomp_key_to_use, uniqeElctds_kcomp_key, uniqeElctd_kcomp_key):\n",
    "    # The grouping is done such that you get 1 event per time frame.\n",
    "    # In the single condition, only one electrode is sufficiant to include event.\n",
    "    #  In the multi condition event will be includede only if it appears in sevral electrodes\n",
    "    # The picked electrode is the one where the event is with the most power\n",
    "    for id in allsubsdata_perFile:\n",
    "        if kcomp_key_to_use not in allsubsdata_perFile[id]:  continue\n",
    "        filterd_events_allElectrodes = allsubsdata_perFile[id][kcomp_key_to_use].copy(deep=True)\n",
    "        \n",
    "        filterd_events_allElectrodes.sort_values(by=['neg_i'],inplace=True) \n",
    "        filterd_events_allElectrodes.reset_index(drop=True, inplace=True)\n",
    "        deleted = filterd_events_allElectrodes.copy(deep=True)\n",
    "        filtered = pd.DataFrame([], columns = deleted.columns)\n",
    "        filtered_moreThan1 = pd.DataFrame([], columns = deleted.columns)\n",
    "        simultan = pd.DataFrame([], columns = deleted.columns)\n",
    "\n",
    "        while len(deleted)>0:\n",
    "            if len(simultan)==0:\n",
    "                simultan = pd.concat([simultan, deleted.iloc[[0]]])\n",
    "                deleted.drop(deleted.index[0], axis=0, inplace=True)\n",
    "                still_overlap = True\n",
    "            else:\n",
    "                while still_overlap == True and len(deleted)>0:\n",
    "                    still_overlap = False\n",
    "                    simultan.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "\n",
    "                    ## check now_overlap_all_in_simultan:\n",
    "                    for index, simultan_row in simultan.iterrows():\n",
    "                        simultan_0 = simultan_row['neg_i']\n",
    "                        simultan_1 = simultan_row['pos_i']\n",
    "                        deleted_0 = deleted.iloc[0]['neg_i']\n",
    "                        deleted_1 = deleted.iloc[0]['pos_i']\n",
    "                        if overlap([simultan_0,simultan_1],[deleted_0,deleted_1]):\n",
    "                            simultan = pd.concat([simultan, deleted.iloc[[0]]])\n",
    "                            deleted.drop(deleted.index[0], axis=0, inplace=True)\n",
    "                            still_overlap = True\n",
    "                            break\n",
    "                    if still_overlap: continue\n",
    "                    else:\n",
    "                        ## check max ps and add to filt\n",
    "                        # if len(simultan)>1:\n",
    "                        #     print('hu')\n",
    "                        simultan.sort_values(by=['power'],ascending=False,inplace=True)\n",
    "                        row_df = simultan.iloc[[0]]\n",
    "                        row_df.iat[0, row_df.columns.get_loc(electrode_column_name)] = np.array(simultan[electrode_column_name])\n",
    "                        filtered = pd.concat([filtered,row_df])\n",
    "                        if len(simultan) > 1: filtered_moreThan1 = pd.concat([filtered_moreThan1,row_df])\n",
    "                        simultan = pd.DataFrame([], columns = deleted.columns)                    \n",
    "            \n",
    "        filtered.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "        filtered_moreThan1.reset_index(drop=True, inplace=True) # make sure indexes pair with number of rows\n",
    "        allsubsdata_perFile[id][uniqeElctd_kcomp_key] = filtered   \n",
    "        allsubsdata_perFile[id][uniqeElctds_kcomp_key] = filtered_moreThan1   \n",
    "        print(f\"sub:{id}, before filt:{np.shape(allsubsdata_perFile[id][kcomp_key_to_use])[0]}, after:{np.shape(allsubsdata_perFile[id][uniqeElctd_kcomp_key])[0]}, after>1: after:{np.shape(allsubsdata_perFile[id][uniqeElctds_kcomp_key])[0]}\")\n",
    "def add_edfViewFormat_kcomp_shaYKM(file_ids,event_key_for_save, kcomp_efdViewFormat_key):\n",
    "    for id in file_ids:\n",
    "        if event_key_for_save not in allsubsdata_perFile[id]:\n",
    "                #print(f\"no {event_key_for_save} for sub {id}\")\n",
    "                continue\n",
    "        kcomp_df = allsubsdata_perFile[id][event_key_for_save]\n",
    "\n",
    "        startTime_arr = np.array(kcomp_df['neg_i'])\n",
    "        endTime_arr = np.array(kcomp_df['pos_i']) \n",
    "        duration_arr = np.round((endTime_arr - startTime_arr)/configu['sample_freq'],2)\n",
    "        electd_arr_per_sw = np.array(kcomp_df[electrode_column_name])\n",
    "        desc = [f\"kcomp@@{electd_arr[0]}\" for electd_arr in electd_arr_per_sw]\n",
    "        new_format_kcomp = np.array([startTime_arr /configu['sample_freq'],duration_arr,desc]).T ## onset (sec), duration,desc\n",
    "\n",
    "        allsubsdata_perFile[id][kcomp_efdViewFormat_key] = new_format_kcomp\n",
    "\n",
    "def save_eventsSepratelyForComparison_edfViewFormat(file_ids,events_types_to_compare,edfViewFormat_output_dir):\n",
    "    # Save events (scoring and ss) per file, in a format suitable for EDF_viewer\n",
    "    for id in file_ids:\n",
    "        for event_type in events_types_to_compare:\n",
    "            filename = f\"{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_events\"\n",
    "            all_events_with_header = np.asarray([header],dtype=object)\n",
    "            events_type_found = [ v for k, v in allsubsdata_perFile[id].items() if ((event_type in k) and (edfviewFormatSuffix in k))]\n",
    "            if np.size(events_type_found) == 0: continue\n",
    "            for events_found in events_type_found:\n",
    "                    for event_found in events_found:\n",
    "                            all_events_with_header = np.concatenate((all_events_with_header,[event_found]),dtype=object)\n",
    "            np.savetxt(f\"{edfViewFormat_output_dir}\\\\{event_type}_{filename}.txt\", all_events_with_header, delimiter='\\t',fmt='%s')\n",
    "\n",
    "run_kcomp_shaYKM = True\n",
    "if(run_kcomp_shaYKM):\n",
    "    testType =0\n",
    "    if(testType==1):\n",
    "        file_ids = ['32_2']\n",
    "        electrodes_names_eventDetect = ['F4']\n",
    "    elif(testType==2):\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['F4']\n",
    "    elif(testType==3):\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['F4','F3']\n",
    "    else:\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','Cz']\n",
    "\n",
    "    with open(import_path, \"rb\") as file:  [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "    fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "    if not os.path.exists(fig_output_dir): os.mkdir(fig_output_dir)\n",
    "\n",
    "    edfViewFormat_events_manual_dir = f'{fig_output_dir}\\\\EDFViewFormat_events'\n",
    "    edfViewFormat_events_output_dir = f'{fig_output_dir}\\\\EDFViewFormat_events\\\\kcomp_tests_ShaYKM'\n",
    "    if os.path.exists(edfViewFormat_events_output_dir):\n",
    "        shutil.rmtree(edfViewFormat_events_output_dir)\n",
    "    os.mkdir(edfViewFormat_events_output_dir)\n",
    "\n",
    "    all_electodes_kcompSrid_key = 'KComp_shaYKM'\n",
    "    res = detect_kComp_shaYKM(file_ids,all_electodes_kcompSrid_key,electrodes_names_eventDetect)\n",
    "\n",
    "    \n",
    "    sw_key_to_use = all_electodes_kcompSrid_key\n",
    "    multiElectdPerKC_text = \"multiElectd\"\n",
    "    singleElectdPerKC_text = \"singleElectd\"\n",
    "    multiElectdPerKC_key = f\"{sw_key_to_use}_{multiElectdPerKC_text}\"\n",
    "    singleElectdPerKC_key = f\"{sw_key_to_use}_{singleElectdPerKC_text}\"\n",
    "    group_kcomp(sw_key_to_use,multiElectdPerKC_key, singleElectdPerKC_key)\n",
    "\n",
    "\n",
    "    edfviewFormatSuffix = 'efdViewFormat'\n",
    "    kcomp_single_efdViewFormat_key = f'{singleElectdPerKC_key}_{edfviewFormatSuffix}'\n",
    "    kcomp_multi_efdViewFormat_key = f'{multiElectdPerKC_key}_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_kcomp_shaYKM(file_ids,multiElectdPerKC_key,kcomp_multi_efdViewFormat_key)\n",
    "    add_edfViewFormat_kcomp_shaYKM(file_ids,singleElectdPerKC_key,kcomp_single_efdViewFormat_key)\n",
    "\n",
    "    scoring_edVfiewFormat_key = f'scoring_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_scoring(scoring_edfViewFormat_key)\n",
    "\n",
    "    events_to_save_seperately = [scoring_edVfiewFormat_key, multiElectdPerKC_text,singleElectdPerKC_text] \n",
    "    save_eventsSepratelyForComparison_edfViewFormat(file_ids,events_to_save_seperately,edfViewFormat_events_output_dir)\n",
    "\n",
    "    # events_types_to_save = ['scoring', kcomp_multi_efdViewFormat_key]    \n",
    "    # save_eventsAllTogether_edfViewFormat(events_types_to_save,edfViewFormat_events_output_dir)\n",
    "\n",
    "    allsubsdata_perFile['32_2'][all_electodes_kcompSrid_key].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preform_KC_comparison(events_types_to_compare,file_ids,minMax_sd_vers):\n",
    "    all_comparisons = pd.DataFrame(columns=['id','eventType','minmax_ver','Positive','TruePos','FalsePos', 'hitRate','falseDiscoveryRate'])\n",
    "    for id in file_ids:\n",
    "        for ver in minMax_sd_vers:\n",
    "            for event_type in events_types_to_compare:\n",
    "                dir_auto_ = f\"{edfViewFormat_events_output_dir}\\\\{ver}\"\n",
    "                manual_detection_filename = f\"{edfViewFormat_events_manual_dir}\\\\sub{id.split('_')[0]}_{id.split('_')[1]}_imported-kcomp.txt\"\n",
    "                auto_detection_filename = f\"{dir_auto_}\\\\{event_type}_{id}_events.txt\"\n",
    "                ## get the array of before manual scanning\n",
    "                if os.path.exists(auto_detection_filename) and os.path.exists(manual_detection_filename):\n",
    "                    all_ss_auto = np.loadtxt(auto_detection_filename, delimiter=\"\\t\",dtype='object')\n",
    "                    all_ss_auto = np.delete(all_ss_auto, np.where(all_ss_auto[:, 2] == \"Annotation\")[0], axis=0)\n",
    "                    all_ss_auto[:,[0,1]] = [np.double(x) for x in all_ss_auto[:,[0,1]]]\n",
    "                    ss_ind = np.array([],dtype=int)\n",
    "                    for ind_i, desc in enumerate(all_ss_auto[:,2]):\n",
    "                        if (\"kcomp\" in desc) or (\"KC\" in desc):\n",
    "                            ss_ind = np.append(ss_ind, ind_i)\n",
    "                    all_ss_auto = all_ss_auto[ss_ind,:]\n",
    "\n",
    "                    ## get the array of after manual scanning\n",
    "                    all_kc_manu = np.loadtxt(manual_detection_filename, delimiter=\"\\t\",dtype='object')\n",
    "                    all_kc_manu = np.delete(all_kc_manu, np.where(all_kc_manu[:, 2] == \"Annotation\")[0], axis=0)\n",
    "                    all_kc_manu[:,[0,1]] = [np.double(x) for x in all_kc_manu[:,[0,1]]]\n",
    "                    ss_ind = np.array([],dtype=int)\n",
    "                    for ind_i, desc in enumerate(all_kc_manu[:,2]):\n",
    "                        if \"kcomp\" in desc:\n",
    "                            ss_ind = np.append(ss_ind, ind_i)\n",
    "                    all_kc_manu = all_kc_manu[ss_ind,:]\n",
    "\n",
    "                    ## compare to find rate od TP and FP\n",
    "                    TP = 0\n",
    "                    FP = 0\n",
    "                    for ss_auto in all_ss_auto:\n",
    "                        found = False\n",
    "                        for ss_manu in all_kc_manu:\n",
    "                            if overlap([ss_auto[0],ss_auto[0]+ss_auto[1]],[ss_manu[0],ss_manu[0]+ss_manu[1]]):\n",
    "                                TP +=1\n",
    "                                found = True\n",
    "                                break\n",
    "                        if found == False:\n",
    "                            FP +=1\n",
    "\n",
    "                    # FN = 0\n",
    "                    # for ss_manu in all_ss_manu:\n",
    "                    #     found = False\n",
    "                    #     for ss_auto in all_ss_auto:\n",
    "                    #         if overlap([ss_auto[0],ss_auto[0]+ss_auto[1]],[ss_manu[0],ss_manu[0]+ss_manu[1]]):\n",
    "                    #             found = True\n",
    "                    #             break\n",
    "                    #     if found == False:\n",
    "                    #         FN +=1\n",
    "\n",
    "                    Positive = np.shape(all_kc_manu)[0]   \n",
    "                    hitRate = TP/Positive\n",
    "                    #missRate = FN/Positive\n",
    "                    falseDiscoveryRate = FP/(FP+TP)\n",
    "                    \n",
    "                    comparison = [id,event_type, ver, Positive, TP, FP,hitRate, falseDiscoveryRate]\n",
    "                    all_comparisons.loc[len(all_comparisons)] = comparison\n",
    "\n",
    "    all_comparisons = all_comparisons.sort_values('minmax_ver')\n",
    "    all_comparisons = all_comparisons.sort_values('id')\n",
    "    return all_comparisons\n",
    "if(run_comparison_of_spindle_minMax):\n",
    "    events_types_to_compare = [multiElectdPerKC_text,singleElectdPerKC_text]\n",
    "    all_comp = preform_minmaxSD_comparison(events_types_to_compare,file_ids)\n",
    "    # all_comp.to_csv('ss_comparisons.csv',index=False)\n",
    "    display(all_comp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN Sridhar_kcomplex events\n",
    "run_kcomp_srid = False\n",
    "\n",
    "def detect_kComp_Srid(file_ids,output_key,electrodes_names):\n",
    "    for id in file_ids:\n",
    "        kcomp_df =  pd.DataFrame([], columns = kcomp_df_columns)\n",
    "\n",
    "        datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "        datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "        scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'])\n",
    "        \n",
    "        for electrode_name in electrodes_names:\n",
    "            curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name)[0][0]\n",
    "            data = allsubsdata_perFile[id]['data'][curr_electrode_num] ## shape (time)\n",
    "            z = wanted_freq / configu['sample_freq']\n",
    "            resampled = signal.resample(data, int(np.size(data)*z)) ## downsample to 100hz\n",
    "            filtered = butter_bandpass_filter(resampled, lowcut, highcut, wanted_freq, order=6) ## oreder can be changed. see: https://stackoverflow.com/questions/12093594/how-to-implement-band-pass-butterworth-filter-with-scipy-signal-butter\n",
    "\n",
    "            for epoch_i in np.arange(np.size(filtered)/wanted_freq/number_of_sec_in_epoch,dtype=int):\n",
    "                if epoch_i == 0: continue\n",
    "                curr_epoch_orig_i = int(epoch_i*wanted_freq*number_of_sec_in_epoch/z)\n",
    "                if (scoring_upsampled[curr_epoch_orig_i] ==2 or scoring_upsampled[curr_epoch_orig_i] ==3):\n",
    "                    curr_start_time = (epoch_i-1)*wanted_freq*number_of_sec_in_epoch\n",
    "                    curr_end_time = epoch_i*wanted_freq*number_of_sec_in_epoch\n",
    "                    epoch_data = filtered[int(curr_start_time) : int(curr_end_time)]\n",
    "                    scaled_epoch_data = max(epoch_data) - epoch_data; #scale the signal by the maximum value.\n",
    "                    \n",
    "                    curr_minimas_inds, _ = find_peaks(-epoch_data, height=40, distance=distance_in_samples)\n",
    "\n",
    "                    for minima_ind in curr_minimas_inds:\n",
    "                        if epoch_data[minima_ind] < -40:\n",
    "                            start_range = max(0, minima_ind-(range_for_pos_peak *wanted_freq))\n",
    "                            end_range = min((number_of_sec_in_epoch *wanted_freq)-1, minima_ind+(range_for_pos_peak *wanted_freq))               \n",
    "                            \n",
    "                            curr_maximas_inds, _ = find_peaks(epoch_data[start_range:end_range], height=0)\n",
    "                            if np.size(curr_minimas_inds) ==0 or np.size(epoch_data[curr_maximas_inds])==0: continue\n",
    "                            curr_maximas_inds = curr_maximas_inds+start_range\n",
    "                            \n",
    "                            max_ind = curr_maximas_inds[np.argmax(epoch_data[curr_maximas_inds])] \n",
    "                            \n",
    "                            if np.abs(max_ind - minima_ind) < max_negPos_gap*wanted_freq and np.abs(max_ind - minima_ind) > min_negPos_gap*wanted_freq:\n",
    "                                ## TODO: check if the negative peak is below ~45milivolt\n",
    "                                if epoch_data[max_ind] >= 2* epoch_data[minima_ind]:\n",
    "                                    if np.abs(epoch_data[max_ind] - epoch_data[minima_ind]) > 100:\n",
    "                                        new_row = {'pos_i':(curr_start_time+max_ind)/z, \n",
    "                                                            'pos_amp':epoch_data[max_ind], \n",
    "                                                            'neg_i':(curr_start_time+minima_ind)/z, \n",
    "                                                            'neg_amp':epoch_data[minima_ind],\n",
    "                                                            electrode_column_name:electrode_name}\n",
    "                                        #kcomp_df = kcomp_df.append(new_row, ignore_index=True)\n",
    "                                        new_row_df = pd.DataFrame([new_row], columns = kcomp_df_columns)\n",
    "                                        kcomp_df = pd.concat([kcomp_df,new_row_df])\n",
    "\n",
    "        if not kcomp_df.empty:\n",
    "            allsubsdata_perFile[id][output_key]  = kcomp_df\n",
    "def add_edfViewFormat_kcomp_Srid(file_ids,event_key_for_save, kcomp_efdViewFormat_key):\n",
    "    for id in file_ids:\n",
    "        if event_key_for_save not in allsubsdata_perFile[id]:\n",
    "                #print(f\"no {event_key_for_save} for sub {id}\")\n",
    "                continue\n",
    "        kcomp_df = allsubsdata_perFile[id][event_key_for_save]\n",
    "\n",
    "        startTime_arr = np.array(kcomp_df['neg_i'])\n",
    "        endTime_arr = np.array(kcomp_df['pos_i']) \n",
    "        duration_arr = np.round((endTime_arr - startTime_arr)/configu['sample_freq'],2)\n",
    "        electd_arr_per_sw = np.array(kcomp_df[electrode_column_name])\n",
    "        desc = [f\"kcomp@@{electd_arr}\" for electd_arr in electd_arr_per_sw]\n",
    "        new_format_kcomp = np.array([startTime_arr /configu['sample_freq'],duration_arr,desc]).T ## onset (sec), duration,desc\n",
    "\n",
    "        allsubsdata_perFile[id][kcomp_efdViewFormat_key] = new_format_kcomp\n",
    "\n",
    "if(run_kcomp_srid):\n",
    "    # Sample rate and desired cutoff frequencies (in Hz).\n",
    "    wanted_freq = 100\n",
    "    lowcut = 0.25\n",
    "    highcut = 6.0\n",
    "\n",
    "    number_of_sec_in_epoch = 4\n",
    "    distance_in_sec = 1.5\n",
    "    distance_in_samples = wanted_freq * distance_in_sec\n",
    "    min_negPos_gap = 0.2 # in sec\n",
    "    max_negPos_gap = 1.5 # in sec\n",
    "    range_for_pos_peak = 1 #in sec\n",
    "\n",
    "    kcomp_df_columns = ['pos_i','pos_amp','neg_i','neg_amp',electrode_column_name]\n",
    "    kcomp_df =  pd.DataFrame([], columns = kcomp_df_columns)    \n",
    "    testType = 1\n",
    "    if(testType==1):\n",
    "        file_ids = ['38_2']\n",
    "        electrodes_names_eventDetect = ['Fp2']\n",
    "    elif(testType==2):\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp2','F3']\n",
    "    else:\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "\n",
    "    with open(import_path, \"rb\") as file:  [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "    fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "    if not os.path.exists(fig_output_dir): os.mkdir(fig_output_dir)\n",
    "\n",
    "    edfViewFormat_events_manual_dir = f'{fig_output_dir}\\\\EDFViewFormat_events'\n",
    "    edfViewFormat_events_output_dir = f'{fig_output_dir}\\\\EDFViewFormat_events\\\\kcomp_tests_Sridhar'\n",
    "    if os.path.exists(edfViewFormat_events_output_dir):\n",
    "        shutil.rmtree(edfViewFormat_events_output_dir)\n",
    "    os.mkdir(edfViewFormat_events_output_dir)\n",
    "\n",
    "    all_electodes_kcompSrid_key = 'KComp_Srid'\n",
    "    res = detect_kComp_Srid(file_ids,all_electodes_kcompSrid_key,electrodes_names_eventDetect)\n",
    "\n",
    "    edfviewFormatSuffix = 'efdViewFormat'\n",
    "    kcomp_efdViewFormat_key = f'{all_electodes_kcompSrid_key}_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_kcomp_Srid(file_ids,all_electodes_kcompSrid_key,kcomp_efdViewFormat_key)\n",
    "\n",
    "    scoring_edfViewFormat_key = f'scoring_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_scoring(scoring_edfViewFormat_key)\n",
    "\n",
    "    events_types_to_save = ['scoring', kcomp_efdViewFormat_key]    \n",
    "    save_eventsAllTogether_edfViewFormat(events_types_to_save,edfViewFormat_events_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08-מרץ-23 14:14:33 | WARNING | Hypnogram is LONGER than data by 3.59 seconds. Cropping hypnogram to match data.size.\n",
      "08-מרץ-23 14:14:34 | WARNING | Hypnogram is LONGER than data by 6.99 seconds. Cropping hypnogram to match data.size.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>pos_i</th>\n",
       "      <th>pos_amp</th>\n",
       "      <th>neg_i</th>\n",
       "      <th>neg_amp</th>\n",
       "      <th>electrode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>307180.0</td>\n",
       "      <td>72.792779</td>\n",
       "      <td>306365.0</td>\n",
       "      <td>-111.299432</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>312160.0</td>\n",
       "      <td>198.979489</td>\n",
       "      <td>311445.0</td>\n",
       "      <td>-212.038488</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>437815.0</td>\n",
       "      <td>131.964299</td>\n",
       "      <td>436970.0</td>\n",
       "      <td>-210.697210</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>443720.0</td>\n",
       "      <td>174.842153</td>\n",
       "      <td>443020.0</td>\n",
       "      <td>-222.760172</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>452630.0</td>\n",
       "      <td>113.701311</td>\n",
       "      <td>451820.0</td>\n",
       "      <td>-141.164822</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "      <td>4429020.0</td>\n",
       "      <td>146.594144</td>\n",
       "      <td>4428250.0</td>\n",
       "      <td>-134.005688</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0</td>\n",
       "      <td>4429705.0</td>\n",
       "      <td>91.556405</td>\n",
       "      <td>4428890.0</td>\n",
       "      <td>-164.257045</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0</td>\n",
       "      <td>4495785.0</td>\n",
       "      <td>84.974502</td>\n",
       "      <td>4495015.0</td>\n",
       "      <td>-109.106649</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0</td>\n",
       "      <td>4496770.0</td>\n",
       "      <td>118.972152</td>\n",
       "      <td>4496050.0</td>\n",
       "      <td>-114.927694</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0</td>\n",
       "      <td>4497820.0</td>\n",
       "      <td>51.347452</td>\n",
       "      <td>4497080.0</td>\n",
       "      <td>-55.257885</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index      pos_i     pos_amp      neg_i     neg_amp electrode\n",
       "0        0   307180.0   72.792779   306365.0 -111.299432        F4\n",
       "1        0   312160.0  198.979489   311445.0 -212.038488        F4\n",
       "2        0   437815.0  131.964299   436970.0 -210.697210        F4\n",
       "3        0   443720.0  174.842153   443020.0 -222.760172        F4\n",
       "4        0   452630.0  113.701311   451820.0 -141.164822        F4\n",
       "..     ...        ...         ...        ...         ...       ...\n",
       "131      0  4429020.0  146.594144  4428250.0 -134.005688        F4\n",
       "132      0  4429705.0   91.556405  4428890.0 -164.257045        F4\n",
       "133      0  4495785.0   84.974502  4495015.0 -109.106649        F4\n",
       "134      0  4496770.0  118.972152  4496050.0 -114.927694        F4\n",
       "135      0  4497820.0   51.347452  4497080.0  -55.257885        F4\n",
       "\n",
       "[136 rows x 6 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN ShaYKM_kcomplex events - Find minimas first. Its not as good as finding the max first. \n",
    "#I think its becuase I need to pad around the maxima scope to allow more slow waves to be detected\n",
    "\n",
    "run_kcomp_shaYKM = False\n",
    "\n",
    "\n",
    "def detect_kComp_shaYKM(file_ids,output_key,electrodes_names):\n",
    "    for id in file_ids:\n",
    "        kcomp_df =  pd.DataFrame([], columns = kcomp_df_columns)\n",
    "\n",
    "        datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "        datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "        scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'])\n",
    "        \n",
    "        for electrode_name in electrodes_names:\n",
    "            curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name)[0][0]\n",
    "            data = allsubsdata_perFile[id]['data'][curr_electrode_num] ## shape (time)\n",
    "            z = new_freq / configu['sample_freq']\n",
    "            resampled = signal.resample(data, int(np.size(data)*z)) ## downsample to 100hz\n",
    "\n",
    "            lowcut = 0.25\n",
    "            highcut = 3.0\n",
    "            filer_order = 3\n",
    "            smoothing_window = 3\n",
    "            smoothen  = scipy.signal.medfilt(resampled, smoothing_window)\n",
    "            filtered_sw = butter_bandpass_filter(smoothen, lowcut, highcut, new_freq, order=filer_order) ## oreder can be changed. see: https://stackoverflow.com/questions/12093594/how-to-implement-band-pass-butterworth-filter-with-scipy-signal-butter\n",
    "            \n",
    "            lowcut = 0.25\n",
    "            highcut = 6.0\n",
    "            filer_order = 2\n",
    "            filtered_transiantMinima = butter_bandpass_filter(resampled, lowcut, highcut, new_freq, order=filer_order)\n",
    "            \n",
    "            distance_in_preSW = 1\n",
    "            \n",
    "            max_scaling = max(filtered_transiantMinima) - filtered_transiantMinima; #scale the signal by the maximum value.\n",
    "            curr_minimas_inds, _ = find_peaks(max_scaling ,distance=distance_in_preSW*new_freq)\n",
    "            for minima_ind in curr_minimas_inds:\n",
    "                if (scoring_upsampled[int(minima_ind/z)] ==2):\n",
    "                    # plt.plot(resampled[max(minima_ind-200,0):min(minima_ind+200,np.size(resampled)-1)])\n",
    "                    # plt.plot(filtered_sw[max(minima_ind-200,0):min(minima_ind+200,np.size(resampled)-1)])\n",
    "                    # plt.plot(filtered_notSW[max(minima_ind-200,0):min(minima_ind+200,np.size(resampled)-1)])\n",
    "                    # plt.show()\n",
    "                    minima = filtered_transiantMinima[minima_ind]\n",
    "\n",
    "                    # locate maxima\n",
    "                    range_for_pos_peak = 1 #in sec\n",
    "                    maxima_scope = filtered_sw[minima_ind : minima_ind + int(range_for_pos_peak*new_freq)]\n",
    "\n",
    "                    distance_sw = 1 #sec\n",
    "                    sw_min_width = 0.3 # sec\n",
    "                    curr_maximas_inds, _ = find_peaks(maxima_scope, threshold=0,width=sw_min_width*new_freq, distance=distance_sw*new_freq)\n",
    "                    if np.size(curr_maximas_inds) ==0 : continue\n",
    "\n",
    "                    max_maxima_ind_inScope = curr_maximas_inds[np.argmax(maxima_scope[curr_maximas_inds])]\n",
    "                    maxima = maxima_scope[max_maxima_ind_inScope]\n",
    "                    maxima_ind = max_maxima_ind_inScope + (minima_ind + int(range_for_pos_peak*new_freq))\n",
    "\n",
    "                    if (minima < maxima-80) and (minima<-40) and (maxima > 0.5*abs(minima)): ## found a k-comp!\n",
    "                        new_row =           {'pos_i':maxima_ind/z, \n",
    "                                            'pos_amp':maxima, \n",
    "                                            'neg_i':minima_ind/z, \n",
    "                                            'neg_amp':minima,\n",
    "                                            electrode_column_name:electrode_name}\n",
    "                        new_row_df = pd.DataFrame([new_row], columns = kcomp_df_columns)\n",
    "                        kcomp_df = pd.concat([kcomp_df,new_row_df])\n",
    "\n",
    "        if not kcomp_df.empty:\n",
    "            allsubsdata_perFile[id][output_key]  = kcomp_df\n",
    "        else: print(f'no events: {output_key}')\n",
    "def add_edfViewFormat_kcomp_shaYKM(file_ids,event_key_for_save, kcomp_efdViewFormat_key):\n",
    "    for id in file_ids:\n",
    "        if event_key_for_save not in allsubsdata_perFile[id]:\n",
    "                #print(f\"no {event_key_for_save} for sub {id}\")\n",
    "                continue\n",
    "        kcomp_df = allsubsdata_perFile[id][event_key_for_save]\n",
    "\n",
    "        startTime_arr = np.array(kcomp_df['neg_i'])\n",
    "        endTime_arr = np.array(kcomp_df['pos_i']) \n",
    "        duration_arr = np.round((endTime_arr - startTime_arr)/configu['sample_freq'],2)\n",
    "        electd_arr_per_sw = np.array(kcomp_df[electrode_column_name])\n",
    "        desc = [f\"kcomp@@{electd_arr}\" for electd_arr in electd_arr_per_sw]\n",
    "        new_format_kcomp = np.array([startTime_arr /configu['sample_freq'],duration_arr,desc]).T ## onset (sec), duration,desc\n",
    "\n",
    "        allsubsdata_perFile[id][kcomp_efdViewFormat_key] = new_format_kcomp\n",
    "\n",
    "if(run_kcomp_shaYKM):\n",
    "    # Sample rate and desired cutoff frequencies (in Hz).\n",
    "    new_freq = 100\n",
    "    kcomp_df_columns = ['pos_i','pos_amp','neg_i','neg_amp',electrode_column_name]\n",
    "    kcomp_df =  pd.DataFrame([], columns = kcomp_df_columns)\n",
    "    testType =2\n",
    "    if(testType==1):\n",
    "        file_ids = ['32_2']\n",
    "        electrodes_names_eventDetect = ['F4']\n",
    "    elif(testType==2):\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['F4']\n",
    "    else:\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','Cz']\n",
    "\n",
    "    with open(import_path, \"rb\") as file:  [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "    fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "    if not os.path.exists(fig_output_dir): os.mkdir(fig_output_dir)\n",
    "\n",
    "    edfViewFormat_events_manual_dir = f'{fig_output_dir}\\\\EDFViewFormat_events'\n",
    "    edfViewFormat_events_output_dir = f'{fig_output_dir}\\\\EDFViewFormat_events\\\\kcomp_tests_ShaYKM'\n",
    "    if os.path.exists(edfViewFormat_events_output_dir):\n",
    "        shutil.rmtree(edfViewFormat_events_output_dir)\n",
    "    os.mkdir(edfViewFormat_events_output_dir)\n",
    "\n",
    "    all_electodes_kcompSrid_key = 'KComp_shaYKM'\n",
    "    res = detect_kComp_shaYKM(file_ids,all_electodes_kcompSrid_key,electrodes_names_eventDetect)\n",
    "\n",
    "    edfviewFormatSuffix = 'efdViewFormat'\n",
    "    kcomp_efdViewFormat_key = f'{all_electodes_kcompSrid_key}_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_kcomp_shaYKM(file_ids,all_electodes_kcompSrid_key,kcomp_efdViewFormat_key)\n",
    "\n",
    "    scoring_edfViewFormat_key = f'scoring_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_scoring(scoring_edfViewFormat_key)\n",
    "\n",
    "    events_types_to_save = ['scoring', kcomp_efdViewFormat_key]    \n",
    "    save_eventsAllTogether_edfViewFormat(events_types_to_save,edfViewFormat_events_output_dir)\n",
    "\n",
    "    allsubsdata_perFile['32_2'][all_electodes_kcompSrid_key].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_kComp_AndrillonNir(file_ids,output_key,electrodes_names):\n",
    "    andriNir_code_dir = os.getcwd()\n",
    "    andriNir_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\\\\Andrillon_Nir\"\n",
    "    andriNir_aux_output_dir = f\"{andriNir_output_dir}\\\\aux_mats\"\n",
    "\n",
    "    if os.path.exists(andriNir_output_dir):\n",
    "        shutil.rmtree(andriNir_output_dir)\n",
    "    os.mkdir(andriNir_output_dir)\n",
    "    os.mkdir(andriNir_aux_output_dir)\n",
    "\n",
    "    kcomp_output_columns = ['wavest' ,'wavend', 'mdpt', 'poszx', 'period' ,'abs(b)', 'bx', 'c' ,'cx' ,'maxb2c', 'n1', 'n1x', 'nEnd', 'nEndx' ,'p1', 'p1x' ,'meanAmp', 'nump', 'nperiod/SR', 'p2p', 'mxdn', 'mxup' ,'currentStage']\n",
    "\n",
    "    for id in file_ids:\n",
    "        datafile_data = allsubsdata_perFile[id]['data'] ## shape (electrode, time)\n",
    "        datafile_scoring = allsubsdata_perFile[id]['scoring'] ## shape (time/sampling/30)\n",
    "        scoring_upsampled = yasa.hypno_upsample_to_data(datafile_scoring, 1/30, datafile_data, sf_data=configu['sample_freq'], verbose=True)\n",
    "        kcomp_allElectrodes = pd.DataFrame()\n",
    "\n",
    "        for electd_i, electrode_name_eventDetect in enumerate(electrodes_names):\n",
    "            curr_electrode_num = np.where(configu['electrodes_names'] == electrode_name_eventDetect)[0][0]\n",
    "\n",
    "            ## create aux files to use in MATLAB\n",
    "            datafile_1elect_eeg = datafile_data[curr_electrode_num,:]\n",
    "            if 2 not in datafile_scoring or 3 not in datafile_scoring:\n",
    "                continue\n",
    "            \n",
    "            mat_to_save = {'datafile_data': datafile_1elect_eeg, 'scoring_upsampled': scoring_upsampled, 'sample_freq': configu['sample_freq'], 'electrode_name':electrode_name_eventDetect}\n",
    "            scipy.io.savemat(f\"{andriNir_aux_output_dir}\\\\{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\",mat_to_save)\n",
    "\n",
    "            ## run Andrillon & Nir Kcomp detection over all subjects files\n",
    "            eng = matlab.engine.start_matlab()\n",
    "            eng.cd(andriNir_code_dir, nargout=0)\n",
    "            out = eng.batch_useAndrillonNirKcompDetection(andriNir_aux_output_dir, andriNir_output_dir,nargout=0)\n",
    "            eng.quit()\n",
    "\n",
    "            ## add the SW data to the main subject dictionary\n",
    "            try: \n",
    "                kcomp_file_name = f\"{andriNir_output_dir}\\\\{output_key}_{allsubsdata_perFile[id]['subnum']}_{allsubsdata_perFile[id]['filenum']}_{electrode_name_eventDetect}AndScoring.mat\"\n",
    "                matlabImport = scipy.io.loadmat(kcomp_file_name, simplify_cells=True)\n",
    "            except Exception: \n",
    "                print(f\"Error importing kcomp sub file at: {kcomp_file_name}\")\n",
    "                continue\n",
    "\n",
    "            kComps = matlabImport['kcomps']\n",
    "            if np.size(np.shape(kComps)) == 1: \n",
    "                temp = np.zeros((1,np.size(kComps)),dtype=object)\n",
    "                temp[0] = kComps\n",
    "                kComps = temp\n",
    "            if np.size(kComps) >0 :\n",
    "                df = pd.DataFrame(np.double(kComps))\n",
    "                df.columns = kcomp_output_columns\n",
    "                tile_electrode = np.tile(f\"{electrode_name_eventDetect}\", kComps.shape[0])[None].T\n",
    "                df[electrode_column_name] = tile_electrode\n",
    "                \n",
    "                allsubsdata_perFile[id][f\"{output_key}@@{electrode_name_eventDetect}\"] = df\n",
    "                if electd_i ==0: kcomp_allElectrodes =  df\n",
    "                else:  kcomp_allElectrodes = pd.concat([kcomp_allElectrodes,df])\n",
    "    \n",
    "        if not kcomp_allElectrodes.empty:\n",
    "            allsubsdata_perFile[id][output_key]  = kcomp_allElectrodes\n",
    "\n",
    "        \n",
    "    shutil.rmtree(andriNir_aux_output_dir)\n",
    "def filter_kcompCriteria(file_ids,event_key_to_use,afterkcompCriteria_exclution_key):\n",
    "            # % Result Matrix\n",
    "        # %1:  wave beginning (sample)\n",
    "        # %2:  wave end (sample)\n",
    "        # %3:  wave middle point (sample)\n",
    "        # %4:  wave negative half-way (sample)\n",
    "        # %5:  period in seconds\n",
    "        # %6:  positive amplitude peak\n",
    "        # %7:  positive amplitude peak position (sample)\n",
    "        # %8:  negative amplitude peak\n",
    "        # %9:  negative amplitude peak position (sample)\n",
    "        # %10: peak-to-peak amplitude\n",
    "        # %11: 1st pos peak amplitude\n",
    "        # %12: 1st pos peak amplitude position (sample)\n",
    "        # %13: Last pos peak amplitude\n",
    "        # %14: Last pos peak amplitude position (sample)\n",
    "        # %15: 1st neg peak amplitude\n",
    "        # %16: 1st neg peak amplitude position (sample)\n",
    "        # %17: mean wave amplitude\n",
    "        # %18: number of positive peaks\n",
    "        # %19: wave negative half-way period\n",
    "        # %20: 1st peak to last peak period\n",
    "        # %21: determines instantaneous positive 1st segement slope on smoothed signal\n",
    "        # %22: determines maximal negative slope for 2nd segement\n",
    "        # %23: stage (if scored data)\n",
    "    for id in file_ids:\n",
    "        if event_key_to_use not in allsubsdata_perFile[id]: continue\n",
    "        event_allElectrodes = allsubsdata_perFile[id][event_key_to_use]\n",
    "        event_filtered_kcompCrit = event_allElectrodes[       (event_allElectrodes['n1'] < 45) \n",
    "                                                        &((event_allElectrodes['currentStage'] == 2.0) | (event_allElectrodes['currentStage'] == 3.0))\n",
    "                                                       ]\n",
    "        \n",
    "    \n",
    "        if len(event_filtered_kcompCrit) == 0:  print(f\"sub:{id} - no kcomp with criteria\")\n",
    "        else: print(f\"sub:{id}, before wave criteria sleep stages:{len(event_allElectrodes)}, after:{len(event_filtered_kcompCrit)}\")\n",
    "        \n",
    "        allsubsdata_perFile[id][afterkcompCriteria_exclution_key] = event_filtered_kcompCrit\n",
    "\n",
    "def add_edfViewFormat_kcomp(file_ids,event_key_for_save, kcomp_efdViewFormat_key):\n",
    "## RUN THIS TO GET AN_kcomplex events\n",
    "run_AN_kcomp_detection = False\n",
    "if(run_AN_kcomp_detection):\n",
    "    testType = 1\n",
    "    if(testType==1):\n",
    "        file_ids = ['38_2']\n",
    "        electrodes_names_eventDetect = ['Fp2']\n",
    "    elif(testType==2):\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp2','F3']\n",
    "    else:\n",
    "        file_ids = ['32_2','38_2']\n",
    "        electrodes_names_eventDetect = ['Fp1','Fp2','F3','F4','C3','C4','P3','P4','O1','O2']\n",
    "\n",
    "    with open(import_path, \"rb\") as file:  [allsubsdata_perFile, configu] = pickle.load(file)\n",
    "    fig_output_dir = f\"{configu['outputs_dir_path']}\\\\{output_dir_name}\"\n",
    "    if not os.path.exists(fig_output_dir): os.mkdir(fig_output_dir)\n",
    "\n",
    "    edfViewFormat_events_manual_dir = f'{fig_output_dir}\\\\EDFViewFormat_events'\n",
    "    edfViewFormat_events_output_dir = f'{fig_output_dir}\\\\EDFViewFormat_events\\\\kcomp_tests_AN'\n",
    "    if os.path.exists(edfViewFormat_events_output_dir):\n",
    "        shutil.rmtree(edfViewFormat_events_output_dir)\n",
    "    os.mkdir(edfViewFormat_events_output_dir)\n",
    "\n",
    "    all_electodes_kcomp_key = 'KComp'\n",
    "    detect_kComp_AndrillonNir(file_ids,all_electodes_kcomp_key,electrodes_names_eventDetect)\n",
    "\n",
    "    after_criteria_key = f\"{all_electodes_kcomp_key}_criteria\"\n",
    "    filter_kcompCriteria(file_ids,all_electodes_kcomp_key,after_criteria_key)\n",
    "\n",
    "    edfviewFormatSuffix = 'efdViewFormat'\n",
    "    kcomp_efdViewFormat_key = f'{after_criteria_key}_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_kcomp(file_ids,after_criteria_key,kcomp_efdViewFormat_key)\n",
    "\n",
    "    scoring_edfViewFormat_key = f'scoring_{edfviewFormatSuffix}'\n",
    "    add_edfViewFormat_scoring(scoring_edfViewFormat_key)\n",
    "\n",
    "    events_types_to_save = ['scoring', kcomp_efdViewFormat_key]    \n",
    "    save_eventsAllTogether_edfViewFormat(events_types_to_save,edfViewFormat_events_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(run_AN_kcomp_detection):\n",
    "    event_allElectrodes = allsubsdata_perFile['38_2']['KComp_criteria']\n",
    "    event_filtered_kcompCrit = pd.DataFrame([], columns=event_allElectrodes.columns)\n",
    "\n",
    "    neg_events_filtered = event_allElectrodes[       (event_allElectrodes['n1'] < 40) &\n",
    "                                                    ((event_allElectrodes['currentStage'] == 2.0) | (event_allElectrodes['currentStage'] == 3.0))\n",
    "                                                    ]\n",
    "    neg_events_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    exclude_indxs = np.zeros([], dtype=int)\n",
    "    for index, row in neg_events_filtered.iterrows():\n",
    "        if index == 0: continue\n",
    "        #print(event_filtered_neg.iloc[index-1]['wavend'] +1)\n",
    "        if neg_events_filtered.iloc[index-1]['n1x'] +1.5 >=neg_events_filtered.iloc[index]['n1x']: # filtered minimas that are seperared by at leaset 1.5s\n",
    "            exclude_indxs = np.append(exclude_indxs, index)\n",
    "\n",
    "    ex = neg_events_filtered.drop(exclude_indxs, axis=0)\n",
    "    ex.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(np.size(allsubsdata_perFile['38_2']['KComp_criteria'],0))\n",
    "    print(allsubsdata_perFile['38_2']['KComp_criteria_efdViewFormat'])\n",
    "    display(allsubsdata_perFile['38_2']['KComp_criteria'])\n",
    "    a= allsubsdata_perFile['38_2']['KComp_criteria']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f544ce1a915a9875fad91c894e2c0bcad4b7a79945aa6027ef3ad27810072aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
